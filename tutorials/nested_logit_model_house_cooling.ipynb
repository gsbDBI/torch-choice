{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Examples on Nested Logit Model\n",
    "Author: Tianyu Du\n",
    "Examples here are modified from [Exercise 2: Nested logit model by Kenneth Train and Yves Croissant](https://cran.r-project.org/web/packages/mlogit/vignettes/e2nlogit.html)\n",
    "The data set HC from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California.\n",
    "\n",
    "The alternatives are:\n",
    "\n",
    "- Gas central heat with cooling gcc,\n",
    "- Electric central resistence heat with cooling ecc,\n",
    "- Electric room resistence heat with cooling erc,\n",
    "- Electric heat pump, which provides cooling also hpc,\n",
    "- Gas central heat without cooling gc,\n",
    "- Electric central resistence heat without cooling ec,\n",
    "- Electric room resistence heat without cooling er.\n",
    "- Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative.\n",
    "\n",
    "The variables are:\n",
    "\n",
    "- depvar gives the name of the chosen alternative,\n",
    "- ich.alt are the installation cost for the heating portion of the system,\n",
    "- icca is the installation cost for cooling\n",
    "- och.alt are the operating cost for the heating portion of the system\n",
    "- occa is the operating cost for cooling\n",
    "- income is the annual income of the household\n",
    "\n",
    "Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Essential Packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from deepchoice.data import ChoiceDataset, JointDataset, utils\n",
    "from deepchoice.model.nested_logit_model import NestedLogitModel\n",
    "from deepchoice.utils.std import parameter_std\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device used: {torch.cuda.get_device_name()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA device used: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encompass Configurations in a Name Space Object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "args = argparse.Namespace(data_path='./',\n",
    "                          batch_size=-1,  # full-batch.\n",
    "                          shuffle=False,\n",
    "                          num_epochs=30000,\n",
    "                          device='cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('./HC.csv', index_col=0)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depvar</th>\n",
       "      <th>icca</th>\n",
       "      <th>occa</th>\n",
       "      <th>income</th>\n",
       "      <th>ich</th>\n",
       "      <th>och</th>\n",
       "      <th>idx.id1</th>\n",
       "      <th>idx.id2</th>\n",
       "      <th>inc.room</th>\n",
       "      <th>inc.cooling</th>\n",
       "      <th>int.cooling</th>\n",
       "      <th>cooling.modes</th>\n",
       "      <th>room.modes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>24.50</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>27.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>20</td>\n",
       "      <td>7.86</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>ecc</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>7.37</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1</td>\n",
       "      <td>er</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>27.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>20</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1</td>\n",
       "      <td>erc</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>24.08</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "      <td>gc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depvar   icca  occa  income    ich   och  idx.id1 idx.id2  inc.room  \\\n",
       "0   False   0.00  0.00      20  24.50  4.09        1      ec         0   \n",
       "1   False  27.28  2.95      20   7.86  4.09        1     ecc         0   \n",
       "2   False   0.00  0.00      20   7.37  3.85        1      er        20   \n",
       "3    True  27.28  2.95      20   8.79  3.85        1     erc        20   \n",
       "4   False   0.00  0.00      20  24.08  2.26        1      gc         0   \n",
       "\n",
       "   inc.cooling  int.cooling  cooling.modes  room.modes  \n",
       "0            0            0          False       False  \n",
       "1           20            1           True       False  \n",
       "2            0            0          False        True  \n",
       "3           20            1           True        True  \n",
       "4            0            0          False       False  "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# label\n",
    "# what was actually chosen.\n",
    "label = df[df['depvar'] == True].sort_values(by='idx.id1')['idx.id2'].reset_index(drop=True)\n",
    "item_names = ['ec', 'ecc', 'er', 'erc', 'gc', 'gcc', 'hpc']\n",
    "num_items = df['idx.id2'].nunique()\n",
    "# cardinal encoder.\n",
    "encoder = dict(zip(item_names, range(num_items)))\n",
    "label = label.map(lambda x: encoder[x])\n",
    "label = torch.LongTensor(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# category feature: no category feature, all features are item-level.\n",
    "category_dataset = ChoiceDataset(label=label.clone()).to(args.device)\n",
    "\n",
    "# item feature.\n",
    "item_feat_cols = ['ich', 'och', 'icca', 'occa', 'inc.room', 'inc.cooling', 'int.cooling']\n",
    "price_obs = utils.pivot3d(df, dim0='idx.id1', dim1='idx.id2', values=item_feat_cols)\n",
    "item_dataset = ChoiceDataset(label=label, price_obs=price_obs).to(args.device)\n",
    "\n",
    "# combine dataets\n",
    "dataset = JointDataset(category=category_dataset, item=item_dataset)\n",
    "data_loader = utils.create_data_loader(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example 1\n",
    "Run a nested logit model on the data for two nests and one log-sum coefficient that applies to both nests. Note that the model is specified to have the cooling alternatives `{gcc, ecc, erc, hpc}` in one nest and the non-cooling alternatives `{gc, ec, er}` in another nest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "category_to_item = {0: ['gcc', 'ecc', 'erc', 'hpc'],\n",
    "                    1: ['gc', 'ec', 'er']}\n",
    "\n",
    "# convert names \n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                         category_coef_variation_dict={},\n",
    "                         category_num_param_dict={},\n",
    "                         item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                         item_num_param_dict={'price_obs': 7},\n",
    "                         shared_lambda=True)\n",
    "\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "    for batch in data_loader:\n",
    "        loss = model.negative_log_likelihood(batch, batch['item'].label)\n",
    "        if e % (args.num_epochs // 10) == 0:\n",
    "            print(f'{e=:}: {loss=:}, {model.lambda_weight.item()=:}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e=0: loss=29266.6640625, model.lambda_weight.item()=1.0\n",
      "e=3000: loss=180.41995239257812, model.lambda_weight.item()=0.9177842140197754\n",
      "e=6000: loss=178.1299591064453, model.lambda_weight.item()=0.5788174271583557\n",
      "e=9000: loss=178.12496948242188, model.lambda_weight.item()=0.5862539410591125\n",
      "e=12000: loss=178.125, model.lambda_weight.item()=0.5866018533706665\n",
      "e=15000: loss=178.19720458984375, model.lambda_weight.item()=0.5874289870262146\n",
      "e=18000: loss=178.12911987304688, model.lambda_weight.item()=0.5867705345153809\n",
      "e=21000: loss=178.12567138671875, model.lambda_weight.item()=0.5864641666412354\n",
      "e=24000: loss=178.127197265625, model.lambda_weight.item()=0.586590588092804\n",
      "e=27000: loss=178.12474060058594, model.lambda_weight.item()=0.5859836339950562\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", \n",
    "##     \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 11 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 7.26E-06 \n",
    "## successive function values within tolerance limits \n",
    "## \n",
    "## Coefficients :\n",
    "##              Estimate Std. Error z-value  Pr(>|z|)    \n",
    "## ich         -0.554878   0.144205 -3.8478 0.0001192 ***\n",
    "## och         -0.857886   0.255313 -3.3601 0.0007791 ***\n",
    "## icca        -0.225079   0.144423 -1.5585 0.1191212    \n",
    "## occa        -1.089458   1.219821 -0.8931 0.3717882    \n",
    "## inc.room    -0.378971   0.099631 -3.8038 0.0001425 ***\n",
    "## inc.cooling  0.249575   0.059213  4.2149 2.499e-05 ***\n",
    "## int.cooling -6.000415   5.562423 -1.0787 0.2807030    \n",
    "## iv           0.585922   0.179708  3.2604 0.0011125 ** \n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -178.12\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(f'{k} = {v.detach().cpu()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lambda_weight = tensor([0.5867])\n",
      "item_coef_dict.price_obs.coef = tensor([-0.5591, -0.8631, -0.2248, -1.0893, -0.3824,  0.2521, -6.0042])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "batch = dataset[torch.arange(len(dataset))]\n",
    "def nll_loss(model):\n",
    "    return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "std = parameter_std(model, nll_loss)\n",
    "std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lambda_weight': tensor([0.1677]),\n",
       " 'item_coef_dict.price_obs.coef': tensor([0.1475, 0.2413, 0.1124, 1.0530, 0.1030, 0.0536, 4.9208])}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example 2\n",
    "Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "category_to_item = {0: ['ec', 'ecc', 'gc', 'gcc', 'hpc'],\n",
    "                    1: ['er', 'erc']}\n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                            category_coef_variation_dict={},\n",
    "                            category_num_param_dict={},\n",
    "                        #  category_num_param_dict={'intercept': 1},\n",
    "                            item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                            item_num_param_dict={'price_obs': 7},\n",
    "                            shared_lambda=True\n",
    "                            )\n",
    "\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9999)\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "    for batch in data_loader:\n",
    "        loss = model.negative_log_likelihood(batch, batch['item'].label)\n",
    "        if e % (args.num_epochs // 10) == 0:\n",
    "            print(f'{e=:}: {loss=:}, {model.lambda_weight.item()=:}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.clamp_lambdas()\n",
    "    scheduler.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e=0: loss=13013.70703125, model.lambda_weight.item()=1.0\n",
      "e=3000: loss=180.76327514648438, model.lambda_weight.item()=1.3446736335754395\n",
      "e=6000: loss=180.1411590576172, model.lambda_weight.item()=1.2529666423797607\n",
      "e=9000: loss=180.02523803710938, model.lambda_weight.item()=1.336637258529663\n",
      "e=12000: loss=180.0230712890625, model.lambda_weight.item()=1.3618589639663696\n",
      "e=15000: loss=180.02305603027344, model.lambda_weight.item()=1.362200140953064\n",
      "e=18000: loss=180.0230712890625, model.lambda_weight.item()=1.3621675968170166\n",
      "e=21000: loss=180.0230712890625, model.lambda_weight.item()=1.3621395826339722\n",
      "e=24000: loss=180.02325439453125, model.lambda_weight.item()=1.3622841835021973\n",
      "e=27000: loss=180.0230712890625, model.lambda_weight.item()=1.362113118171692\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", \n",
    "##     \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 10 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 5.87E-07 \n",
    "## gradient close to zero \n",
    "## \n",
    "## Coefficients :\n",
    "##              Estimate Std. Error z-value Pr(>|z|)  \n",
    "## ich          -1.13818    0.54216 -2.0993  0.03579 *\n",
    "## och          -1.82532    0.93228 -1.9579  0.05024 .\n",
    "## icca         -0.33746    0.26934 -1.2529  0.21024  \n",
    "## occa         -2.06328    1.89726 -1.0875  0.27681  \n",
    "## inc.room     -0.75722    0.34292 -2.2081  0.02723 *\n",
    "## inc.cooling   0.41689    0.20742  2.0099  0.04444 *\n",
    "## int.cooling -13.82487    7.94031 -1.7411  0.08167 .\n",
    "## iv            1.36201    0.65393  2.0828  0.03727 *\n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -180.02\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(f'{k} = {v.detach().cpu()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lambda_weight = tensor([1.3622])\n",
      "item_coef_dict.price_obs.coef = tensor([ -1.1383,  -1.8255,  -0.3374,  -2.0634,  -0.7573,   0.4170, -13.8256])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "batch = dataset[torch.arange(len(dataset))]\n",
    "def nll_loss(model):\n",
    "    return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "std = parameter_std(model, nll_loss)\n",
    "std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lambda_weight': tensor([0.5553]),\n",
       " 'item_coef_dict.price_obs.coef': tensor([0.4444, 0.7384, 0.2026, 1.7623, 0.2785, 0.1701, 8.0962])}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example 3\n",
    "Rewrite the code to allow three nests. For simplicity, estimate only one log-sum coefficient which is applied to all three nests. Estimate a model with alternatives gcc, ecc and erc in a nest, hpc in a nest alone, and alternatives gc, ec and er in a nest. Does this model seem better or worse than the model in exercise 1, which puts alternative hpc in the same nest as alternatives gcc, ecc and erc?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "category_to_item = {0: ['gcc', 'ecc', 'erc'],\n",
    "                    1: ['hpc'],\n",
    "                    2: ['gc', 'ec', 'er']}\n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                         category_coef_variation_dict={},\n",
    "                         category_num_param_dict={},\n",
    "                         item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                         item_num_param_dict={'price_obs': 7},\n",
    "                         shared_lambda=True\n",
    "                         )\n",
    "\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9999)\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "    for batch in data_loader:\n",
    "        loss = model.negative_log_likelihood(batch, batch['item'].label)\n",
    "        if e % (args.num_epochs // 10) == 0:\n",
    "            print(f'{e=:}: {loss=:}, {model.lambda_weight=:}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.clamp_lambdas()\n",
    "    scheduler.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e=0: loss=2655.865478515625, model.lambda_weight=Parameter containing:\n",
      "tensor([1.], device='cuda:0', requires_grad=True)\n",
      "e=3000: loss=194.99459838867188, model.lambda_weight=Parameter containing:\n",
      "tensor([1.0698], device='cuda:0', requires_grad=True)\n",
      "e=6000: loss=182.6705322265625, model.lambda_weight=Parameter containing:\n",
      "tensor([0.8543], device='cuda:0', requires_grad=True)\n",
      "e=9000: loss=181.5911102294922, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9128], device='cuda:0', requires_grad=True)\n",
      "e=12000: loss=180.92767333984375, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9212], device='cuda:0', requires_grad=True)\n",
      "e=15000: loss=180.55140686035156, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9310], device='cuda:0', requires_grad=True)\n",
      "e=18000: loss=180.37588500976562, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9398], device='cuda:0', requires_grad=True)\n",
      "e=21000: loss=180.30047607421875, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9466], device='cuda:0', requires_grad=True)\n",
      "e=24000: loss=180.2721405029297, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9516], device='cuda:0', requires_grad=True)\n",
      "e=27000: loss=180.26431274414062, model.lambda_weight=Parameter containing:\n",
      "tensor([0.9549], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", \n",
    "##     \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), \n",
    "##     un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 8 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 3.71E-08 \n",
    "## gradient close to zero \n",
    "## \n",
    "## Coefficients :\n",
    "##               Estimate Std. Error z-value  Pr(>|z|)    \n",
    "## ich          -0.838394   0.100546 -8.3384 < 2.2e-16 ***\n",
    "## och          -1.331598   0.252069 -5.2827 1.273e-07 ***\n",
    "## icca         -0.256131   0.145564 -1.7596   0.07848 .  \n",
    "## occa         -1.405656   1.207281 -1.1643   0.24430    \n",
    "## inc.room     -0.571352   0.077950 -7.3297 2.307e-13 ***\n",
    "## inc.cooling   0.311355   0.056357  5.5247 3.301e-08 ***\n",
    "## int.cooling -10.413384   5.612445 -1.8554   0.06354 .  \n",
    "## iv            0.956544   0.180722  5.2929 1.204e-07 ***\n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -180.26\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(f'{k} = {v.detach().cpu()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lambda_weight = tensor([0.9563])\n",
      "item_coef_dict.price_obs.coef = tensor([ -0.8382,  -1.3314,  -0.2567,  -1.4107,  -0.5712,   0.3113, -10.3844])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "batch = dataset[torch.arange(len(dataset))]\n",
    "def nll_loss(model):\n",
    "    return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "std = parameter_std(model, nll_loss)\n",
    "std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lambda_weight': tensor([0.1970]),\n",
       " 'item_coef_dict.price_obs.coef': tensor([0.0991, 0.1849, 0.1263, 1.1446, 0.0750, 0.0551, 5.1917])}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7dc80b2c4d9dbaf52e273e24444ebf2c26f0fdc466c7e783c99ad3a1ce41bbd"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}