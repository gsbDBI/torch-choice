{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples on Nested Logit Model\n",
    "Author: Tianyu Du\n",
    "Examples here are modified from [Exercise 2: Nested logit model by Kenneth Train and Yves Croissant](https://cran.r-project.org/web/packages/mlogit/vignettes/e2nlogit.html)\n",
    "The data set HC from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California.\n",
    "\n",
    "The alternatives are:\n",
    "\n",
    "- Gas central heat with cooling gcc,\n",
    "- Electric central resistence heat with cooling ecc,\n",
    "- Electric room resistence heat with cooling erc,\n",
    "- Electric heat pump, which provides cooling also hpc,\n",
    "- Gas central heat without cooling gc,\n",
    "- Electric central resistence heat without cooling ec,\n",
    "- Electric room resistence heat without cooling er.\n",
    "- Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative.\n",
    "\n",
    "The variables are:\n",
    "\n",
    "- depvar gives the name of the chosen alternative,\n",
    "- ich.alt are the installation cost for the heating portion of the system,\n",
    "- icca is the installation cost for cooling\n",
    "- och.alt are the operating cost for the heating portion of the system\n",
    "- occa is the operating cost for cooling\n",
    "- income is the annual income of the household\n",
    "\n",
    "Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Essential Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device used: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch_choice.data import ChoiceDataset, JointDataset, utils\n",
    "from torch_choice.model.nested_logit_model import NestedLogitModel\n",
    "from torch_choice.utils.run_helper import run\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device used: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encompass Configurations in a Name Space Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_path='./',\n",
    "                          batch_size=-1,  # full-batch.\n",
    "                          shuffle=False,\n",
    "                          num_epochs=30000,\n",
    "                          device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depvar</th>\n",
       "      <th>icca</th>\n",
       "      <th>occa</th>\n",
       "      <th>income</th>\n",
       "      <th>ich</th>\n",
       "      <th>och</th>\n",
       "      <th>idx.id1</th>\n",
       "      <th>idx.id2</th>\n",
       "      <th>inc.room</th>\n",
       "      <th>inc.cooling</th>\n",
       "      <th>int.cooling</th>\n",
       "      <th>cooling.modes</th>\n",
       "      <th>room.modes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>24.50</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>27.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>20</td>\n",
       "      <td>7.86</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>ecc</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>7.37</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1</td>\n",
       "      <td>er</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>27.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>20</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1</td>\n",
       "      <td>erc</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>24.08</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "      <td>gc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depvar   icca  occa  income    ich   och  idx.id1 idx.id2  inc.room  \\\n",
       "0   False   0.00  0.00      20  24.50  4.09        1      ec         0   \n",
       "1   False  27.28  2.95      20   7.86  4.09        1     ecc         0   \n",
       "2   False   0.00  0.00      20   7.37  3.85        1      er        20   \n",
       "3    True  27.28  2.95      20   8.79  3.85        1     erc        20   \n",
       "4   False   0.00  0.00      20  24.08  2.26        1      gc         0   \n",
       "\n",
       "   inc.cooling  int.cooling  cooling.modes  room.modes  \n",
       "0            0            0          False       False  \n",
       "1           20            1           True       False  \n",
       "2            0            0          False        True  \n",
       "3           20            1           True        True  \n",
       "4            0            0          False       False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./public_datasets/HC.csv', index_col=0)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "# what was actually chosen.\n",
    "label = df[df['depvar'] == True].sort_values(by='idx.id1')['idx.id2'].reset_index(drop=True)\n",
    "item_names = ['ec', 'ecc', 'er', 'erc', 'gc', 'gcc', 'hpc']\n",
    "num_items = df['idx.id2'].nunique()\n",
    "# cardinal encoder.\n",
    "encoder = dict(zip(item_names, range(num_items)))\n",
    "label = label.map(lambda x: encoder[x])\n",
    "label = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category feature: no category feature, all features are item-level.\n",
    "category_dataset = ChoiceDataset(label=label.clone()).to(args.device)\n",
    "\n",
    "# item feature.\n",
    "item_feat_cols = ['ich', 'och', 'icca', 'occa', 'inc.room', 'inc.cooling', 'int.cooling']\n",
    "price_obs = utils.pivot3d(df, dim0='idx.id1', dim1='idx.id2', values=item_feat_cols)\n",
    "item_dataset = ChoiceDataset(label=label, price_obs=price_obs).to(args.device)\n",
    "\n",
    "# combine dataets\n",
    "dataset = JointDataset(category=category_dataset, item=item_dataset)\n",
    "data_loader = utils.create_data_loader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "Run a nested logit model on the data for two nests and one log-sum coefficient that applies to both nests. Note that the model is specified to have the cooling alternatives `{gcc, ecc, erc, hpc}` in one nest and the non-cooling alternatives `{gc, ec, er}` in another nest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_item = {0: ['gcc', 'ecc', 'erc', 'hpc'],\n",
    "                    1: ['gc', 'ec', 'er']}\n",
    "\n",
    "# convert names \n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                         category_coef_variation_dict={},\n",
    "                         category_num_param_dict={},\n",
    "                         item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                         item_num_param_dict={'price_obs': 7},\n",
    "                         shared_lambda=True)\n",
    "\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== received model ====================\n",
      "NestedLogitModel(\n",
      "  (category_coef_dict): ModuleDict()\n",
      "  (item_coef_dict): ModuleDict(\n",
      "    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total).\n",
      "  )\n",
      ")\n",
      "==================== received dataset ====================\n",
      "JointDataset with 2 sub-datasets: (\n",
      "\tcategory: ChoiceDataset(label=[250], user_index=[], session_index=[], item_availability=[], observable_prefix=[5], device=cuda:0)\n",
      "\titem: ChoiceDataset(label=[250], user_index=[], session_index=[250], item_availability=[], observable_prefix=[5], price_obs=[250, 7, 7], device=cuda:0)\n",
      ")\n",
      "==================== training the model ====================\n",
      "Epoch 1000: Mean Log-likelihood=-211.021728515625\n",
      "Epoch 2000: Mean Log-likelihood=-193.19943237304688\n",
      "Epoch 3000: Mean Log-likelihood=-182.55709838867188\n",
      "Epoch 4000: Mean Log-likelihood=-179.28445434570312\n",
      "Epoch 5000: Mean Log-likelihood=-178.64239501953125\n",
      "Epoch 6000: Mean Log-likelihood=-178.47711181640625\n",
      "Epoch 7000: Mean Log-likelihood=-178.3613739013672\n",
      "Epoch 8000: Mean Log-likelihood=-178.25039672851562\n",
      "Epoch 9000: Mean Log-likelihood=-178.1700439453125\n",
      "Epoch 10000: Mean Log-likelihood=-178.1334686279297\n",
      "==================== model results ====================\n",
      "Training Epochs: 10000\n",
      "\n",
      "Learning Rate: 0.01\n",
      "\n",
      "Batch Size: 250 out of 250 observations in total\n",
      "\n",
      "Final Log-likelihood: -178.1334686279297\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient      |   Estimation |   Std. Err. |\n",
      "|:-----------------|-------------:|------------:|\n",
      "| lambda_weight_0  |     0.577392 |   0.164925  |\n",
      "| item_price_obs_0 |    -0.546838 |   0.14313   |\n",
      "| item_price_obs_1 |    -0.845657 |   0.23527   |\n",
      "| item_price_obs_2 |    -0.234306 |   0.110563  |\n",
      "| item_price_obs_3 |    -1.18538  |   1.03586   |\n",
      "| item_price_obs_4 |    -0.373288 |   0.0996275 |\n",
      "| item_price_obs_5 |     0.248309 |   0.051553  |\n",
      "| item_price_obs_6 |    -5.36669  |   4.76851   |\n"
     ]
    }
   ],
   "source": [
    "run(model, dataset, num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "# for e in range(args.num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         loss = model.negative_log_likelihood(batch, batch['item'].label)\n",
    "#         if e % (args.num_epochs // 10) == 0:\n",
    "#             print(f'{e=:}: {loss=:}, {model.lambda_weight.item()=:}')\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", \n",
    "##     \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 11 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 7.26E-06 \n",
    "## successive function values within tolerance limits \n",
    "## \n",
    "## Coefficients :\n",
    "##              Estimate Std. Error z-value  Pr(>|z|)    \n",
    "## ich         -0.554878   0.144205 -3.8478 0.0001192 ***\n",
    "## och         -0.857886   0.255313 -3.3601 0.0007791 ***\n",
    "## icca        -0.225079   0.144423 -1.5585 0.1191212    \n",
    "## occa        -1.089458   1.219821 -0.8931 0.3717882    \n",
    "## inc.room    -0.378971   0.099631 -3.8038 0.0001425 ***\n",
    "## inc.cooling  0.249575   0.059213  4.2149 2.499e-05 ***\n",
    "## int.cooling -6.000415   5.562423 -1.0787 0.2807030    \n",
    "## iv           0.585922   0.179708  3.2604 0.0011125 ** \n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -178.12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.named_parameters():\n",
    "#     print(f'{k} = {v.detach().cpu()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = dataset[torch.arange(len(dataset))]\n",
    "# def nll_loss(model):\n",
    "#     return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "# std = parameter_std(model, nll_loss)\n",
    "# std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_item = {0: ['ec', 'ecc', 'gc', 'gcc', 'hpc'],\n",
    "                    1: ['er', 'erc']}\n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                            category_coef_variation_dict={},\n",
    "                            category_num_param_dict={},\n",
    "                        #  category_num_param_dict={'intercept': 1},\n",
    "                            item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                            item_num_param_dict={'price_obs': 7},\n",
    "                            shared_lambda=True\n",
    "                            )\n",
    "\n",
    "model = model.to(args.device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9999)\n",
    "\n",
    "# for e in range(args.num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         loss = model.negative_log_likelihood(batch, batch['item'].label)\n",
    "#         if e % (args.num_epochs // 10) == 0:\n",
    "#             print(f'{e=:}: {loss=:}, {model.lambda_weight.item()=:}')\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # model.clamp_lambdas()\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== received model ====================\n",
      "NestedLogitModel(\n",
      "  (category_coef_dict): ModuleDict()\n",
      "  (item_coef_dict): ModuleDict(\n",
      "    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total).\n",
      "  )\n",
      ")\n",
      "==================== received dataset ====================\n",
      "JointDataset with 2 sub-datasets: (\n",
      "\tcategory: ChoiceDataset(label=[250], user_index=[], session_index=[], item_availability=[], observable_prefix=[5], device=cuda:0)\n",
      "\titem: ChoiceDataset(label=[250], user_index=[], session_index=[250], item_availability=[], observable_prefix=[5], price_obs=[250, 7, 7], device=cuda:0)\n",
      ")\n",
      "==================== training the model ====================\n",
      "Epoch 500: Mean Log-likelihood=-189.6171875\n",
      "Epoch 1000: Mean Log-likelihood=-183.52847290039062\n",
      "Epoch 1500: Mean Log-likelihood=-181.83969116210938\n",
      "Epoch 2000: Mean Log-likelihood=-180.43809509277344\n",
      "Epoch 2500: Mean Log-likelihood=-180.04177856445312\n",
      "Epoch 3000: Mean Log-likelihood=-180.17776489257812\n",
      "Epoch 3500: Mean Log-likelihood=-180.0731964111328\n",
      "Epoch 4000: Mean Log-likelihood=-180.2925567626953\n",
      "Epoch 4500: Mean Log-likelihood=-180.3995361328125\n",
      "Epoch 5000: Mean Log-likelihood=-180.7696533203125\n",
      "==================== model results ====================\n",
      "Training Epochs: 5000\n",
      "\n",
      "Learning Rate: 0.3\n",
      "\n",
      "Batch Size: 250 out of 250 observations in total\n",
      "\n",
      "Final Log-likelihood: -180.7696533203125\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient      |   Estimation |   Std. Err. |\n",
      "|:-----------------|-------------:|------------:|\n",
      "| lambda_weight_0  |     1.59468  |    0.755122 |\n",
      "| item_price_obs_0 |    -1.33103  |    0.602767 |\n",
      "| item_price_obs_1 |    -2.13653  |    0.999629 |\n",
      "| item_price_obs_2 |    -0.380562 |    0.243608 |\n",
      "| item_price_obs_3 |    -2.55752  |    2.22184  |\n",
      "| item_price_obs_4 |    -0.875605 |    0.365037 |\n",
      "| item_price_obs_5 |     0.493142 |    0.238891 |\n",
      "| item_price_obs_6 |   -15.6913   |    9.80992  |\n"
     ]
    }
   ],
   "source": [
    "run(model, dataset, num_epochs=5000, learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", \n",
    "##     \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 10 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 5.87E-07 \n",
    "## gradient close to zero \n",
    "## \n",
    "## Coefficients :\n",
    "##              Estimate Std. Error z-value Pr(>|z|)  \n",
    "## ich          -1.13818    0.54216 -2.0993  0.03579 *\n",
    "## och          -1.82532    0.93228 -1.9579  0.05024 .\n",
    "## icca         -0.33746    0.26934 -1.2529  0.21024  \n",
    "## occa         -2.06328    1.89726 -1.0875  0.27681  \n",
    "## inc.room     -0.75722    0.34292 -2.2081  0.02723 *\n",
    "## inc.cooling   0.41689    0.20742  2.0099  0.04444 *\n",
    "## int.cooling -13.82487    7.94031 -1.7411  0.08167 .\n",
    "## iv            1.36201    0.65393  2.0828  0.03727 *\n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -180.02\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.named_parameters():\n",
    "#     print(f'{k} = {v.detach().cpu()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = dataset[torch.arange(len(dataset))]\n",
    "# def nll_loss(model):\n",
    "#     return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "# std = parameter_std(model, nll_loss)\n",
    "# std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3\n",
    "Rewrite the code to allow three nests. For simplicity, estimate only one log-sum coefficient which is applied to all three nests. Estimate a model with alternatives gcc, ecc and erc in a nest, hpc in a nest alone, and alternatives gc, ec and er in a nest. Does this model seem better or worse than the model in exercise 1, which puts alternative hpc in the same nest as alternatives gcc, ecc and erc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_item = {0: ['gcc', 'ecc', 'erc'],\n",
    "                    1: ['hpc'],\n",
    "                    2: ['gc', 'ec', 'er']}\n",
    "for k, v in category_to_item.items():\n",
    "    v = [encoder[item] for item in v]\n",
    "    category_to_item[k] = sorted(v)\n",
    "\n",
    "model = NestedLogitModel(category_to_item=category_to_item,\n",
    "                         category_coef_variation_dict={},\n",
    "                         category_num_param_dict={},\n",
    "                         item_coef_variation_dict={'price_obs': 'constant'},\n",
    "                         item_num_param_dict={'price_obs': 7},\n",
    "                         shared_lambda=True\n",
    "                         )\n",
    "\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== received model ====================\n",
      "NestedLogitModel(\n",
      "  (category_coef_dict): ModuleDict()\n",
      "  (item_coef_dict): ModuleDict(\n",
      "    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total).\n",
      "  )\n",
      ")\n",
      "==================== received dataset ====================\n",
      "JointDataset with 2 sub-datasets: (\n",
      "\tcategory: ChoiceDataset(label=[250], user_index=[], session_index=[], item_availability=[], observable_prefix=[5], device=cuda:0)\n",
      "\titem: ChoiceDataset(label=[250], user_index=[], session_index=[250], item_availability=[], observable_prefix=[5], price_obs=[250, 7, 7], device=cuda:0)\n",
      ")\n",
      "==================== training the model ====================\n",
      "Epoch 500: Mean Log-likelihood=-186.1810302734375\n",
      "Epoch 1000: Mean Log-likelihood=-182.77426147460938\n",
      "Epoch 1500: Mean Log-likelihood=-181.76400756835938\n",
      "Epoch 2000: Mean Log-likelihood=-181.4237060546875\n",
      "Epoch 2500: Mean Log-likelihood=-181.26036071777344\n",
      "Epoch 3000: Mean Log-likelihood=-181.11245727539062\n",
      "Epoch 3500: Mean Log-likelihood=-180.95578002929688\n",
      "Epoch 4000: Mean Log-likelihood=-180.79638671875\n",
      "Epoch 4500: Mean Log-likelihood=-180.64500427246094\n",
      "Epoch 5000: Mean Log-likelihood=-180.5127410888672\n",
      "==================== model results ====================\n",
      "Training Epochs: 5000\n",
      "\n",
      "Learning Rate: 0.01\n",
      "\n",
      "Batch Size: 250 out of 250 observations in total\n",
      "\n",
      "Final Log-likelihood: -180.5127410888672\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient      |   Estimation |   Std. Err. |\n",
      "|:-----------------|-------------:|------------:|\n",
      "| lambda_weight_0  |     0.934802 |   0.19279   |\n",
      "| item_price_obs_0 |    -0.819485 |   0.0967955 |\n",
      "| item_price_obs_1 |    -1.30911  |   0.182129  |\n",
      "| item_price_obs_2 |    -0.321997 |   0.127624  |\n",
      "| item_price_obs_3 |    -2.05462  |   1.15039   |\n",
      "| item_price_obs_4 |    -0.55606  |   0.0729355 |\n",
      "| item_price_obs_5 |     0.310064 |   0.0552311 |\n",
      "| item_price_obs_6 |    -6.78603  |   5.06669   |\n"
     ]
    }
   ],
   "source": [
    "run(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Output\n",
    "```\n",
    "## \n",
    "## Call:\n",
    "## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n",
    "##     inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", \n",
    "##     \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), \n",
    "##     un.nest.el = TRUE)\n",
    "## \n",
    "## Frequencies of alternatives:choice\n",
    "##    ec   ecc    er   erc    gc   gcc   hpc \n",
    "## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n",
    "## \n",
    "## bfgs method\n",
    "## 8 iterations, 0h:0m:0s \n",
    "## g'(-H)^-1g = 3.71E-08 \n",
    "## gradient close to zero \n",
    "## \n",
    "## Coefficients :\n",
    "##               Estimate Std. Error z-value  Pr(>|z|)    \n",
    "## ich          -0.838394   0.100546 -8.3384 < 2.2e-16 ***\n",
    "## och          -1.331598   0.252069 -5.2827 1.273e-07 ***\n",
    "## icca         -0.256131   0.145564 -1.7596   0.07848 .  \n",
    "## occa         -1.405656   1.207281 -1.1643   0.24430    \n",
    "## inc.room     -0.571352   0.077950 -7.3297 2.307e-13 ***\n",
    "## inc.cooling   0.311355   0.056357  5.5247 3.301e-08 ***\n",
    "## int.cooling -10.413384   5.612445 -1.8554   0.06354 .  \n",
    "## iv            0.956544   0.180722  5.2929 1.204e-07 ***\n",
    "## ---\n",
    "## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "## \n",
    "## Log-Likelihood: -180.26\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Standard Errors\n",
    "**NOTE**: We are computing the standard errors using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the\n",
    "hessian of negative log-likelihood with repsect to model parameters. This leads to slight different\n",
    "results compared with R implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = dataset[torch.arange(len(dataset))]\n",
    "# def nll_loss(model):\n",
    "#     return model.negative_log_likelihood(batch, batch['item'].label)\n",
    "\n",
    "# std = parameter_std(model, nll_loss)\n",
    "# std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7dc80b2c4d9dbaf52e273e24444ebf2c26f0fdc466c7e783c99ad3a1ce41bbd"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
