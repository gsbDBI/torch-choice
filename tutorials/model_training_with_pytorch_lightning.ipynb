{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with PyTorch Lightning\n",
    "\n",
    "> This document is currently in a minimal version without sufficient annotations. We will update it in the future.\n",
    "\n",
    "> Author: Tianyu Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <B3E58761-2785-34C6-A89B-F37110C88A05> /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <AE6DCE26-A528-35ED-BB3D-88890D27E6B9> /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_choice.data import ChoiceDataset, utils\n",
    "from torch_choice.model import ConditionalLogitModel\n",
    "\n",
    "# ======\n",
    "# delete import statements below.\n",
    "\n",
    "from torch_choice.utils.run_helper_lightning import LightningModelWrapper, section_print\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from torch_choice.data import ChoiceDataset\n",
    "from torch_choice.data.utils import create_data_loader\n",
    "from torch_choice.model.conditional_logit_model import ConditionalLogitModel\n",
    "from torch_choice.model.nested_logit_model import NestedLogitModel\n",
    "from torch_choice.utils.std import parameter_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to a separate file.\n",
    "def load_mode_canada_dataset():\n",
    "    df = pd.read_csv('./public_datasets/ModeCanada.csv')\n",
    "    df = df.query('noalt == 4').reset_index(drop=True)\n",
    "    df.sort_values(by='case', inplace=True)\n",
    "    item_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\n",
    "    item_names = ['air', 'bus', 'car', 'train']\n",
    "    num_items = 4\n",
    "    encoder = dict(zip(item_names, range(num_items)))\n",
    "    print(f\"{encoder=:}\")\n",
    "    item_index = item_index.map(lambda x: encoder[x])\n",
    "    item_index = torch.LongTensor(item_index)\n",
    "    print(f\"{item_index=:}\")\n",
    "    price_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt',\n",
    "                                    values=['cost', 'freq', 'ovt'])\n",
    "    print(f'{price_cost_freq_ovt.shape=:}')\n",
    "\n",
    "    price_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\n",
    "    print(f'{price_ivt.shape=:}')\n",
    "    \n",
    "    session_income = df.groupby('case')['income'].first()\n",
    "    session_income = torch.Tensor(session_income.values).view(-1, 1)\n",
    "    dataset = ChoiceDataset(item_index=item_index,\n",
    "                        price_cost_freq_ovt=price_cost_freq_ovt,\n",
    "                        session_income=session_income,\n",
    "                        price_ivt=price_ivt\n",
    "                        )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3}\n",
      "item_index=tensor([0, 0, 0,  ..., 2, 2, 2])\n",
      "price_cost_freq_ovt.shape=torch.Size([2779, 4, 3])\n",
      "price_ivt.shape=torch.Size([2779, 4, 1])\n",
      "No `session_index` is provided, assume each choice instance is in its own session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_mode_canada_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Use the `run()` Helper Function as Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(model: Union [ConditionalLogitModel, NestedLogitModel],\n",
    "        dataset_train: ChoiceDataset,\n",
    "        dataset_val: Optional[ChoiceDataset]=None,\n",
    "        dataset_test: Optional[ChoiceDataset]=None,\n",
    "        batch_size: int=-1,\n",
    "        learning_rate: float=0.01,\n",
    "        num_epochs: int=10,\n",
    "        num_workers: int=0,\n",
    "        device: Optional[str]=None,\n",
    "        **kwargs) -> Union[ConditionalLogitModel, NestedLogitModel]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (Union[ConditionalLogitModel, NestedLogitModel]): the constructed model.\n",
    "        dataset_train (ChoiceDataset): the dataset for training.\n",
    "        dataset_val (ChoiceDataset): an optional dataset for validation.\n",
    "        dataset_test (ChoiceDataset): an optional dataset for testing.\n",
    "        batch_size (int, optional): batch size for model training. Defaults to -1.\n",
    "        learning_rate (float, optional): learning rate for model training. Defaults to 0.01.\n",
    "        num_epochs (int, optional): maximum number of epochs for the training. Defaults to 10.\n",
    "        num_workers (int, optional): number of parallel workers for data loading. Defaults to 0.\n",
    "        device (Optional[str], optional): the device that trains the model, if None is specified, the function will\n",
    "            use the current device of the provided model. Defaults to None.\n",
    "        **kwargs: other keyword arguments for the pytorch lightning trainer, this is for users with experience in\n",
    "            pytorch lightning and wish to customize the training process.\n",
    "\n",
    "    Returns:\n",
    "        Union[ConditionalLogitModel, NestedLogitModel]: the trained model.\n",
    "    \"\"\"\n",
    "    # ==================================================================================================================\n",
    "    # Setup the lightning wrapper.\n",
    "    # ==================================================================================================================\n",
    "    lightning_model = LightningModelWrapper(model, learning_rate=learning_rate)\n",
    "    if device is None:\n",
    "        # infer from the model device.\n",
    "        device = model.device\n",
    "    # the cloned model will be used for standard error calculation later.\n",
    "    model_clone = deepcopy(model)\n",
    "    section_print('model received')\n",
    "    print(model)\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Prepare the data.\n",
    "    # ==================================================================================================================\n",
    "    # present a summary of datasets received.\n",
    "    section_print('data set received')\n",
    "    print('[Train dataset]', dataset_train)\n",
    "    print('[Validation dataset]', dataset_val)\n",
    "    print('[Test dataset]', dataset_test)\n",
    "\n",
    "    # create pytorch dataloader objects.\n",
    "    train_dataloader = create_data_loader(dataset_train.to(device), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        val_dataloader = create_data_loader(dataset_val.to(device), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    else:\n",
    "        val_dataloader = None\n",
    "\n",
    "    if dataset_test is not None:\n",
    "        test_dataloader = create_data_loader(dataset_test.to(device), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    else:\n",
    "        test_dataloader = None\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Training the model.\n",
    "    # ==================================================================================================================\n",
    "    # if the validation dataset is provided, do early stopping.\n",
    "    callbacks = list()\n",
    "    if val_dataloader is not None:\n",
    "        print(\"Validation dataset provided, do early stopping based on validation log-likelihood.\")\n",
    "        callbacks.append(EarlyStopping(monitor=\"val_ll\", mode=\"max\", patience=10, min_delta=0.001))\n",
    "    else:\n",
    "        print(\"No validation dataset provided, do early stopping based on training loss.\")\n",
    "        # TODO: figure out why early stopping isn't working.\n",
    "        callbacks.append(EarlyStopping(monitor=\"train_loss\", mode=\"min\", patience=10, min_delta=0.001))\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"auto\",\n",
    "                         devices=\"auto\",\n",
    "                         auto_lr_find=False,\n",
    "                         # gpus=1 if ('cuda' in str(model.device)) else 0,  # use GPU if the model is currently on the GPU.\n",
    "                         max_epochs=num_epochs,\n",
    "                         check_val_every_n_epoch=num_epochs // 100,\n",
    "                         log_every_n_steps=num_epochs // 100,\n",
    "                         callbacks=callbacks,\n",
    "                         **kwargs)\n",
    "    start_time = time.time()\n",
    "    trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    print(f'Time taken for training: {time.time() - start_time}')\n",
    "    if test_dataloader is not None:\n",
    "        trainer.test(lightning_model, test_dataloaders=test_dataloader)\n",
    "    else:\n",
    "        print('Skip testing, no test dataset is provided.')\n",
    "\n",
    "    # ====== get the standard error of the model ====== #\n",
    "    # current methods of computing standard deviation will corrupt the model, load weights into another model for returning.\n",
    "    state_dict = deepcopy(lightning_model.model.state_dict())\n",
    "    model_clone.load_state_dict(state_dict)\n",
    "\n",
    "    # get mean of estimation.\n",
    "    mean_dict = dict()\n",
    "    for k, v in lightning_model.model.named_parameters():\n",
    "        mean_dict[k] = v.clone()\n",
    "\n",
    "    # estimate the standard error of the model.\n",
    "    dataset_for_std = dataset_train.clone()\n",
    "\n",
    "    if isinstance(model, ConditionalLogitModel):\n",
    "        def nll_loss(model):\n",
    "            y_pred = model(dataset_for_std)\n",
    "            return F.cross_entropy(y_pred, dataset_for_std.item_index, reduction='sum')\n",
    "    elif isinstance(model, NestedLogitModel):\n",
    "        def nll_loss(model):\n",
    "            d = dataset_for_std[torch.arange(len(dataset_for_std))]\n",
    "            return model.negative_log_likelihood(d, d['item'].item_index)\n",
    "    std_dict = parameter_std(model_clone, nll_loss)\n",
    "\n",
    "    print('=' * 20, 'model results', '=' * 20)\n",
    "    report = list()\n",
    "    for coef_name, std in std_dict.items():\n",
    "        std = std.cpu().detach().numpy()\n",
    "        mean = mean_dict[coef_name].cpu().detach().numpy()\n",
    "        coef_name = coef_name.replace('coef_dict.', '').replace('.coef', '')\n",
    "        for i in range(mean.size):\n",
    "            report.append({'Coefficient': coef_name + f'_{i}',\n",
    "                           'Estimation': float(mean[i]),\n",
    "                           'Std. Err.': float(std[i])})\n",
    "    report = pd.DataFrame(report).set_index('Coefficient')\n",
    "    # print(f'Training Epochs: {num_epochs}\\n')\n",
    "    # print(f'Learning Rate: {learning_rate}\\n')\n",
    "    # print(f'Batch Size: {batch_size if batch_size != -1 else len(dataset_list[0])} out of {len(dataset_list[0])} observations in total in test set\\n')\n",
    "\n",
    "    lightning_model.model.to(device)\n",
    "    train_ll = - lightning_model.model.negative_log_likelihood(dataset_train, dataset_train.item_index).detach().item()\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        val_ll = - lightning_model.model.negative_log_likelihood(dataset_val, dataset_val.item_index).detach().item()\n",
    "    else:\n",
    "        val_ll = 'N/A'\n",
    "\n",
    "    if dataset_test is not None:\n",
    "        test_ll = - lightning_model.model.negative_log_likelihood(dataset_test, dataset_test.item_index).detach().item()\n",
    "    else:\n",
    "        test_ll = 'N/A'\n",
    "    print(f'Final Log-likelihood: [Training] {train_ll}, [Validation] {val_ll}, [Test] {test_ll}\\n')\n",
    "    print('Coefficients:\\n')\n",
    "    print(report.to_markdown())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | ConditionalLogitModel | 13    \n",
      "------------------------------------------------\n",
      "13        Trainable params\n",
      "0         Non-trainable params\n",
      "13        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== model received ====================\n",
      "ConditionalLogitModel(\n",
      "  (coef_dict): ModuleDict(\n",
      "    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=mps:0).\n",
      "    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=mps:0).\n",
      "    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=mps:0).\n",
      "    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=mps:0).\n",
      "  )\n",
      ")\n",
      "Conditional logistic discrete choice model, expects input features:\n",
      "\n",
      "X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\n",
      "X[session_income[item]] with 1 parameters, with item level variation.\n",
      "X[price_ivt[item-full]] with 1 parameters, with item-full level variation.\n",
      "X[intercept[item]] with 1 parameters, with item level variation.\n",
      "device=mps:0\n",
      "==================== data set received ====================\n",
      "[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=mps:0)\n",
      "[Validation dataset] None\n",
      "[Test dataset] None\n",
      "No validation dataset provided, do early stopping based on training loss.\n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 26.64it/s, loss=8.46e+03, v_num=30, train_loss=7.93e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 25.79it/s, loss=8.46e+03, v_num=30, train_loss=7.93e+3]\n",
      "Time taken for training: 22.78681492805481\n",
      "Skip testing, no test dataset is provided.\n",
      "==================== model results ====================\n",
      "Final Log-likelihood: [Training] -7899.5009765625, [Validation] N/A, [Test] N/A\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient                     |   Estimation |   Std. Err. |\n",
      "|:--------------------------------|-------------:|------------:|\n",
      "| price_cost_freq_ovt[constant]_0 |   -0.235216  |         nan |\n",
      "| price_cost_freq_ovt[constant]_1 |    0.357215  |         nan |\n",
      "| price_cost_freq_ovt[constant]_2 |   -0.109957  |         nan |\n",
      "| session_income[item]_0          |   -2.5607    |         nan |\n",
      "| session_income[item]_1          |   -0.0483948 |         nan |\n",
      "| session_income[item]_2          |   -0.179991  |         nan |\n",
      "| price_ivt[item-full]_0          |    0.290493  |         nan |\n",
      "| price_ivt[item-full]_1          |   -0.790084  |         nan |\n",
      "| price_ivt[item-full]_2          |   -0.0222478 |         nan |\n",
      "| price_ivt[item-full]_3          |   -0.0029005 |         nan |\n",
      "| intercept[item]_0               |    0.235873  |         nan |\n",
      "| intercept[item]_1               |   -3.54071   |         nan |\n",
      "| intercept[item]_2               |    8.71336   |         nan |\n"
     ]
    }
   ],
   "source": [
    "model = ConditionalLogitModel(coef_variation_dict={'price_cost_freq_ovt': 'constant',\n",
    "                                                   'session_income': 'item',\n",
    "                                                   'price_ivt': 'item-full',\n",
    "                                                   'intercept': 'item'},\n",
    "                              num_param_dict={'price_cost_freq_ovt': 3,\n",
    "                                              'session_income': 1,\n",
    "                                              'price_ivt': 1,\n",
    "                                              'intercept': 1},\n",
    "                              num_items=4)\n",
    "DEVICE = \"mps\"\n",
    "trained_model = run(model.to(DEVICE), dataset.to(DEVICE), learning_rate=0.03, num_epochs=1000, device=DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Have Full Control over the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
