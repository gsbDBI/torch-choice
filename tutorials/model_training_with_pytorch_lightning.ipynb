{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with PyTorch Lightning (Beta)\n",
    "\n",
    "> This document is currently in a minimal version without sufficient annotations. We will update it in the future.\n",
    "\n",
    "> Author: Tianyu Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <B3E58761-2785-34C6-A89B-F37110C88A05> /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <AE6DCE26-A528-35ED-BB3D-88890D27E6B9> /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_choice.data import ChoiceDataset, utils\n",
    "from torch_choice.model import ConditionalLogitModel\n",
    "\n",
    "from torch_choice.utils.run_helper_lightning import run as run_lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to a separate file.\n",
    "def load_mode_canada_dataset():\n",
    "    df = pd.read_csv('./public_datasets/ModeCanada.csv')\n",
    "    df = df.query('noalt == 4').reset_index(drop=True)\n",
    "    df.sort_values(by='case', inplace=True)\n",
    "    item_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\n",
    "    item_names = ['air', 'bus', 'car', 'train']\n",
    "    num_items = 4\n",
    "    encoder = dict(zip(item_names, range(num_items)))\n",
    "    print(f\"{encoder=:}\")\n",
    "    item_index = item_index.map(lambda x: encoder[x])\n",
    "    item_index = torch.LongTensor(item_index)\n",
    "    print(f\"{item_index=:}\")\n",
    "    price_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt', values=['cost', 'freq', 'ovt'])\n",
    "    print(f'{price_cost_freq_ovt.shape=:}')\n",
    "\n",
    "    price_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\n",
    "    print(f'{price_ivt.shape=:}')\n",
    "    \n",
    "    session_income = df.groupby('case')['income'].first()\n",
    "    session_income = torch.Tensor(session_income.values).view(-1, 1)\n",
    "    dataset = ChoiceDataset(item_index=item_index,\n",
    "                        price_cost_freq_ovt=price_cost_freq_ovt,\n",
    "                        session_income=session_income,\n",
    "                        price_ivt=price_ivt)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3}\n",
      "item_index=tensor([0, 0, 0,  ..., 2, 2, 2])\n",
      "price_cost_freq_ovt.shape=torch.Size([2779, 4, 3])\n",
      "price_ivt.shape=torch.Size([2779, 4, 1])\n",
      "No `session_index` is provided, assume each choice instance is in its own session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_mode_canada_dataset()\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Use the `run()` Helper Function as Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== model received ====================\n",
      "ConditionalLogitModel(\n",
      "  (coef_dict): ModuleDict(\n",
      "    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=mps:0).\n",
      "    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=mps:0).\n",
      "    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=mps:0).\n",
      "    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=mps:0).\n",
      "  )\n",
      ")\n",
      "Conditional logistic discrete choice model, expects input features:\n",
      "\n",
      "X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\n",
      "X[session_income[item]] with 1 parameters, with item level variation.\n",
      "X[price_ivt[item-full]] with 1 parameters, with item-full level variation.\n",
      "X[intercept[item]] with 1 parameters, with item level variation.\n",
      "device=mps:0\n",
      "==================== data set received ====================\n",
      "[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=mps:0)\n",
      "[Validation dataset] None\n",
      "[Test dataset] None\n",
      "No validation dataset provided, do early stopping based on training loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | ConditionalLogitModel | 13    \n",
      "------------------------------------------------\n",
      "13        Trainable params\n",
      "0         Non-trainable params\n",
      "13        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 43.75it/s, loss=7.11e+04, v_num=11, train_loss=3.01e+4]Epoch 00028: reducing learning rate of group 0 to 1.5000e-01.\n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 47.14it/s, loss=1.17e+04, v_num=11, train_loss=1.51e+4]Epoch 00174: reducing learning rate of group 0 to 7.5000e-02.\n",
      "Epoch 446: 100%|██████████| 1/1 [00:00<00:00, 43.68it/s, loss=2.25e+03, v_num=11, train_loss=2.07e+3]Epoch 00447: reducing learning rate of group 0 to 3.7500e-02.\n",
      "Epoch 661: 100%|██████████| 1/1 [00:00<00:00, 33.34it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 00662: reducing learning rate of group 0 to 1.8750e-02.\n",
      "Epoch 772: 100%|██████████| 1/1 [00:00<00:00, 41.95it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 00773: reducing learning rate of group 0 to 9.3750e-03.\n",
      "Epoch 883: 100%|██████████| 1/1 [00:00<00:00, 43.51it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 00884: reducing learning rate of group 0 to 4.6875e-03.\n",
      "Epoch 994: 100%|██████████| 1/1 [00:00<00:00, 40.22it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 00995: reducing learning rate of group 0 to 2.3437e-03.\n",
      "Epoch 1105: 100%|██████████| 1/1 [00:00<00:00, 44.07it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01106: reducing learning rate of group 0 to 1.1719e-03.\n",
      "Epoch 1216: 100%|██████████| 1/1 [00:00<00:00, 39.08it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01217: reducing learning rate of group 0 to 5.8594e-04.\n",
      "Epoch 1327: 100%|██████████| 1/1 [00:00<00:00, 42.00it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01328: reducing learning rate of group 0 to 2.9297e-04.\n",
      "Epoch 1438: 100%|██████████| 1/1 [00:00<00:00, 40.26it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01439: reducing learning rate of group 0 to 1.4648e-04.\n",
      "Epoch 1549: 100%|██████████| 1/1 [00:00<00:00, 38.13it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01550: reducing learning rate of group 0 to 7.3242e-05.\n",
      "Epoch 1660: 100%|██████████| 1/1 [00:00<00:00, 41.11it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01661: reducing learning rate of group 0 to 3.6621e-05.\n",
      "Epoch 1771: 100%|██████████| 1/1 [00:00<00:00, 29.90it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01772: reducing learning rate of group 0 to 1.8311e-05.\n",
      "Epoch 1882: 100%|██████████| 1/1 [00:00<00:00, 26.48it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01883: reducing learning rate of group 0 to 9.1553e-06.\n",
      "Epoch 1993: 100%|██████████| 1/1 [00:00<00:00, 33.48it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 01994: reducing learning rate of group 0 to 4.5776e-06.\n",
      "Epoch 2104: 100%|██████████| 1/1 [00:00<00:00, 40.61it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02105: reducing learning rate of group 0 to 2.2888e-06.\n",
      "Epoch 2215: 100%|██████████| 1/1 [00:00<00:00, 47.16it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02216: reducing learning rate of group 0 to 1.1444e-06.\n",
      "Epoch 2326: 100%|██████████| 1/1 [00:00<00:00, 41.68it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02327: reducing learning rate of group 0 to 5.7220e-07.\n",
      "Epoch 2437: 100%|██████████| 1/1 [00:00<00:00, 37.25it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02438: reducing learning rate of group 0 to 2.8610e-07.\n",
      "Epoch 2548: 100%|██████████| 1/1 [00:00<00:00, 42.46it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02549: reducing learning rate of group 0 to 1.4305e-07.\n",
      "Epoch 2659: 100%|██████████| 1/1 [00:00<00:00, 40.42it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02660: reducing learning rate of group 0 to 7.1526e-08.\n",
      "Epoch 2770: 100%|██████████| 1/1 [00:00<00:00, 40.56it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02771: reducing learning rate of group 0 to 3.5763e-08.\n",
      "Epoch 2881: 100%|██████████| 1/1 [00:00<00:00, 43.85it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]Epoch 02882: reducing learning rate of group 0 to 1.7881e-08.\n",
      "Epoch 4999: 100%|██████████| 1/1 [00:00<00:00, 44.68it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 1/1 [00:00<00:00, 41.91it/s, loss=1.88e+03, v_num=11, train_loss=1.88e+3]\n",
      "Time taken for training: 126.8951518535614\n",
      "Skip testing, no test dataset is provided.\n",
      "==================== model results ====================\n",
      "Final Log-likelihood: [Training] -1880.45166015625, [Validation] N/A, [Test] N/A\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient                     |   Estimation |   Std. Err. |\n",
      "|:--------------------------------|-------------:|------------:|\n",
      "| price_cost_freq_ovt[constant]_0 |  -0.0482738  |  0.00714503 |\n",
      "| price_cost_freq_ovt[constant]_1 |   0.0962115  |  0.00511947 |\n",
      "| price_cost_freq_ovt[constant]_2 |  -0.0441688  |  0.00325923 |\n",
      "| session_income[item]_0          |  -0.101072   |  0.0189843  |\n",
      "| session_income[item]_1          |  -0.0237517  |  0.00379524 |\n",
      "| session_income[item]_2          |  -0.0348146  |  0.00402362 |\n",
      "| price_ivt[item-full]_0          |   0.0587401  |  0.0101634  |\n",
      "| price_ivt[item-full]_1          |  -0.014254   |  0.00614856 |\n",
      "| price_ivt[item-full]_2          |  -0.00417209 |  0.00189329 |\n",
      "| price_ivt[item-full]_3          |  -0.00132935 |  0.00119452 |\n",
      "| intercept[item]_0               |   0.550331   |  1.37808    |\n",
      "| intercept[item]_1               |  -0.300745   |  0.695031   |\n",
      "| intercept[item]_2               |   1.58825    |  0.605094   |\n"
     ]
    }
   ],
   "source": [
    "model = ConditionalLogitModel(coef_variation_dict={'price_cost_freq_ovt': 'constant',\n",
    "                                                   'session_income': 'item',\n",
    "                                                   'price_ivt': 'item-full',\n",
    "                                                   'intercept': 'item'},\n",
    "                              num_param_dict={'price_cost_freq_ovt': 3,\n",
    "                                              'session_income': 1,\n",
    "                                              'price_ivt': 1,\n",
    "                                              'intercept': 1},\n",
    "                              num_items=4)\n",
    "DEVICE = \"mps\"\n",
    "trained_model = run_lightning(model.to(DEVICE), dataset.to(DEVICE), learning_rate=0.3, num_epochs=5000, device=DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Have Full Control over the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
