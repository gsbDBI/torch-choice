{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Optimization Algorithms\n",
    "**Author: Tianyu Du (tianyudu@stanford.edu)**\n",
    "\n",
    "**Update: May. 14, 2023**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import essential Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_choice.data import ChoiceDataset, utils\n",
    "from torch_choice.model import ConditionalLogitModel\n",
    "\n",
    "from torch_choice.utils.run_helper import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "torch.cuda.is_available()=True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(f\"{torch.cuda.is_available()=:}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will run both with and without graphic processing unit (GPU). However, our package is *much* faster with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device used: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device used: {torch.cuda.get_device_name()}')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Running tutorial on CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "We have included the `ModeCanada` dataset in our package, which is located at `./public_datasets/`.\n",
    "\n",
    "The `ModeCanada` dataset contains individuals' choice on traveling methods.\n",
    "\n",
    "The raw dataset is in a long-format, in which the `case` variable identifies each choice.\n",
    "Using the terminology mentioned in the data management tutorial, each choice is called a *purchasing record* (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as $B$.\n",
    "\n",
    "For example, the first four row below (with `case == 109`) corresponds to the first choice, the `alt` column lists all alternatives/items available.\n",
    "\n",
    "The `choice` column identifies which alternative/item is chosen. The second row in the data snapshot below, we have `choice == 1` and `alt == 'air'` for `case == 109`. This indicates the travelling mode chosen in `case = 109` was `air`.\n",
    "\n",
    "Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on cases when four alternatives were available by filtering `noalt == 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>case</th>\n",
       "      <th>alt</th>\n",
       "      <th>choice</th>\n",
       "      <th>dist</th>\n",
       "      <th>cost</th>\n",
       "      <th>ivt</th>\n",
       "      <th>ovt</th>\n",
       "      <th>freq</th>\n",
       "      <th>income</th>\n",
       "      <th>urban</th>\n",
       "      <th>noalt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304</td>\n",
       "      <td>109</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>58.25</td>\n",
       "      <td>215</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>109</td>\n",
       "      <td>air</td>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>142.80</td>\n",
       "      <td>56</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>109</td>\n",
       "      <td>bus</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>27.52</td>\n",
       "      <td>301</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307</td>\n",
       "      <td>109</td>\n",
       "      <td>car</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>71.63</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308</td>\n",
       "      <td>110</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>58.25</td>\n",
       "      <td>215</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  case    alt  choice  dist    cost  ivt  ovt  freq  income  \\\n",
       "0         304   109  train       0   377   58.25  215   74     4      45   \n",
       "1         305   109    air       1   377  142.80   56   85     9      45   \n",
       "2         306   109    bus       0   377   27.52  301   63     8      45   \n",
       "3         307   109    car       0   377   71.63  262    0     0      45   \n",
       "4         308   110  train       0   377   58.25  215   74     4      70   \n",
       "\n",
       "   urban  noalt  \n",
       "0      0      4  \n",
       "1      0      4  \n",
       "2      0      4  \n",
       "3      0      4  \n",
       "4      0      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./public_datasets/ModeCanada.csv')\n",
    "df = df.query('noalt == 4').reset_index(drop=True)\n",
    "df.sort_values(by='case', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 4 rows corresponding to each *purchasing record*, the length of the long-format data is $4 \\times B$.\n",
    "Please refer to the data management tutorial for notations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11116, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       air\n",
      "1       air\n",
      "2       air\n",
      "3       air\n",
      "4       air\n",
      "       ... \n",
      "2774    car\n",
      "2775    car\n",
      "2776    car\n",
      "2777    car\n",
      "2778    car\n",
      "Name: alt, Length: 2779, dtype: object\n"
     ]
    }
   ],
   "source": [
    "item_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\n",
    "print(item_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3}\n",
      "item_index=tensor([0, 0, 0,  ..., 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "item_names = ['air', 'bus', 'car', 'train']\n",
    "num_items = 4\n",
    "encoder = dict(zip(item_names, range(num_items)))\n",
    "print(f\"{encoder=:}\")\n",
    "item_index = item_index.map(lambda x: encoder[x])\n",
    "item_index = torch.LongTensor(item_index)\n",
    "print(f\"{item_index=:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_cost_freq_ovt.shape=torch.Size([2779, 4, 3])\n",
      "price_ivt.shape=torch.Size([2779, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "price_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt',\n",
    "                                    values=['cost', 'freq', 'ovt'])\n",
    "print(f'{price_cost_freq_ovt.shape=:}')\n",
    "\n",
    "price_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\n",
    "print(f'{price_ivt.shape=:}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_income.shape=torch.Size([2779, 1])\n"
     ]
    }
   ],
   "source": [
    "session_income = df.groupby('case')['income'].first()\n",
    "session_income = torch.Tensor(session_income.values).view(-1, 1)\n",
    "print(f'{session_income.shape=:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we put all tensors we created to a single `ChoiceDataset` object, and move the dataset to the appropriate device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No `session_index` is provided, assume each choice instance is in its own session.\n"
     ]
    }
   ],
   "source": [
    "dataset = ChoiceDataset(item_index=item_index,\n",
    "                        price_cost_freq_ovt=price_cost_freq_ovt,\n",
    "                        session_income=session_income,\n",
    "                        price_ivt=price_ivt\n",
    "                        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a template script for researchers to train the PyTorch-based model with minimal effort.\n",
    "The researcher only needs to initialize the dataset and the model, this training template comes with default\n",
    "hyper-parameters including batch size and learning rate. The researcher should experiment with different levels\n",
    "of hyper-parameter if the default setting doesn't converge well.\n",
    "\n",
    "This is a modified version of the original run_helper.py script, which is modified to work with PyTorch Lightning.\n",
    "\"\"\"\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Union\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from torch_choice.data import ChoiceDataset\n",
    "from torch_choice.data.utils import create_data_loader\n",
    "from torch_choice.model.conditional_logit_model import ConditionalLogitModel\n",
    "from torch_choice.model.nested_logit_model import NestedLogitModel\n",
    "from torch_choice.utils.std import parameter_std\n",
    "\n",
    "\n",
    "class LightningModelWrapper(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: Union [ConditionalLogitModel, NestedLogitModel],\n",
    "                 learning_rate: float,\n",
    "                 optimizer: str):\n",
    "        \"\"\"\n",
    "        The pytorch-lightning model wrapper for conditional and nested logit model.\n",
    "        Ideally, end users don't need to interact with this class. This wrapper will be called by the run() function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_class_string = optimizer\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.model)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_performance_dict(self, batch):\n",
    "        item_index = batch['item'].item_index if isinstance(self.model, NestedLogitModel) else batch.item_index\n",
    "        ll = - self.model.negative_log_likelihood(batch, item_index).detach().item()\n",
    "        return {'log_likelihood': ll}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        item_index = batch['item'].item_index if isinstance(self.model, NestedLogitModel) else batch.item_index\n",
    "        loss = self.model.loss(batch, item_index)\n",
    "        self.log('train_loss', loss, prog_bar=False, batch_size=len(batch))\n",
    "        # skip computing log-likelihood for training steps to speed up training.\n",
    "        # for key, val in self._get_performance_dict(batch).items():\n",
    "            # self.log('test_' + key, val, prog_bar=True, batch_size=len(batch))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        for key, val in self._get_performance_dict(batch).items():\n",
    "            self.log('val_' + key, val, prog_bar=False, batch_size=len(batch))\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        for key, val in self._get_performance_dict(batch).items():\n",
    "            self.log('test_' + key, val, prog_bar=False, batch_size=len(batch))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return getattr(torch.optim, self.optimizer_class_string)(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# def run_original(model, dataset, dataset_test=None, batch_size=-1, learning_rate=0.01, num_epochs=5000, report_frequency=None):\n",
    "#     \"\"\"All in one script for the model training and result presentation.\"\"\"\n",
    "#     if report_frequency is None:\n",
    "#         report_frequency = (num_epochs // 10)\n",
    "\n",
    "#     assert isinstance(model, ConditionalLogitModel) or isinstance(model, NestedLogitModel), \\\n",
    "#         f'A model of type {type(model)} is not supported by this runner.'\n",
    "#     model = deepcopy(model)  # do not modify the model outside.\n",
    "#     trained_model = deepcopy(model)  # create another copy for returning.\n",
    "#     print('=' * 20, 'received model', '=' * 20)\n",
    "#     print(model)\n",
    "#     print('=' * 20, 'received dataset', '=' * 20)\n",
    "#     print(dataset)\n",
    "#     print('=' * 20, 'training the model', '=' * 20)\n",
    "\n",
    "\n",
    "def section_print(input_text):\n",
    "    \"\"\"Helper function for printing\"\"\"\n",
    "    print('=' * 20, input_text, '=' * 20)\n",
    "\n",
    "\n",
    "def run(model: Union [ConditionalLogitModel, NestedLogitModel],\n",
    "        dataset_train: ChoiceDataset,\n",
    "        dataset_val: Optional[ChoiceDataset]=None,\n",
    "        dataset_test: Optional[ChoiceDataset]=None,\n",
    "        optimizer: str='Adam',\n",
    "        batch_size: int=-1,\n",
    "        learning_rate: float=0.01,\n",
    "        num_epochs: int=10,\n",
    "        num_workers: int=0,\n",
    "        device: Optional[str]=None,\n",
    "        **kwargs) -> Union[ConditionalLogitModel, NestedLogitModel]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (Union[ConditionalLogitModel, NestedLogitModel]): the constructed model.\n",
    "        dataset_train (ChoiceDataset): the dataset for training.\n",
    "        dataset_val (ChoiceDataset): an optional dataset for validation.\n",
    "        dataset_test (ChoiceDataset): an optional dataset for testing.\n",
    "        batch_size (int, optional): batch size for model training. Defaults to -1.\n",
    "        learning_rate (float, optional): learning rate for model training. Defaults to 0.01.\n",
    "        num_epochs (int, optional): number of epochs for the training. Defaults to 10.\n",
    "        num_workers (int, optional): number of parallel workers for data loading. Defaults to 0.\n",
    "        device (Optional[str], optional): the device that trains the model, if None is specified, the function will\n",
    "            use the current device of the provided model. Defaults to None.\n",
    "        **kwargs: other keyword arguments for the pytorch lightning trainer, this is for users with experience in\n",
    "            pytorch lightning and wish to customize the training process.\n",
    "\n",
    "    Returns:\n",
    "        Union[ConditionalLogitModel, NestedLogitModel]: the trained model.\n",
    "    \"\"\"\n",
    "    # ==================================================================================================================\n",
    "    # Setup the lightning wrapper.\n",
    "    # ==================================================================================================================\n",
    "    lightning_model = LightningModelWrapper(model, learning_rate=learning_rate, optimizer=optimizer)\n",
    "    if device is None:\n",
    "        # infer from the model device.\n",
    "        device = model.device\n",
    "    # the cloned model will be used for standard error calculation later.\n",
    "    model_clone = deepcopy(model)\n",
    "    section_print('model received')\n",
    "    print(model)\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Prepare the data.\n",
    "    # ==================================================================================================================\n",
    "    # present a summary of datasets received.\n",
    "    section_print('data set received')\n",
    "    print('[Train dataset]', dataset_train)\n",
    "    print('[Validation dataset]', dataset_val)\n",
    "    print('[Test dataset]', dataset_test)\n",
    "\n",
    "    # create pytorch dataloader objects.\n",
    "    train_dataloader = create_data_loader(dataset_train.to(device), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        val_dataloader = create_data_loader(dataset_val.to(device), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    else:\n",
    "        val_dataloader = None\n",
    "\n",
    "    if dataset_test is not None:\n",
    "        test_dataloader = create_data_loader(dataset_test.to(device), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    else:\n",
    "        test_dataloader = None\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # Training the model.\n",
    "    # ==================================================================================================================\n",
    "    # if the validation dataset is provided, do early stopping.\n",
    "    # callbacks = [EarlyStopping(monitor=\"val_ll\", mode=\"max\", patience=10, min_delta=0.001)] if val_dataloader is not None else []\n",
    "    callbacks = [EarlyStopping(monitor=\"val_ll\", mode=\"max\", patience=10, min_delta=0.001)] if val_dataloader is not None else []\n",
    "\n",
    "    trainer = pl.Trainer(accelerator='gpu' if ('cuda' in str(model.device)) else None,  # use GPU if the model is currently on the GPU.\n",
    "                         devices=\"auto\",\n",
    "                         max_epochs=num_epochs,\n",
    "                         check_val_every_n_epoch=num_epochs // 100,\n",
    "                         log_every_n_steps=num_epochs // 100,\n",
    "                         callbacks=callbacks,\n",
    "                         **kwargs)\n",
    "    start_time = time.time()\n",
    "    trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    print(f'Time taken for training: {time.time() - start_time}')\n",
    "    if test_dataloader is not None:\n",
    "        trainer.test(lightning_model, test_dataloaders=test_dataloader)\n",
    "    else:\n",
    "        print('Skip testing, no test dataset is provided.')\n",
    "\n",
    "    # ====== get the standard error of the model ====== #\n",
    "    # current methods of computing standard deviation will corrupt the model, load weights into another model for returning.\n",
    "    state_dict = deepcopy(lightning_model.model.state_dict())\n",
    "    model_clone.load_state_dict(state_dict)\n",
    "\n",
    "    # get mean of estimation.\n",
    "    mean_dict = dict()\n",
    "    for k, v in lightning_model.model.named_parameters():\n",
    "        mean_dict[k] = v.clone()\n",
    "\n",
    "    # estimate the standard error of the model.\n",
    "    dataset_for_std = dataset_train.clone()\n",
    "\n",
    "    if isinstance(model, ConditionalLogitModel):\n",
    "        def nll_loss(model):\n",
    "            y_pred = model(dataset_for_std)\n",
    "            return F.cross_entropy(y_pred, dataset_for_std.item_index, reduction='sum')\n",
    "    elif isinstance(model, NestedLogitModel):\n",
    "        def nll_loss(model):\n",
    "            d = dataset_for_std[torch.arange(len(dataset_for_std))]\n",
    "            return model.negative_log_likelihood(d, d['item'].item_index)\n",
    "    std_dict = parameter_std(model_clone, nll_loss)\n",
    "\n",
    "    print('=' * 20, 'model results', '=' * 20)\n",
    "    report = list()\n",
    "    for coef_name, std in std_dict.items():\n",
    "        std = std.cpu().detach().numpy()\n",
    "        mean = mean_dict[coef_name].cpu().detach().numpy()\n",
    "        coef_name = coef_name.replace('coef_dict.', '').replace('.coef', '')\n",
    "        for i in range(mean.size):\n",
    "            report.append({'Coefficient': coef_name + f'_{i}',\n",
    "                           'Estimation': float(mean[i]),\n",
    "                           'Std. Err.': float(std[i])})\n",
    "    report = pd.DataFrame(report).set_index('Coefficient')\n",
    "\n",
    "    # Compute z-value\n",
    "    report['z-value'] = report['Estimation'] / report['Std. Err.']\n",
    "\n",
    "    # Compute p-value (two tails).\n",
    "    report['Pr(>|z|)'] = (1 - norm.cdf(abs(report['z-value']))) * 2\n",
    "\n",
    "    # Compute significance stars\n",
    "    report['Significance'] = ''\n",
    "    report.loc[report['Pr(>|z|)'] < 0.001, 'Significance'] = '***'\n",
    "    report.loc[(report['Pr(>|z|)'] >= 0.001) & (report['Pr(>|z|)'] < 0.01), 'Significance'] = '**'\n",
    "    report.loc[(report['Pr(>|z|)'] >= 0.01) & (report['Pr(>|z|)'] < 0.05), 'Significance'] = '*'\n",
    "    \n",
    "    # print(f'Training Epochs: {num_epochs}\\n')\n",
    "    # print(f'Learning Rate: {learning_rate}\\n')\n",
    "    # print(f'Batch Size: {batch_size if batch_size != -1 else len(dataset_list[0])} out of {len(dataset_list[0])} observations in total in test set\\n')\n",
    "\n",
    "    lightning_model.model.to(device)\n",
    "    train_ll = - lightning_model.model.negative_log_likelihood(dataset_train, dataset_train.item_index).detach().item()\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        val_ll = - lightning_model.model.negative_log_likelihood(dataset_val, dataset_val.item_index).detach().item()\n",
    "    else:\n",
    "        val_ll = 'N/A'\n",
    "\n",
    "    if dataset_test is not None:\n",
    "        test_ll = - lightning_model.model.negative_log_likelihood(dataset_test, dataset_test.item_index).detach().item()\n",
    "    else:\n",
    "        test_ll = 'N/A'\n",
    "    print(f'Final Log-likelihood: [Training] {train_ll}, [Validation] {val_ll}, [Test] {test_ll}\\n')\n",
    "    print(report.to_markdown())\n",
    "    print(\"Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This is a template script for researchers to train the PyTorch-based model with minimal effort.\n",
    "# The researcher only needs to initialize the dataset and the model, this training template comes with default\n",
    "# hyper-parameters including batch size and learning rate. The researcher should experiment with different levels\n",
    "# of hyper-parameter if the default setting doesn't converge well.\n",
    "# \"\"\"\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from copy import deepcopy\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim\n",
    "# from torch_choice.data import utils as data_utils\n",
    "# from torch_choice.utils.std import parameter_std\n",
    "# from torch_choice.model.conditional_logit_model import ConditionalLogitModel\n",
    "# from torch_choice.model.nested_logit_model import NestedLogitModel\n",
    "\n",
    "\n",
    "# def run(model, dataset, dataset_test=None, batch_size=-1, learning_rate=0.01, num_epochs=5000, report_frequency=None, compute_std=True, return_final_training_log_likelihood=False, model_optimizer='Adam'):\n",
    "#     \"\"\"All in one script for the model training and result presentation.\"\"\"\n",
    "#     if report_frequency is None:\n",
    "#         report_frequency = (num_epochs // 10)\n",
    "\n",
    "#     assert isinstance(model, ConditionalLogitModel) or isinstance(model, NestedLogitModel), \\\n",
    "#         f'A model of type {type(model)} is not supported by this runner.'\n",
    "#     model = deepcopy(model)  # do not modify the model outside.\n",
    "#     trained_model = deepcopy(model)  # create another copy for returning.\n",
    "#     data_loader = data_utils.create_data_loader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     optimizer = {'SGD': torch.optim.SGD,\n",
    "#                  'Adagrad': torch.optim.Adagrad,\n",
    "#                  'Adadelta': torch.optim.Adadelta,\n",
    "#                  'Adam': torch.optim.Adam,\n",
    "#                  'LBFGS': torch.optim.LBFGS}[model_optimizer](model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     # optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "#     # optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "#     # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.7)\n",
    "#     # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n",
    "#     print('=' * 20, 'received model', '=' * 20)\n",
    "#     print(model)\n",
    "#     print('=' * 20, 'received dataset', '=' * 20)\n",
    "#     print(dataset)\n",
    "#     print('=' * 20, 'training the model', '=' * 20)\n",
    "\n",
    "#     total_loss_history = list()\n",
    "#     tol = 0.001  # stop if the loss failed to improve tol proportion of average performance in the last k iterations.\n",
    "#     k = 5\n",
    "#     # fit the model.\n",
    "#     for e in range(1, num_epochs + 1):\n",
    "#         # track the log-likelihood to minimize.\n",
    "#         ll, count, total_loss = 0.0, 0.0, 0.0\n",
    "#         for batch in data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             item_index = batch['item'].item_index if isinstance(model, NestedLogitModel) else batch.item_index\n",
    "#             # the model.loss returns negative log-likelihood + regularization term.\n",
    "#             loss = model.loss(batch, item_index)\n",
    "#             total_loss -= loss\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 if (e % report_frequency) == 0:\n",
    "#                     # record log-likelihood.\n",
    "#                     ll -= model.negative_log_likelihood(batch, item_index).detach().item() # * len(batch)\n",
    "#                     count += len(batch)\n",
    "\n",
    "#                     pred = model.forward(batch).argmax(dim=1)\n",
    "#                     acc = (pred == item_index).float().mean().item()\n",
    "#                     print('Accuracy: ', acc)\n",
    "\n",
    "#             loss.backward()\n",
    "            \n",
    "#             if model_optimizer == \"LBFGS\":\n",
    "#                 def closure():\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss = model.loss(batch, item_index)\n",
    "#                     loss.backward()\n",
    "#                     return loss\n",
    "#                 optimizer.step(closure)\n",
    "#             else:\n",
    "#                 optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         current_loss = float(total_loss.detach().item())\n",
    "#         # if e > k:\n",
    "#         if False:\n",
    "#             past_avg = np.mean(total_loss_history[-k:])\n",
    "#             improvement = (past_avg - current_loss) / past_avg\n",
    "#             if improvement < tol:\n",
    "#                 print(f'Early stopped at {e} epochs.')\n",
    "#                 break\n",
    "#         total_loss_history.append(current_loss)\n",
    "#         # ll /= count\n",
    "#         if (e % report_frequency) == 0:\n",
    "#             print(f'Epoch {e}: Log-likelihood={ll}')\n",
    "\n",
    "#     if dataset_test is not None:\n",
    "#         test_ll = - model.negative_log_likelihood(dataset_test, dataset_test.item_index).detach().item()\n",
    "#         print('Test set log-likelihood: ', test_ll)\n",
    "\n",
    "#     # final training log-likelihood.\n",
    "#     ll = - model.negative_log_likelihood(dataset, dataset.item_index).detach().item() # * len(batch)\n",
    "\n",
    "#     if not compute_std:\n",
    "#         if return_final_training_log_likelihood:\n",
    "#             return model, ll\n",
    "#         else:\n",
    "#             return model\n",
    "#     else:\n",
    "#         # current methods of computing standard deviation will corrupt the model, load weights into another model for returning.\n",
    "#         state_dict = deepcopy(model.state_dict())\n",
    "#         trained_model.load_state_dict(state_dict)\n",
    "\n",
    "#         # get mean of estimation.\n",
    "#         mean_dict = dict()\n",
    "#         for k, v in model.named_parameters():\n",
    "#             mean_dict[k] = v.clone()\n",
    "\n",
    "#         # estimate the standard error of the model.\n",
    "#         if isinstance(model, ConditionalLogitModel):\n",
    "#             def nll_loss(model):\n",
    "#                 y_pred = model(dataset)\n",
    "#                 return F.cross_entropy(y_pred, dataset.item_index, reduction='sum')\n",
    "#         elif isinstance(model, NestedLogitModel):\n",
    "#             def nll_loss(model):\n",
    "#                 d = dataset[torch.arange(len(dataset))]\n",
    "#                 return model.negative_log_likelihood(d, d['item'].item_index)\n",
    "\n",
    "#         std_dict = parameter_std(model, nll_loss)\n",
    "\n",
    "#         print('=' * 20, 'model results', '=' * 20)\n",
    "#         report = list()\n",
    "#         for coef_name, std in std_dict.items():\n",
    "#             std = std.cpu().detach().numpy()\n",
    "#             mean = mean_dict[coef_name].cpu().detach().numpy()\n",
    "#             coef_name = coef_name.replace('coef_dict.', '').replace('.coef', '')\n",
    "#             for i in range(mean.size):\n",
    "#                 report.append({'Coefficient': coef_name + f'_{i}',\n",
    "#                             'Estimation': float(mean[i]),\n",
    "#                             'Std. Err.': float(std[i])})\n",
    "#         report = pd.DataFrame(report).set_index('Coefficient')\n",
    "#         print(f'Training Epochs: stopped at {e}, maximum allowed: {num_epochs}\\n')\n",
    "#         print(f'Learning Rate: {learning_rate}\\n')\n",
    "#         print(f'Batch Size: {batch_size if batch_size != -1 else len(dataset)} out of {len(dataset)} observations in total\\n')\n",
    "#         print(f'Final Log-likelihood: {ll}\\n')\n",
    "#         print('Coefficients:\\n')\n",
    "#         print(report.to_markdown())\n",
    "#         if return_final_training_log_likelihood:\n",
    "#             return trained_model, ll\n",
    "#         else:\n",
    "#             return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | ConditionalLogitModel | 13    \n",
      "------------------------------------------------\n",
      "13        Trainable params\n",
      "0         Non-trainable params\n",
      "13        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== model received ====================\n",
      "ConditionalLogitModel(\n",
      "  (coef_dict): ModuleDict(\n",
      "    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n",
      "    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n",
      "    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n",
      "    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n",
      "  )\n",
      ")\n",
      "Conditional logistic discrete choice model, expects input features:\n",
      "\n",
      "X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\n",
      "X[session_income[item]] with 1 parameters, with item level variation.\n",
      "X[price_ivt[item-full]] with 1 parameters, with item-full level variation.\n",
      "X[intercept[item]] with 1 parameters, with item level variation.\n",
      "device=cuda:0\n",
      "==================== data set received ====================\n",
      "[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n",
      "[Validation dataset] None\n",
      "[Test dataset] None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyudu/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d4ce692420448ea2e64f338f204534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training: 20.58278727531433\n",
      "Skip testing, no test dataset is provided.\n",
      "==================== model results ====================\n",
      "Final Log-likelihood: [Training] -1874.3427734375, [Validation] N/A, [Test] N/A\n",
      "\n",
      "Coefficients:\n",
      "\n",
      "| Coefficient                     |   Estimation |   Std. Err. |    z-value |    Pr(>|z|) | Significance   |\n",
      "|:--------------------------------|-------------:|------------:|-----------:|------------:|:---------------|\n",
      "| price_cost_freq_ovt[constant]_0 |  -0.0333405  |  0.00709546 |  -4.69885  | 2.61628e-06 | ***            |\n",
      "| price_cost_freq_ovt[constant]_1 |   0.0925296  |  0.00509755 |  18.1518   | 0           | ***            |\n",
      "| price_cost_freq_ovt[constant]_2 |  -0.0430024  |  0.00322471 | -13.3353   | 0           | ***            |\n",
      "| session_income[item]_0          |  -0.0890684  |  0.0183464  |  -4.85482  | 1.20496e-06 | ***            |\n",
      "| session_income[item]_1          |  -0.0279925  |  0.00387252 |  -7.22848  | 4.88498e-13 | ***            |\n",
      "| session_income[item]_2          |  -0.0381456  |  0.00408306 |  -9.34241  | 0           | ***            |\n",
      "| price_ivt[item-full]_0          |   0.0595084  |  0.0100727  |   5.90789  | 3.46514e-09 | ***            |\n",
      "| price_ivt[item-full]_1          |  -0.0067793  |  0.00443223 |  -1.52954  | 0.12613     |                |\n",
      "| price_ivt[item-full]_2          |  -0.00646024 |  0.00189847 |  -3.40286  | 0.000666836 | ***            |\n",
      "| price_ivt[item-full]_3          |  -0.00145042 |  0.00118748 |  -1.22143  | 0.221922    |                |\n",
      "| intercept[item]_0               |   0.696585   |  1.28019    |   0.544128 | 0.586353    |                |\n",
      "| intercept[item]_1               |   1.84397    |  0.708506   |   2.60261  | 0.00925164  | **             |\n",
      "| intercept[item]_2               |   3.27395    |  0.624412   |   5.24325  | 1.57772e-07 | ***            |\n",
      "Significance codes: '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConditionalLogitModel(\n",
       "  (coef_dict): ModuleDict(\n",
       "    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n",
       "    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n",
       "    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n",
       "    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n",
       "  )\n",
       ")\n",
       "Conditional logistic discrete choice model, expects input features:\n",
       "\n",
       "X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\n",
       "X[session_income[item]] with 1 parameters, with item level variation.\n",
       "X[price_ivt[item-full]] with 1 parameters, with item-full level variation.\n",
       "X[intercept[item]] with 1 parameters, with item level variation.\n",
       "device=cuda:0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConditionalLogitModel(\n",
    "    formula='(price_cost_freq_ovt|constant) + (session_income|item) + (price_ivt|item-full) + (intercept|item)',\n",
    "    dataset=dataset,\n",
    "    num_items=4).to(device)\n",
    "# run(model, dataset, num_epochs=500, learning_rate=0.01, batch_size=-1, model_optimizer=\"LBFGS\")\n",
    "run(model, dataset, num_epochs=500, learning_rate=0.01, batch_size=-1, optimizer=\"LBFGS\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation from `R`\n",
    "The following is the R-output from the `mlogit` implementation, the estimation, standard error, and log-likelihood from our `torch_choice` implementation is the same as the result from `mlogit` implementation.\n",
    "\n",
    "We see that the final log-likelihood of models estimated using two packages are all around `-1874`.\n",
    "\n",
    "The `run()` method calculates the standard deviation using $\\sqrt{\\text{diag}(H^{-1})}$, where $H$ is the hessian of negative log-likelihood with repsect to model parameters.\n",
    "\n",
    "Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages.\n",
    "\n",
    "<!-- | Coefficient Name in Python |  Estimation |   Std. Err. |  Coeffcient Name in R | R Estimation | R Std. Err. | \n",
    "|:---------------------:|-------------:|------------:| :--------------: | ----------: | ------: |\n",
    "| price_cost_freq_ovt_0 |  -0.0342194  |  0.00731707 | cost             | -0.0333389  |0.0070955|\n",
    "| price_cost_freq_ovt_1 |   0.092262   |  0.00520946 | freq             |  0.0925297  |0.0050976|\n",
    "| price_cost_freq_ovt_2 |  -0.0439827  |  0.00342765 | ovt              | -0.0430036  |0.0032247|\n",
    "| session_income_0      |  -0.0901207  |  0.0205214  | income:bus       | -0.0890867  |0.0183471|\n",
    "| session_income_1      |  -0.0272581  |  0.00385396 | income:car       | -0.0279930  |0.0038726|\n",
    "| session_income_2      |  -0.0390468  |  0.00428838 | ivt:train        | -0.0014504  |0.0011875|\n",
    "| price_ivt_0           |   0.0592097  |  0.0102933  | ivt:air          |  0.0595097  |0.0100727|\n",
    "| price_ivt_1           |  -0.00753696 |  0.00496264 | ivt:bus          | -0.0067835  |0.0044334|\n",
    "| price_ivt_2           |  -0.00604297 |  0.00193414 | ivt:car          | -0.0064603  |0.0018985|\n",
    "| price_ivt_3           |  -0.00207518 |  0.00123286 | ivt:train        | -0.0014504  |0.0011875|\n",
    "| intercept_0           |   0.700786   |  1.39368    | (Intercept):bus  |  0.6983381  |1.2802466|\n",
    "| intercept_1           |   1.85016    |  0.728283   | (Intercept):car  |  1.8441129  |0.7085089|\n",
    "| intercept_2           |   3.2782     |  0.648064   | (Intercept):train|  3.2741952  |0.6244152| -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Output\n",
    "```r\n",
    "install.packages(\"mlogit\")\n",
    "library(\"mlogit\")\n",
    "data(\"ModeCanada\", package = \"mlogit\")\n",
    "MC <- dfidx(ModeCanada, subset = noalt == 4)\n",
    "ml.MC1 <- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air')\n",
    "\n",
    "summary(ml.MC1)\n",
    "```\n",
    "```\n",
    "Call:\n",
    "mlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, \n",
    "    reflevel = \"air\", method = \"nr\")\n",
    "\n",
    "Frequencies of alternatives:choice\n",
    "      air     train       bus       car \n",
    "0.3738755 0.1666067 0.0035984 0.4559194 \n",
    "\n",
    "nr method\n",
    "9 iterations, 0h:0m:0s \n",
    "g'(-H)^-1g = 0.00014 \n",
    "successive function values within tolerance limits \n",
    "\n",
    "Coefficients :\n",
    "                    Estimate Std. Error  z-value  Pr(>|z|)    \n",
    "(Intercept):train  3.2741952  0.6244152   5.2436 1.575e-07 ***\n",
    "(Intercept):bus    0.6983381  1.2802466   0.5455 0.5854292    \n",
    "(Intercept):car    1.8441129  0.7085089   2.6028 0.0092464 ** \n",
    "cost              -0.0333389  0.0070955  -4.6986 2.620e-06 ***\n",
    "freq               0.0925297  0.0050976  18.1517 < 2.2e-16 ***\n",
    "ovt               -0.0430036  0.0032247 -13.3356 < 2.2e-16 ***\n",
    "income:train      -0.0381466  0.0040831  -9.3426 < 2.2e-16 ***\n",
    "income:bus        -0.0890867  0.0183471  -4.8556 1.200e-06 ***\n",
    "income:car        -0.0279930  0.0038726  -7.2286 4.881e-13 ***\n",
    "ivt:air            0.0595097  0.0100727   5.9080 3.463e-09 ***\n",
    "ivt:train         -0.0014504  0.0011875  -1.2214 0.2219430    \n",
    "ivt:bus           -0.0067835  0.0044334  -1.5301 0.1259938    \n",
    "ivt:car           -0.0064603  0.0018985  -3.4029 0.0006668 ***\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Log-Likelihood: -1874.3\n",
    "McFadden R^2:  0.35443 \n",
    "Likelihood ratio test : chisq = 2058.1 (p.value = < 2.22e-16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5859d33511df864b0b7226a715510a0165ef032ed4b83eb4ae2c092f0788759c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
