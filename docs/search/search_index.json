{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torch-choice Authors: Tianyu Du and Ayush Kanodia; PI: Susan Athey; Contact: tianyudu@stanford.edu torch-choice is a flexible, fast choice modeling with PyTorch: logit and nested logit models, designed for both estimation and prediction. See the complete documentation for more details. Unique features: 1. GPU support via torch for speed 2. Specify customized models 3. Specify availability sets 4. Report standard errors Installation Clone the repository to your local machine or server. Install required dependencies using: pip3 install -r requirements.txt . Run pip3 install torch-choice . Check installation by running python3 -c 'import torch_choice; print(torch_choice.__version__)' . The installation page provides more details on installation. In this demonstration, we will guide you through a minimal example of fitting a conditional logit model using our package. We will be referencing to R code and Stata code as well to deliver a smooth knowledge transfer. Mode Canada Example In this demonstration, we will guide you through a minimal example of fitting a conditional logit model using our package. We will be referencing R code as well to deliver a smooth knowledge transfer. More information about the ModeCanada: Mode Choice for the Montreal-Toronto Corridor . Mode Canada with Torch-Choice # load packages. import pandas as pd import torch_choice # load data. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA' ) . query ( 'noalt == 4' ) . reset_index ( drop = True ) # format data. data = torch_choice . utils . easy_data_wrapper . EasyDatasetWrapper ( main_data = df , purchase_record_column = 'case' , choice_column = 'choice' , item_name_column = 'alt' , user_index_column = 'case' , session_index_column = 'case' , session_observable_columns = [ 'income' ], price_observable_columns = [ 'cost' , 'freq' , 'ovt' , 'ivt' ]) # define the conditional logit model. model = torch_choice . model . ConditionalLogitModel ( coef_variation_dict = { 'price_cost' : 'constant' , 'price_freq' : 'constant' , 'price_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_items = 4 ) # fit the conditional logit model. torch_choice . utils . run_helper . run ( model , data . choice_dataset , num_epochs = 5000 , learning_rate = 0.01 , batch_size =- 1 ) Mode Canada with R We include the R code for the ModeCanada example as well. # load packages. library(\"mlogit\") # load data. ModeCanada <- read.csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA') ModeCanada <- select(ModeCanada, -X) ModeCanada$alt <- as.factor(ModeCanada$alt) # format data. MC <- dfidx(ModeCanada, subset = noalt == 4) # fit the data. ml.MC1 <- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air') summary(ml.MC1) What's in the package? Overall, the torch-choice package offers the following features: The package includes a data management module called ChoiceDataset , which is built upon PyTorch's dataset module. Our dataset implementation allows users to easily move data between CPU and GPU. Unlike traditional long or wide formats, the ChoiceDataset offers a memory-efficient way to manage observables. The package provides a (1) conditional logit model and (2) a nested logit model for consumer choice modeling. The package leverage GPU acceleration using PyTorch and easily scale to large dataset of millions of choice records. All models are trained using state-of-the-art optimizers by in PyTorch. These optimization algorithms are tested to be scalable by modern machine learning practitioners. However, you can rest assure that the package runs flawlessly when no GPU is used as well. Setting up the PyTorch training pipelines can be frustrating. We provide easy-to-use PyTorch lightning wrapper of models to free researchers from the hassle from setting up PyTorch optimizers and training loops.","title":"Home"},{"location":"#torch-choice","text":"Authors: Tianyu Du and Ayush Kanodia; PI: Susan Athey; Contact: tianyudu@stanford.edu torch-choice is a flexible, fast choice modeling with PyTorch: logit and nested logit models, designed for both estimation and prediction. See the complete documentation for more details. Unique features: 1. GPU support via torch for speed 2. Specify customized models 3. Specify availability sets 4. Report standard errors","title":"torch-choice"},{"location":"#installation","text":"Clone the repository to your local machine or server. Install required dependencies using: pip3 install -r requirements.txt . Run pip3 install torch-choice . Check installation by running python3 -c 'import torch_choice; print(torch_choice.__version__)' . The installation page provides more details on installation. In this demonstration, we will guide you through a minimal example of fitting a conditional logit model using our package. We will be referencing to R code and Stata code as well to deliver a smooth knowledge transfer.","title":"Installation"},{"location":"#mode-canada-example","text":"In this demonstration, we will guide you through a minimal example of fitting a conditional logit model using our package. We will be referencing R code as well to deliver a smooth knowledge transfer. More information about the ModeCanada: Mode Choice for the Montreal-Toronto Corridor .","title":"Mode Canada Example"},{"location":"#mode-canada-with-torch-choice","text":"# load packages. import pandas as pd import torch_choice # load data. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA' ) . query ( 'noalt == 4' ) . reset_index ( drop = True ) # format data. data = torch_choice . utils . easy_data_wrapper . EasyDatasetWrapper ( main_data = df , purchase_record_column = 'case' , choice_column = 'choice' , item_name_column = 'alt' , user_index_column = 'case' , session_index_column = 'case' , session_observable_columns = [ 'income' ], price_observable_columns = [ 'cost' , 'freq' , 'ovt' , 'ivt' ]) # define the conditional logit model. model = torch_choice . model . ConditionalLogitModel ( coef_variation_dict = { 'price_cost' : 'constant' , 'price_freq' : 'constant' , 'price_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_items = 4 ) # fit the conditional logit model. torch_choice . utils . run_helper . run ( model , data . choice_dataset , num_epochs = 5000 , learning_rate = 0.01 , batch_size =- 1 )","title":"Mode Canada with Torch-Choice"},{"location":"#mode-canada-with-r","text":"We include the R code for the ModeCanada example as well. # load packages. library(\"mlogit\") # load data. ModeCanada <- read.csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA') ModeCanada <- select(ModeCanada, -X) ModeCanada$alt <- as.factor(ModeCanada$alt) # format data. MC <- dfidx(ModeCanada, subset = noalt == 4) # fit the data. ml.MC1 <- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air') summary(ml.MC1)","title":"Mode Canada with R"},{"location":"#whats-in-the-package","text":"Overall, the torch-choice package offers the following features: The package includes a data management module called ChoiceDataset , which is built upon PyTorch's dataset module. Our dataset implementation allows users to easily move data between CPU and GPU. Unlike traditional long or wide formats, the ChoiceDataset offers a memory-efficient way to manage observables. The package provides a (1) conditional logit model and (2) a nested logit model for consumer choice modeling. The package leverage GPU acceleration using PyTorch and easily scale to large dataset of millions of choice records. All models are trained using state-of-the-art optimizers by in PyTorch. These optimization algorithms are tested to be scalable by modern machine learning practitioners. However, you can rest assure that the package runs flawlessly when no GPU is used as well. Setting up the PyTorch training pipelines can be frustrating. We provide easy-to-use PyTorch lightning wrapper of models to free researchers from the hassle from setting up PyTorch optimizers and training loops.","title":"What's in the package?"},{"location":"api_torch_choice/","text":"API Reference: Torch Choice data special choice_dataset The dataset object for management large scale consumer choice datasets. Please refer to the documentation and tutorials for more details on using ChoiceDataset . Author: Tianyu Du Update: Apr. 27, 2022 ChoiceDataset ( Dataset ) Source code in torch_choice/data/choice_dataset.py class ChoiceDataset ( torch . utils . data . Dataset ): def __init__ ( self , item_index : torch . LongTensor , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset. def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) def __contains__ ( self , key : str ) -> bool : return key in self . keys def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag @property def device ( self ) -> str : \"\"\"Returns the device of the dataset. Returns: str: the device of the dataset. \"\"\" for attr in self . __dict__ . values (): if torch . is_tensor ( attr ): return attr . device @property def num_users ( self ) -> int : \"\"\"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: int: the number of users involved in this dataset. \"\"\" # query from user_index if self . user_index is not None : return len ( torch . unique ( self . user_index )) else : return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_user_attribute(key) or self._is_taste_attribute(key): # return val.shape[0] # return 1 @property def num_items ( self ) -> int : \"\"\"Returns the number of items involved in this dataset. Returns: int: the number of items involved in this dataset. \"\"\" return len ( torch . unique ( self . item_index )) # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_item_attribute(key): # return val.shape[0] # elif self._is_taste_attribute(key) or self._is_price_attribute(key): # return val.shape[1] # return 1 @property def num_sessions ( self ) -> int : \"\"\"Returns the number of sessions involved in this dataset. Returns: int: the number of sessions involved in this dataset. \"\"\" return len ( torch . unique ( self . session_index )) # if self.session_index is None: # return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_session_attribute(key) or self._is_price_attribute(key): # return val.shape[0] # return 1 @property def x_dict ( self ) -> Dict [ object , torch . Tensor ]: \"\"\"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. \"\"\" out = dict () for key , val in self . __dict__ . items (): if self . _is_attribute ( key ): # only include attributes. out [ key ] = self . _expand_tensor ( key , val ) # reshape to (num_sessions, num_items, num_params). return out @classmethod def _from_dict ( cls , dictionary : Dict [ str , torch . tensor ]) -> \"ChoiceDataset\" : \"\"\"Creates an instance of ChoiceDataset from a dictionary of arguments. Args: dictionary (Dict[str, torch.tensor]): a dictionary with keys as argument names and values as arguments. Returns: ChoiceDataset: the created copy of dataset. \"\"\" dataset = cls ( ** dictionary ) for key , item in dictionary . items (): setattr ( dataset , key , item ) return dataset def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) def _check_device_consistency ( self ) -> None : \"\"\"Checks if all tensors in this dataset are on the same device. Raises: Exception: an exception is raised if not all tensors are on the same device. \"\"\" # assert all tensors are on the same device. devices = list () for val in self . __dict__ . values (): if torch . is_tensor ( val ): devices . append ( val . device ) if len ( set ( devices )) > 1 : raise Exception ( f 'Found tensors on different devices: { set ( devices ) } .' , 'Use dataset.to() method to align devices.' ) def _size_repr ( self , value : object ) -> List [ int ]: \"\"\"A helper method to get the string-representation of object sizes, this is helpful while constructing the string representation of the dataset. Args: value (object): an object to examine its size. Returns: List[int]: list of integers representing the size of the object, length of the list is equal to dimension of `value`. \"\"\" if torch . is_tensor ( value ): return list ( value . size ()) elif isinstance ( value , int ) or isinstance ( value , float ): return [ 1 ] elif isinstance ( value , list ) or isinstance ( value , tuple ): return [ len ( value )] else : return [] def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items ()] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" # ================================================================================================================== # methods for checking attribute categories. # ================================================================================================================== @staticmethod def _is_item_attribute ( key : str ) -> bool : return key . startswith ( 'item_' ) and ( key != 'item_availability' ) and ( key != 'item_index' ) @staticmethod def _is_user_attribute ( key : str ) -> bool : return key . startswith ( 'user_' ) and ( key != 'user_index' ) @staticmethod def _is_session_attribute ( key : str ) -> bool : return key . startswith ( 'session_' ) and ( key != 'session_index' ) @staticmethod def _is_taste_attribute ( key : str ) -> bool : return key . startswith ( 'taste_' ) @staticmethod def _is_price_attribute ( key : str ) -> bool : return key . startswith ( 'price_' ) def _is_attribute ( self , key : str ) -> bool : return self . _is_item_attribute ( key ) \\ or self . _is_user_attribute ( key ) \\ or self . _is_session_attribute ( key ) \\ or self . _is_taste_attribute ( key ) \\ or self . _is_price_attribute ( key ) def _expand_tensor ( self , key : str , val : torch . Tensor ) -> torch . Tensor : \"\"\"Expands attribute tensor to (num_sessions, num_items, num_params) shape for prediction tasks, this method won't reshape the tensor at all if the `key` (i.e., name of the tensor) suggests its not an attribute of any kind. Args: key (str): name of the attribute used to determine the raw shape of the tensor. For example, 'item_obs' means the raw tensor is in shape (num_items, num_params). val (torch.Tensor): the attribute tensor to be reshaped. Returns: torch.Tensor: the reshaped tensor with shape (num_sessions, num_items, num_params). \"\"\" if not self . _is_attribute ( key ): print ( f 'Warning: the input key { key } is not an attribute of the dataset, will NOT modify the provided tensor.' ) # don't expand non-attribute tensors, if any. return val num_params = val . shape [ - 1 ] if self . _is_user_attribute ( key ): # user_attribute (num_users, *) out = val [ self . user_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_item_attribute ( key ): # item_attribute (num_items, *) out = val . view ( 1 , self . num_items , num_params ) . expand ( len ( self ), - 1 , - 1 ) elif self . _is_session_attribute ( key ): # session_attribute (num_sessions, *) out = val [ self . session_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_taste_attribute ( key ): # taste_attribute (num_users, num_items, *) out = val [ self . user_index , :, :] elif self . _is_price_attribute ( key ): # price_attribute (num_sessions, num_items, *) out = val [ self . session_index , :, :] assert out . shape == ( len ( self ), self . num_items , num_params ) return out device : str property readonly Returns the device of the dataset. Returns: Type Description str the device of the dataset. num_items : int property readonly Returns the number of items involved in this dataset. Returns: Type Description int the number of items involved in this dataset. num_sessions : int property readonly Returns the number of sessions involved in this dataset. Returns: Type Description int the number of sessions involved in this dataset. num_users : int property readonly Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: Type Description int the number of users involved in this dataset. x_dict : Dict [ object , torch . Tensor ] property readonly Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Type Description Dict[object, torch.Tensor] a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. __eq__ ( self , other ) special Returns whether all tensor attributes of both ChoiceDatasets are equal. Source code in torch_choice/data/choice_dataset.py def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag __getitem__ ( self , indices ) special Retrieves samples corresponding to the provided index or list of indices. Parameters: Name Type Description Default indices Union[int, torch.LongTensor] a single integer index or a tensor of indices. required Returns: Type Description ChoiceDataset a subset of the dataset. Source code in torch_choice/data/choice_dataset.py def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) __init__ ( self , item_index , label = None , user_index = None , session_index = None , item_availability = None , ** kwargs ) special Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called batch_size in the documentation. The batch_size corresponds to the file length in wide-format dataset, and often denoted using N . We call it batch_size to follow the convention in machine learning literature. A choice instance is a row of the dataset, so there are batch_size choice instances in each ChoiceDataset . The dataset consists of: (1) a collection of batch_size tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of observables associated with item, user, session, etc. Parameters: Name Type Description Default item_index torch.LongTensor a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the label tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. required label Optional[torch.LongTensor] a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the label argument as None in the initialization method, and the model will use item_index as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. None user_index Optional[torch.LongTensor] a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If None user index is provided, it's assumed that the choice instances are from the same user. user_index is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. None session_index Optional[torch.LongTensor] a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as None . In this case, the ChoiceDataset object will assume each choice instance to be in its own session. Defaults to None. None item_availability Optional[torch.BoolTensor] A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. None Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, ) 2. item observables must start with 'item_' and have shape (num_items, ) 3. session observables must start with 'session_' and have shape (num_sessions, ) 4. taste observables (those vary by user and item) must start with taste_ and have shape (num_users, num_items, ). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with price_ and have shape (num_sessions, num_items, *) Source code in torch_choice/data/choice_dataset.py def __init__ ( self , item_index : torch . LongTensor , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset. __len__ ( self ) special Returns number of samples in this dataset. Returns: Type Description int length of the dataset. Source code in torch_choice/data/choice_dataset.py def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) __repr__ ( self ) special A method to get a string representation of the dataset. Returns: Type Description str the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items ()] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" apply_tensor ( self , func ) This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Parameters: Name Type Description Default func callable a callable function to be applied on tensors and tensor-values of dictionaries. required Returns: Type Description ChoiceDataset the modified dataset. Source code in torch_choice/data/choice_dataset.py def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self clone ( self ) Creates a copy of self. Returns: Type Description ChoiceDataset a copy of self. Source code in torch_choice/data/choice_dataset.py def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) to ( self , device ) Moves all tensors in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union[str, torch.device] the destination device. required Returns: Type Description ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/choice_dataset.py def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) joint_dataset The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Author: Tianyu Du Update: Apr. 28, 2022 JointDataset ( Dataset ) A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Source code in torch_choice/data/joint_dataset.py class JointDataset ( torch . utils . data . Dataset ): \"\"\"A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. \"\"\" def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) @property def device ( self ) -> str : \"\"\"Returns the device of datasets contained in the joint dataset. Returns: str: the device of the dataset. \"\"\" for d in self . datasets . values (): return d . device def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self device : str property readonly Returns the device of datasets contained in the joint dataset. Returns: Type Description str the device of the dataset. __getitem__ ( self , indices ) special Queries samples from the dataset by index. Parameters: Name Type Description Default indices Union[int, torch.LongTensor] an integer or a 1D tensor of multiple indices. required Returns: Type Description Dict[str, ChoiceDataset] the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the datasets argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. Source code in torch_choice/data/joint_dataset.py def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) __init__ ( self , ** datasets ) special The initialize methods. Source code in torch_choice/data/joint_dataset.py def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 __len__ ( self ) special Get the number of samples in the joint dataset. Returns: Type Description int the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. Source code in torch_choice/data/joint_dataset.py def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) __repr__ ( self ) special A method to get a string representation of the dataset. Returns: Type Description str the string representation of the dataset. Source code in torch_choice/data/joint_dataset.py def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) to ( self , device ) Moves all datasets in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union[str, torch.device] the destination device. required Returns: Type Description ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/joint_dataset.py def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self utils pivot3d ( df , dim0 , dim1 , values ) Creates a tensor of shape (df[dim0].nunique(), df[dim1].nunique(), len(values)) from the provided data frame. Example, if dim0 is the column of session ID, dim1 is the column of alternative names, then out[t, i, k] is the feature values[k] of item i in session t. The returned tensor has shape (num_sessions, num_items, num_params), which fits the purpose of conditioanl logit models. Source code in torch_choice/data/utils.py def pivot3d ( df : pd . DataFrame , dim0 : str , dim1 : str , values : Union [ str , List [ str ]]) -> torch . Tensor : \"\"\" Creates a tensor of shape (df[dim0].nunique(), df[dim1].nunique(), len(values)) from the provided data frame. Example, if dim0 is the column of session ID, dim1 is the column of alternative names, then out[t, i, k] is the feature values[k] of item i in session t. The returned tensor has shape (num_sessions, num_items, num_params), which fits the purpose of conditioanl logit models. \"\"\" if not isinstance ( values , list ): values = [ values ] dim1_list = sorted ( df [ dim1 ] . unique ()) tensor_slice = list () for value in values : layer = df . pivot ( index = dim0 , columns = dim1 , values = value ) tensor_slice . append ( torch . Tensor ( layer [ dim1_list ] . values )) tensor = torch . stack ( tensor_slice , dim =- 1 ) assert tensor . shape == ( df [ dim0 ] . nunique (), df [ dim1 ] . nunique (), len ( values )) return tensor model special coefficient The general class of learnable coefficients in various models, this class serves as the building blocks for models in this package. The weights (i.e., learnable parameters) in the Coefficient class are implemented using PyTorch and can be trained directly using optimizers from PyTorch. NOTE: torch-choice package users don't interact with classes in this file directly, please use conditional_logit_model.py and nested_logit_model.py instead. Author: Tianyu Du Update: Apr. 28, 2022 Coefficient ( Module ) Source code in torch_choice/model/coefficient.py class Coefficient ( nn . Module ): def __init__ ( self , variation : str , num_params : int , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\"A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Args: variation (str): the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. num_params (int): number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called `theta_user`. The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, `num_params` should be set to `K`, NOT `K*U`. num_items (int): the number of items in the prediction problem, this is required to reshape the parameter correctly. num_users (Optional[int], optional): number of users, this is only necessary if the coefficient varies by users. Defaults to None. \"\"\" super ( Coefficient , self ) . __init__ () self . variation = variation self . num_items = num_items self . num_users = num_users self . num_params = num_params # construct the trainable. if self . variation == 'constant' : # constant for all users and items. self . coef = nn . Parameter ( torch . randn ( num_params ), requires_grad = True ) elif self . variation == 'item' : # coef depends on item j but not on user i. # force coefficients for the first item class to be zero. self . coef = nn . Parameter ( torch . zeros ( num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'item-full' : # coef depends on item j but not on user i. # model coefficient for every item. self . coef = nn . Parameter ( torch . zeros ( num_items , num_params ), requires_grad = True ) elif self . variation == 'user' : # coef depends on the user. # we always model coefficient for all users. self . coef = nn . Parameter ( torch . zeros ( num_users , num_params ), requires_grad = True ) elif self . variation == 'user-item' : # coefficients of the first item is forced to be zero, model coefficients for N - 1 items only. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'user-item-full' : # construct coefficients for every items. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items , num_params ), requires_grad = True ) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of the coefficient. Returns: str: the string representation of the coefficient. \"\"\" return f 'Coefficient(variation= { self . variation } , num_items= { self . num_items } ,' \\ + f ' num_users= { self . num_users } , num_params= { self . num_params } ,' \\ + f ' { self . coef . numel () } trainable parameters in total).' def forward ( self , x : torch . Tensor , user_index : Optional [ torch . Tensor ] = None , manual_coef_value : Optional [ torch . Tensor ] = None ) -> torch . Tensor : \"\"\" The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Args: x (torch.Tensor): a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. user_index (Optional[torch.Tensor], optional): a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. manual_coef_value (Optional[torch.Tensor], optional): a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. \"\"\" if manual_coef_value is not None : assert manual_coef_value . numel () == self . coef . numel () # plugin the provided coefficient values, coef is a tensor. coef = manual_coef_value . reshape ( * self . coef . shape ) else : # use the learned coefficient values, coef is a nn.Parameter. coef = self . coef num_trips , num_items , num_feats = x . shape assert self . num_params == num_feats # cast coefficient tensor to (num_trips, num_items, self.num_params). if self . variation == 'constant' : coef = coef . view ( 1 , 1 , self . num_params ) . expand ( num_trips , num_items , - 1 ) elif self . variation == 'item' : # coef has shape (num_items-1, num_params) # force coefficient for the first item to be zero. zeros = torch . zeros ( 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 0 ) # (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'item-full' : # coef has shape (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'user' : # coef has shape (num_users, num_params) coef = coef [ user_index , :] # (num_trips, num_params) user-specific coefficients. coef = coef . view ( num_trips , 1 , self . num_params ) . expand ( - 1 , num_items , - 1 ) elif self . variation == 'user-item' : # (num_trips,) long tensor of user ID. # originally, coef has shape (num_users, num_items-1, num_params) # transform to (num_trips, num_items - 1, num_params), user-specific. coef = coef [ user_index , :, :] # coefs for the first item for all users are enforced to 0. zeros = torch . zeros ( num_trips , 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 1 ) # (num_trips, num_items, num_params) elif self . variation == 'user-item-full' : # originally, coef has shape (num_users, num_items, num_params) coef = coef [ user_index , :, :] # (num_trips, num_items, num_params) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) assert coef . shape == ( num_trips , num_items , num_feats ) == x . shape # compute the utility of each item in each trip, take summation along the feature dimension, the same as taking # the inner product. return ( x * coef ) . sum ( dim =- 1 ) __init__ ( self , variation , num_params , num_items = None , num_users = None ) special A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Parameters: Name Type Description Default variation str the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. required num_params int number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called theta_user . The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, num_params should be set to K , NOT K*U . required num_items int the number of items in the prediction problem, this is required to reshape the parameter correctly. None num_users Optional[int] number of users, this is only necessary if the coefficient varies by users. Defaults to None. None Source code in torch_choice/model/coefficient.py def __init__ ( self , variation : str , num_params : int , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\"A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Args: variation (str): the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. num_params (int): number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called `theta_user`. The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, `num_params` should be set to `K`, NOT `K*U`. num_items (int): the number of items in the prediction problem, this is required to reshape the parameter correctly. num_users (Optional[int], optional): number of users, this is only necessary if the coefficient varies by users. Defaults to None. \"\"\" super ( Coefficient , self ) . __init__ () self . variation = variation self . num_items = num_items self . num_users = num_users self . num_params = num_params # construct the trainable. if self . variation == 'constant' : # constant for all users and items. self . coef = nn . Parameter ( torch . randn ( num_params ), requires_grad = True ) elif self . variation == 'item' : # coef depends on item j but not on user i. # force coefficients for the first item class to be zero. self . coef = nn . Parameter ( torch . zeros ( num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'item-full' : # coef depends on item j but not on user i. # model coefficient for every item. self . coef = nn . Parameter ( torch . zeros ( num_items , num_params ), requires_grad = True ) elif self . variation == 'user' : # coef depends on the user. # we always model coefficient for all users. self . coef = nn . Parameter ( torch . zeros ( num_users , num_params ), requires_grad = True ) elif self . variation == 'user-item' : # coefficients of the first item is forced to be zero, model coefficients for N - 1 items only. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'user-item-full' : # construct coefficients for every items. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items , num_params ), requires_grad = True ) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) __repr__ ( self ) special Returns a string representation of the coefficient. Returns: Type Description str the string representation of the coefficient. Source code in torch_choice/model/coefficient.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of the coefficient. Returns: str: the string representation of the coefficient. \"\"\" return f 'Coefficient(variation= { self . variation } , num_items= { self . num_items } ,' \\ + f ' num_users= { self . num_users } , num_params= { self . num_params } ,' \\ + f ' { self . coef . numel () } trainable parameters in total).' forward ( self , x , user_index = None , manual_coef_value = None ) The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Parameters: Name Type Description Default x torch.Tensor a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. required user_index Optional[torch.Tensor] a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. None manual_coef_value Optional[torch.Tensor] a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. None Returns: Type Description torch.Tensor a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. Source code in torch_choice/model/coefficient.py def forward ( self , x : torch . Tensor , user_index : Optional [ torch . Tensor ] = None , manual_coef_value : Optional [ torch . Tensor ] = None ) -> torch . Tensor : \"\"\" The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Args: x (torch.Tensor): a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. user_index (Optional[torch.Tensor], optional): a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. manual_coef_value (Optional[torch.Tensor], optional): a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. \"\"\" if manual_coef_value is not None : assert manual_coef_value . numel () == self . coef . numel () # plugin the provided coefficient values, coef is a tensor. coef = manual_coef_value . reshape ( * self . coef . shape ) else : # use the learned coefficient values, coef is a nn.Parameter. coef = self . coef num_trips , num_items , num_feats = x . shape assert self . num_params == num_feats # cast coefficient tensor to (num_trips, num_items, self.num_params). if self . variation == 'constant' : coef = coef . view ( 1 , 1 , self . num_params ) . expand ( num_trips , num_items , - 1 ) elif self . variation == 'item' : # coef has shape (num_items-1, num_params) # force coefficient for the first item to be zero. zeros = torch . zeros ( 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 0 ) # (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'item-full' : # coef has shape (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'user' : # coef has shape (num_users, num_params) coef = coef [ user_index , :] # (num_trips, num_params) user-specific coefficients. coef = coef . view ( num_trips , 1 , self . num_params ) . expand ( - 1 , num_items , - 1 ) elif self . variation == 'user-item' : # (num_trips,) long tensor of user ID. # originally, coef has shape (num_users, num_items-1, num_params) # transform to (num_trips, num_items - 1, num_params), user-specific. coef = coef [ user_index , :, :] # coefs for the first item for all users are enforced to 0. zeros = torch . zeros ( num_trips , 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 1 ) # (num_trips, num_items, num_params) elif self . variation == 'user-item-full' : # originally, coef has shape (num_users, num_items, num_params) coef = coef [ user_index , :, :] # (num_trips, num_items, num_params) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) assert coef . shape == ( num_trips , num_items , num_feats ) == x . shape # compute the utility of each item in each trip, take summation along the feature dimension, the same as taking # the inner product. return ( x * coef ) . sum ( dim =- 1 ) conditional_logit_model Conditional Logit Model. Author: Tianyu Du Date: Aug. 8, 2021 Update: Apr. 28, 2022 ConditionalLogitModel ( Module ) The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: !!! note \"unless the -full flag is specified (which means we want to explicitly model coefficients\" for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. constant: constant over all users and items, user: user-specific parameters but constant across all items, item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. item-full: item-specific parameters but constant across all users, explicitly model for all items. user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full: parameters that are specific to both user and item, explicitly model for all items. Source code in torch_choice/model/conditional_logit_model.py class ConditionalLogitModel ( nn . Module ): \"\"\"The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: NOTE: unless the `-full` flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. - constant: constant over all users and items, - user: user-specific parameters but constant across all items, - item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. - item-full: item-specific parameters but constant across all users, explicitly model for all items. - user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - user-item-full: parameters that are specific to both user and item, explicitly model for all items. \"\"\" def __init__ ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\" Args: num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. \"\"\" super ( ConditionalLogitModel , self ) . __init__ () if num_param_dict is None : num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () self . variable_types = list ( deepcopy ( num_param_dict ) . keys ()) self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. if 'intercept' in self . coef_variation_dict . keys () and 'intercept' not in self . num_param_dict . keys (): warnings . warn ( \"'intercept' key found in coef_variation_dict but not in num_param_dict, num_param_dict['intercept'] has been set to 1.\" ) self . num_param_dict [ 'intercept' ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict if 'intercept' in self . coef_variation_dict . keys (): # intercept term has no input tensor, which has only 1 feature. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): total_utility += coef ( x_dict [ var_type ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavilable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll # NOTE: the method for computing Hessian and standard deviation has been moved to std.py. # @staticmethod # def flatten_coef_dict(coef_dict: Dict[str, Union[torch.Tensor, torch.nn.Parameter]]) -> Tuple[torch.Tensor, dict]: # \"\"\"Flattens the coef_dict into a 1-dimension tensor, used for hessian computation. # Args: # coef_dict (Dict[str, Union[torch.Tensor, torch.nn.Parameter]]): a dictionary holding learnable parameters. # Returns: # Tuple[torch.Tensor, dict]: 1. the flattened tensors with shape (num_params,), 2. an indexing dictionary # used for reconstructing the original coef_dict from the flatten tensor. # \"\"\" # type2idx = dict() # param_list = list() # start = 0 # for var_type in coef_dict.keys(): # num_params = coef_dict[var_type].coef.numel() # # track which portion of all_param tensor belongs to this variable type. # type2idx[var_type] = (start, start + num_params) # start += num_params # # use reshape instead of view to make a copy. # param_list.append(coef_dict[var_type].coef.clone().reshape(-1,)) # all_param = torch.cat(param_list) # (self.num_params(), ) # return all_param, type2idx # @staticmethod # def unwrap_coef_dict(param: torch.Tensor, type2idx: Dict[str, Tuple[int, int]]) -> Dict[str, torch.Tensor]: # \"\"\"Rebuilds coef_dict from output of self.flatten_coef_dict method. # Args: # param (torch.Tensor): the flattened coef_dict from self.flatten_coef_dict. # type2idx (Dict[str, Tuple[int, int]]): the indexing dictionary from self.flatten_coef_dict. # Returns: # Dict[str, torch.Tensor]: the re-constructed coefficient dictionary. # \"\"\" # coef_dict = dict() # for var_type in type2idx.keys(): # start, end = type2idx[var_type] # # no need to reshape here, Coefficient handles it. # coef_dict[var_type] = param[start:end] # return coef_dict # def compute_hessian(self, x_dict, availability, user_index, y) -> torch.Tensor: # \"\"\"Computes the Hessian of negative log-likelihood (total cross-entropy loss) with respect # to all parameters in this model. The Hessian can be later used for constructing the standard deviation of # parameters. # Args: # x_dict ,availability, user_index: see definitions in self.forward method. # y (torch.LongTensor): a tensor with shape (num_trips,) of IDs of items actually purchased. # Returns: # torch.Tensor: a (self.num_params, self.num_params) tensor of the Hessian matrix. # \"\"\" # all_coefs, type2idx = self.flatten_coef_dict(self.coef_dict) # def compute_nll(P: torch.Tensor) -> float: # coef_dict = self.unwrap_coef_dict(P, type2idx) # y_pred = self._forward(x_dict=x_dict, # availability=availability, # user_index=user_index, # manual_coef_value_dict=coef_dict) # # the reduction needs to be 'sum' to obtain NLL. # loss = F.cross_entropy(y_pred, y, reduction='sum') # return loss # H = torch.autograd.functional.hessian(compute_nll, all_coefs) # assert H.shape == (self.num_params, self.num_params) # return H # def compute_std(self, x_dict, availability, user_index, y) -> Dict[str, torch.Tensor]: # \"\"\"Computes # Args:f # See definitions in self.compute_hessian. # Returns: # Dict[str, torch.Tensor]: a dictionary whose keys are the same as self.coef_dict.keys() # the values are standard errors of coefficients in each coefficient group. # \"\"\" # _, type2idx = self.flatten_coef_dict(self.coef_dict) # H = self.compute_hessian(x_dict, availability, user_index, y) # std_all = torch.sqrt(torch.diag(torch.inverse(H))) # std_dict = dict() # for var_type in type2idx.keys(): # # get std of variables belonging to each type. # start, end = type2idx[var_type] # std_dict[var_type] = std_all[start:end] # return std_dict num_params : int property readonly Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Type Description int the total number of learnable parameters. __init__ ( self , coef_variation_dict , num_param_dict = None , num_items = None , num_users = None ) special Parameters: Name Type Description Default num_items int number of items in the dataset. None num_users int number of users in the dataset. None coef_variation_dict Dict[str, str] variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with price_ , user_ , etc), or intercept if the researcher requires an intercept term. For each variable name X_var (e.g., user_income ) or intercept , the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. constant : the coefficient constant over all users and items: \\(X \beta\\) . user : user-specific parameters but constant across all items: \\(X \beta_{u}\\) . item : item-specific parameters but constant across all users, \\(X \beta_{i}\\) . Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. item-full : the same configuration as item , but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - user-item : parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full : parameters that are specific to both user and item, explicitly model for all items. required num_param_dict Optional[Dict[str, int]] variable type to number of parameters dictionary with keys exactly the same as the coef_variation_dict . Values of num_param_dict records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the coef_variation_dict dictionary and values of all ones. Default to be None. None Source code in torch_choice/model/conditional_logit_model.py def __init__ ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\" Args: num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. \"\"\" super ( ConditionalLogitModel , self ) . __init__ () if num_param_dict is None : num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () self . variable_types = list ( deepcopy ( num_param_dict ) . keys ()) self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. if 'intercept' in self . coef_variation_dict . keys () and 'intercept' not in self . num_param_dict . keys (): warnings . warn ( \"'intercept' key found in coef_variation_dict but not in num_param_dict, num_param_dict['intercept'] has been set to 1.\" ) self . num_param_dict [ 'intercept' ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) __repr__ ( self ) special Return a string representation of the model. Returns: Type Description str the string representation of the model. Source code in torch_choice/model/conditional_logit_model.py def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) forward ( self , batch , manual_coef_value_dict = None ) Forward pass of the model. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object. required manual_coef_value_dict Optional[Dict[str, torch.Tensor]] a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. None Returns: Type Description torch.Tensor a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. Source code in torch_choice/model/conditional_logit_model.py def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict if 'intercept' in self . coef_variation_dict . keys (): # intercept term has no input tensor, which has only 1 feature. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): total_utility += coef ( x_dict [ var_type ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavilable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility negative_log_likelihood ( self , batch , y , is_train = True ) Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data. required y torch.Tensor the label. required is_train bool whether to trace the gradient. Defaults to True. True Returns: Type Description torch.Tensor the negative log-likelihood. Source code in torch_choice/model/conditional_logit_model.py def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll summary ( self ) Print out the current model parameter. Source code in torch_choice/model/conditional_logit_model.py def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) nested_logit_model Implementation of the nested logit model, see page 86 of the book \"discrete choice methods with simulation\" by Train. for more details. Author: Tianyu Du Update; Apr. 28, 2022 NestedLogitModel ( Module ) Source code in torch_choice/model/nested_logit_model.py class NestedLogitModel ( nn . Module ): def __init__ ( self , category_to_item : Dict [ object , List [ int ]], category_coef_variation_dict : Dict [ str , str ], category_num_param_dict : Dict [ str , int ], item_coef_variation_dict : Dict [ str , str ], item_num_param_dict : Dict [ str , int ], num_users : Optional [ int ] = None , shared_lambda : bool = False ) -> None : \"\"\"Initialization method of the nested logit model. Args: category_to_item (Dict[object, List[int]]): a dictionary maps a category ID to a list of items IDs of the queried category. category_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. category_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as category_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as category_num_param_dict but for item features. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. \"\"\" super ( NestedLogitModel , self ) . __init__ () self . category_to_item = category_to_item self . category_coef_variation_dict = category_coef_variation_dict self . category_num_param_dict = category_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . categories = list ( category_to_item . keys ()) self . num_categories = len ( self . categories ) self . num_items = sum ( len ( items ) for items in category_to_item . values ()) # category coefficients. self . category_coef_dict = self . _build_coef_dict ( self . category_coef_variation_dict , self . category_num_param_dict , self . num_categories ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_categories ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def _build_coef_dict ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Dict [ str , int ], num_items : int ) -> nn . ModuleDict : \"\"\"Builds a coefficient dictionary containing all trainable components of the model, mapping coefficient names to the corresponding Coefficient Module. num_items could be the actual number of items or the number of categories depends on the use case. NOTE: torch-choice users don't directly interact with this method. Args: coef_variation_dict (Dict[str, str]): a dictionary mapping coefficient names (e.g., theta_user) to the level of variation (e.g., 'user'). num_param_dict (Dict[str, int]): a dictionary mapping coefficient names to the number of parameters in this coefficient. Be aware that, for example, if there is one K-dimensional coefficient for every user, then the `num_param` should be K instead of K x number of users. num_items (int): the total number of items in the prediction problem. `num_items` should be the number of categories if _build_coef_dict() is used for category-level prediction. Returns: nn.ModuleDict: a PyTorch ModuleDict object mapping from coefficient names to training Coefficient. \"\"\" coef_dict = dict () for var_type , variation in coef_variation_dict . items (): num_params = num_param_dict [ var_type ] coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = num_items , num_users = self . num_users , num_params = num_params ) return nn . ModuleDict ( coef_dict ) # def _check_input_shapes(self, category_x_dict, item_x_dict, user_index, item_availability) -> None: # T = list(category_x_dict.values())[0].shape[0] # batch size. # for var_type, x_category in category_x_dict.items(): # x_item = item_x_dict[var_type] # assert len(x_item.shape) == len(x_item.shape) == 3 # assert x_category.shape[0] == x_item.shape[0] # assert x_category.shape == (T, self.num_categories, self.category_num_param_dict[var_type]) # assert x_item.shape == (T, self.num_items, self.item_num_param_dict[var_type]) # if (user_index is not None) and (self.num_users is not None): # assert user_index.shape == (T,) # if item_availability is not None: # assert item_availability.shape == (T, self.num_items) def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'category' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) def _forward ( self , category_x_dict : Dict [ str , torch . Tensor ], item_x_dict : Dict [ str , torch . Tensor ], user_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None ) -> torch . Tensor : \"\"\"\"Computes log P[t, i] = the log probability for the user involved in trip t to choose item i. Let n denote the ID of the user involved in trip t, then P[t, i] = P_{ni} on page 86 of the book \"discrete choice methods with simulation\" by Train. Args: x_category (torch.Tensor): a tensor with shape (num_trips, num_categories, *) including features of all categories in each trip. x_item (torch.Tensor): a tensor with shape (num_trips, num_items, *) including features of all items in each trip. user_index (torch.LongTensor): a tensor of shape (num_trips,) indicating which user is making decision in each trip. Setting user_index = None assumes the same user is making decisions in all trips. item_availability (torch.BoolTensor): a boolean tensor with shape (num_trips, num_items) indicating the aviliability of items in each trip. If item_availability[t, i] = False, the utility of choosing item i in trip t, V[t, i], will be set to -inf. Given the decomposition V[t, i] = W[t, k(i)] + Y[t, i] + eps, V[t, i] is set to -inf by setting Y[t, i] = -inf for unavilable items. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" if self . shared_lambda : self . lambdas = self . lambda_weight . expand ( self . num_categories ) else : self . lambdas = self . lambda_weight # if not self._clamp_called_flag: # warnings.warn('Did you forget to call clamp_lambdas() after optimizer.step()?') # The overall utility of item can be decomposed into V[item] = W[category] + Y[item] + eps. T = list ( item_x_dict . values ())[ 0 ] . shape [ 0 ] device = list ( item_x_dict . values ())[ 0 ] . device # compute category-specific utility with shape (T, num_categories). W = torch . zeros ( T , self . num_categories ) . to ( device ) if 'intercept' in self . category_coef_variation_dict . keys (): category_x_dict [ 'intercept' ] = torch . ones (( T , self . num_categories , 1 )) . to ( device ) for var_type , coef in self . category_coef_dict . items (): W += coef ( category_x_dict [ var_type ], user_index ) # compute item-specific utility (T, num_items). Y = torch . zeros ( T , self . num_items ) . to ( device ) for var_type , coef in self . item_coef_dict . items (): Y += coef ( item_x_dict [ var_type ], user_index ) if item_availability is not None : Y [ ~ item_availability ] = torch . finfo ( Y . dtype ) . min / 2 # ============================================================================= # compute the inclusive value of each category. inclusive_value = dict () for k , Bk in self . category_to_item . items (): # for nest k, divide the Y of all items in Bk by lambda_k. Y [:, Bk ] /= self . lambdas [ k ] # compute inclusive value for category k. # mask out unavilable items. inclusive_value [ k ] = torch . logsumexp ( Y [:, Bk ], dim = 1 , keepdim = False ) # (T,) # boardcast inclusive value from (T, num_categories) to (T, num_items). # for trip t, I[t, i] is the inclusive value of the category item i belongs to. I = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . category_to_item . items (): I [:, Bk ] = inclusive_value [ k ] . view ( - 1 , 1 ) # (T, |Bk|) # logP_item[t, i] = log P(ni|Bk), where Bk is the category item i is in, n is the user in trip t. logP_item = Y - I # (T, num_items) # ============================================================================= # logP_category[t, i] = log P(Bk), for item i in trip t, the probability of choosing the nest/bucket # item i belongs to. logP_category has shape (T, num_items) # logit[t, i] = W[n, k] + lambda[k] I[n, k], where n is the user involved in trip t, k is # the category item i belongs to. logit = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . category_to_item . items (): logit [:, Bk ] = ( W [:, k ] + self . lambdas [ k ] * inclusive_value [ k ]) . view ( - 1 , 1 ) # (T, |Bk|) # only count each category once in the logsumexp within the category level model. cols = [ x [ 0 ] for x in self . category_to_item . values ()] logP_category = logit - torch . logsumexp ( logit [:, cols ], dim = 1 , keepdim = True ) # ============================================================================= # compute the joint log P_{ni} as in the textbook. logP = logP_item + logP_category self . _clamp_called_flag = False return logP def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll # def clamp_lambdas(self): # \"\"\" # Restrict values of lambdas to 0 < lambda <= 1 to guarantee the utility maximization property # of the model. # This method should be called everytime after optimizer.step(). # We add a self_clamp_called_flag to remind researchers if this method is not called. # \"\"\" # for k in range(len(self.lambdas)): # self.lambdas[k] = torch.clamp(self.lambdas[k], 1e-5, 1) # self._clam_called_flag = True # @staticmethod # def add_constant(x: torch.Tensor, where: str='prepend') -> torch.Tensor: # \"\"\"A helper function used to add constant to feature tensor, # x has shape (batch_size, num_classes, num_parameters), # returns a tensor of shape (*, num_parameters+1). # \"\"\" # batch_size, num_classes, num_parameters = x.shape # ones = torch.ones((batch_size, num_classes, 1)) # if where == 'prepend': # new = torch.cat((ones, x), dim=-1) # elif where == 'append': # new = torch.cat((x, ones), dim=-1) # else: # raise Exception # return new num_params : int property readonly Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Type Description int the total number of learnable parameters. __init__ ( self , category_to_item , category_coef_variation_dict , category_num_param_dict , item_coef_variation_dict , item_num_param_dict , num_users = None , shared_lambda = False ) special Initialization method of the nested logit model. Parameters: Name Type Description Default category_to_item Dict[object, List[int]] a dictionary maps a category ID to a list of items IDs of the queried category. required category_coef_variation_dict Dict[str, str] a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. required category_num_param_dict Dict[str, int] a dictionary maps a variable type name to the number of parameters in this variable group. required item_coef_variation_dict Dict[str, str] the same as category_coef_variation_dict but for item features. required item_num_param_dict Dict[str, int] the same as category_num_param_dict but for item features. required num_users Optional[int] number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. None shared_lambda bool a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. False Source code in torch_choice/model/nested_logit_model.py def __init__ ( self , category_to_item : Dict [ object , List [ int ]], category_coef_variation_dict : Dict [ str , str ], category_num_param_dict : Dict [ str , int ], item_coef_variation_dict : Dict [ str , str ], item_num_param_dict : Dict [ str , int ], num_users : Optional [ int ] = None , shared_lambda : bool = False ) -> None : \"\"\"Initialization method of the nested logit model. Args: category_to_item (Dict[object, List[int]]): a dictionary maps a category ID to a list of items IDs of the queried category. category_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. category_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as category_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as category_num_param_dict but for item features. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. \"\"\" super ( NestedLogitModel , self ) . __init__ () self . category_to_item = category_to_item self . category_coef_variation_dict = category_coef_variation_dict self . category_num_param_dict = category_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . categories = list ( category_to_item . keys ()) self . num_categories = len ( self . categories ) self . num_items = sum ( len ( items ) for items in category_to_item . values ()) # category coefficients. self . category_coef_dict = self . _build_coef_dict ( self . category_coef_variation_dict , self . category_num_param_dict , self . num_categories ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_categories ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True forward ( self , batch ) An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data batch. required Returns: Type Description torch.Tensor a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. Source code in torch_choice/model/nested_logit_model.py def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'category' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) log_likelihood ( self , * args ) Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: Type Description _type_ the log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) negative_log_likelihood ( self , batch , y , is_train = True ) Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Parameters: Name Type Description Default batch ChoiceDataset the ChoiceDataset object containing the data. required y torch.LongTensor the label. required is_train bool which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, is_train should be set to True. If we merely need a performance metric, then is_train can be set to False for better performance. Defaults to True. True Returns: Type Description torch.scalar_tensor the negative log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll utils special easy_data_wrapper This is a helper class for creating ChoiceDataset class, we only assume very basic python knowledge to use this utility. EasyDatasetWrapper An easy-to-use interface for creating ChoiceDataset object, please refer to the doc-string of the __init__ method for more details. You feed it with a couple of pandas data-frames and necessary information, this EasyDatasetWrapper would create the ChoiceDataset object for you. Source code in torch_choice/utils/easy_data_wrapper.py class EasyDatasetWrapper (): \"\"\"An easy-to-use interface for creating ChoiceDataset object, please refer to the doc-string of the `__init__` method for more details. You feed it with a couple of pandas data-frames and necessary information, this EasyDatasetWrapper would create the ChoiceDataset object for you. \"\"\" def __init__ ( self , main_data : pd . DataFrame , purchase_record_column : str , item_name_column : str , choice_column : str , user_index_column : Optional [ str ] = None , session_index_column : Optional [ str ] = None , # Option 1: feed in data-frames of observables. user_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , session_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , price_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , # Option 2: derive observables from columns of main_data. user_observable_columns : Optional [ List [ str ]] = None , item_observable_columns : Optional [ List [ str ]] = None , session_observable_columns : Optional [ List [ str ]] = None , price_observable_columns : Optional [ List [ str ]] = None , device : str = 'cpu' ): \"\"\"The initialization method of EasyDatasetWrapper. Args: main_data (pd.DataFrame): the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: purchase_record_column (str): the column in main_data identifies the index of purchasing records. item_name_column (str): the column in main_data identifies the name of items. choice_column (str): the column in the main_data identifies the bought item, for all rows with the same value of `purchase_record_column`, there should be exactly one row with `choice_column` equal to 1, all other rows should be 0. user_index_column (Optional[str], optional): an optional column indicating the user in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. session_index_column (Optional[str], optional): an optional column indicating the session in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. The keys of all of *_observable_data are the name of the observable data. user_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `user_index_column` and consisting of values from `main_data[user_index_column]`. Defaults to dict(). item_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). session_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `session_index_column` and consisting of values from `main_data[session_index_column]`. Defaults to dict(). price_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of `session_index_column` and consists of values from `main_data[session_index_column]` and (2) a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). Another method to include observables is via *_observable_columns keywords, which takes column name(s) of the main_data data-frame. The data wrapper will derive observable data from the main_data data-frame. For example, with `user_observable_columns = ['feature_A', 'feature_B']`, this wrapper will create two user-specific observable tensors derived from main_data['feature_A'] and main_data['feature_B']. # format (str, optional): the input format of the dataset. Defaults to 'stata'. Raises: ValueError: _description_ \"\"\" # read in data. self . main_data = main_data self . purchase_record_column = purchase_record_column # in alphabetical order of purchase record indices. # this is kept internally. self . purchase_record_index = main_data [ purchase_record_column ] . unique () self . item_name_column = item_name_column self . choice_column = choice_column self . user_index_column = user_index_column self . session_index_column = session_index_column self . device = device # encode item name, user index, session index. self . encode () # re-format observable data-frames and set correct index. self . align_observable_data ( item_observable_data , user_observable_data , session_observable_data , price_observable_data ) # derive observables from columns of the main data-frame. self . derive_observable_from_main_data ( item_observable_columns , user_observable_columns , session_observable_columns , price_observable_columns ) self . observable_data_to_observable_tensors () self . create_choice_dataset () print ( 'Finished Creating Choice Dataset.' ) def encode ( self ) -> None : \"\"\" Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. \"\"\" # encode item names. self . item_name_encoder = LabelEncoder () . fit ( self . main_data [ self . item_name_column ] . unique ()) # encode user indices. if self . user_index_column is not None : self . user_name_encoder = LabelEncoder () . fit ( self . main_data [ self . user_index_column ] . unique ()) # encode session indices. if self . session_index_column is not None : self . session_name_encoder = LabelEncoder () . fit ( self . main_data [ self . session_index_column ] . unique ()) def align_observable_data ( self , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]], user_observable_data : Optional [ Dict [ str , pd . DataFrame ]], session_observable_data : Optional [ Dict [ str , pd . DataFrame ]], price_observable_data : Optional [ Dict [ str , pd . DataFrame ]]) -> None : \"\"\"This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Args: item_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ user_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ session_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ price_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ \"\"\" self . item_observable_data = dict () if item_observable_data is not None : for key , val in item_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided item observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ], f \"item { item } is not in the { self . item_name_column } column of the item observable data-frame.\" self . item_observable_data [ 'item_' + key ] = val . set_index ( self . item_name_column ) . loc [ self . item_name_encoder . classes_ ] self . user_observable_data = dict () if user_observable_data is not None : for key , val in user_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . user_index_column is not None , \"user observable data is provided but user index column is not provided.\" assert self . user_index_column in val . columns , f \" { self . user_index_column } is not a column of provided user observable data-frame.\" for user in self . user_name_encoder . classes_ : assert user in val [ self . user_index_column ] . values , f \"user { user } is not in the { self . user_index_column } column of the user observable data-frame.\" self . user_observable_data [ 'user_' + key ] = val . set_index ( self . user_index_column ) . loc [ self . user_name_encoder . classes_ ] self . session_observable_data = dict () if session_observable_data is not None : for key , val in session_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"session observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided session observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the session observable data-frame.\" self . session_observable_data [ 'session_' + key ] = val . set_index ( self . session_index_column ) . loc [ self . session_name_encoder . classes_ ] self . price_observable_data = dict () if price_observable_data is not None : for key , val in price_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"price observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided price observable data-frame.\" assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided price observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the price observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ] . values , f \"item { item } is not in the { self . item_name_column } column of the price observable data-frame.\" # need to re-index since some (session, item) pairs are allowed to be unavailable. # complete index = Cartesian product of all sessions and all items. complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + key ] = val . set_index ([ self . session_index_column , self . item_name_column ]) . reindex ( complete_index ) def derive_observable_from_main_data ( self , item_observable_columns : Optional [ List [ str ]], user_observable_columns : Optional [ List [ str ]], session_observable_columns : Optional [ List [ str ]], price_observable_columns : Optional [ List [ str ]]) -> None : \"\"\" Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. \"\"\" if item_observable_columns is not None : for obs_col in item_observable_columns : # get the value of `obs_col` for each item. # note: values in self.main_data[self.item_name_column] are NOT encoded, they are raw values. self . item_observable_data [ 'item_' + obs_col ] = self . main_data . groupby ( self . item_name_column ) . first ()[[ obs_col ]] . loc [ self . item_name_encoder . classes_ ] if user_observable_columns is not None : for obs_col in user_observable_columns : # TODO: move to sanity check part. assert self . user_index_column is not None , \"user observable data is required but user index column is not provided.\" self . user_observable_data [ 'user_' + obs_col ] = self . main_data . groupby ( self . user_index_column ) . first ()[[ obs_col ]] . loc [ self . user_name_encoder . classes_ ] if session_observable_columns is not None : for obs_col in session_observable_columns : self . session_observable_data [ 'session_' + obs_col ] = self . main_data . groupby ( self . session_index_column ) . first ()[[ obs_col ]] . loc [ self . session_name_encoder . classes_ ] if price_observable_columns is not None : for obs_col in price_observable_columns : val = self . main_data . groupby ([ self . session_index_column , self . item_name_column ]) . first ()[[ obs_col ]] complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + obs_col ] = val . reindex ( complete_index ) def observable_data_to_observable_tensors ( self ) -> None : \"\"\"Convert all self.*_observable_data to self.*_observable_tensors for PyTorch.\"\"\" self . item_observable_tensors = dict () for key , val in self . item_observable_data . items (): assert all ( val . index == self . item_name_encoder . classes_ ), \"item observable data is not alighted with user name encoder.\" self . item_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . user_observable_tensors = dict () for key , val in self . user_observable_data . items (): assert all ( val . index == self . user_name_encoder . classes_ ), \"user observable data is not alighted with user name encoder.\" self . user_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . session_observable_tensors = dict () for key , val in self . session_observable_data . items (): assert all ( val . index == self . session_name_encoder . classes_ ), \"session observable data is not aligned with session name encoder.\" self . session_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . price_observable_tensors = dict () for key , val in self . price_observable_data . items (): tensor_slice = list () # if there are multiple columns (i.e., multiple observables) in the data-frame, we stack them together. for column in val . columns : df_slice = val . reset_index () . pivot ( index = self . session_index_column , columns = self . item_name_column , values = column ) tensor_slice . append ( torch . tensor ( df_slice . values , dtype = torch . float32 )) assert np . all ( df_slice . index == self . session_name_encoder . classes_ ) assert np . all ( df_slice . columns == self . item_name_encoder . classes_ ) # (num_sessions, num_items, num_params) self . price_observable_tensors [ key ] = torch . stack ( tensor_slice , dim =- 1 ) def create_choice_dataset ( self ) -> None : print ( 'Creating choice dataset from stata format data-frames...' ) # get choice set in each purchase record. choice_set_size = self . main_data . groupby ( self . purchase_record_column )[ self . item_name_column ] . nunique () s = choice_set_size . value_counts () # choice set size might be different in different purchase records due to unavailability of items. rep = dict ( zip ([ f 'size { x } ' for x in s . index ], [ f 'occurrence { x } ' for x in s . values ])) if len ( np . unique ( choice_set_size )) > 1 : print ( f 'Note: choice sets of different sizes found in different purchase records: { rep } ' ) self . item_availability = self . get_item_availability_tensor () else : # None means all items are available. self . item_availability = None # get the name of item bought in each purchase record. assert all ( self . main_data [ self . main_data [ self . choice_column ] == 1 ] . groupby ( self . purchase_record_column ) . size () == 1 ) item_bought = self . main_data [ self . main_data [ self . choice_column ] == 1 ] . set_index ( self . purchase_record_column ) . loc [ self . purchase_record_index , self . item_name_column ] . values # encode names of item bought. self . item_index = self . item_name_encoder . transform ( item_bought ) # user index if self . user_index_column is None : # no user index is supplied. self . user_index = None else : # get the user index of each purchase record. self . user_index = self . main_data . groupby ( self . purchase_record_column )[ self . user_index_column ] . first () . loc [ self . purchase_record_index ] . values # encode user indices. self . user_index = self . user_name_encoder . transform ( self . user_index ) # session index if self . session_index_column is None : # print('Note: no session index provided, assign each case/purchase record to a unique session index.') self . session_index = None else : # get session index of each purchase record. self . session_index = self . main_data . groupby ( self . purchase_record_column )[ self . session_index_column ] . first () . loc [ self . purchase_record_index ] . values self . session_index = self . session_name_encoder . transform ( self . session_index ) self . choice_dataset = ChoiceDataset ( item_index = torch . LongTensor ( self . item_index ), user_index = torch . LongTensor ( self . user_index ) if self . user_index is not None else None , session_index = torch . LongTensor ( self . session_index ) if self . session_index is not None else None , item_availability = self . item_availability , # keyword arguments for observables. ** self . item_observable_tensors , ** self . user_observable_tensors , ** self . session_observable_tensors , ** self . price_observable_tensors ) self . choice_dataset . to ( self . device ) def get_item_availability_tensor ( self ) -> torch . BoolTensor : \"\"\"Get the item availability tensor from the main_data data-frame.\"\"\" if self . session_index_column is None : raise ValueError ( f 'Item availability cannot be constructed without session index column.' ) A = self . main_data . pivot ( self . session_index_column , self . item_name_column , self . choice_column ) return torch . BoolTensor ( ~ np . isnan ( A . values )) def __len__ ( self ): return len ( self . purchase_record_index ) def summary ( self ): print ( f '* purchase record index range:' , self . purchase_record_index [: 3 ], '...' , self . purchase_record_index [ - 3 :]) print ( f '* Space of { len ( self . item_name_encoder . classes_ ) } items: \\n ' , pd . DataFrame ( data = { 'item name' : self . item_name_encoder . classes_ }, index = np . arange ( len ( self . item_name_encoder . classes_ ))) . T ) print ( f '* Number of purchase records/cases: { len ( self ) } .' ) print ( '* Preview of main data frame:' ) print ( self . main_data ) print ( '* Preview of ChoiceDataset:' ) print ( self . choice_dataset ) __init__ ( self , main_data , purchase_record_column , item_name_column , choice_column , user_index_column = None , session_index_column = None , user_observable_data = None , item_observable_data = None , session_observable_data = None , price_observable_data = None , user_observable_columns = None , item_observable_columns = None , session_observable_columns = None , price_observable_columns = None , device = 'cpu' ) special The initialization method of EasyDatasetWrapper. Parameters: Name Type Description Default main_data pd.DataFrame the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: required purchase_record_column str the column in main_data identifies the index of purchasing records. required item_name_column str the column in main_data identifies the name of items. required choice_column str the column in the main_data identifies the bought item, for all rows with the same value of purchase_record_column , there should be exactly one row with choice_column equal to 1, all other rows should be 0. required user_index_column Optional[str] an optional column indicating the user in each purchasing records, values should be constant across all rows with the same purchase_record_column . Defaults to None. None session_index_column Optional[str] an optional column indicating the session in each purchasing records, values should be constant across all rows with the same purchase_record_column . Defaults to None. None user_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of user_index_column and consisting of values from main_data[user_index_column] . Defaults to dict(). None item_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of item_name_column and consisting of values from main_data[item_name_column] . Defaults to dict(). None session_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of session_index_column and consisting of values from main_data[session_index_column] . Defaults to dict(). None price_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of session_index_column and consists of values from main_data[session_index_column] and (2) a column named by the value of item_name_column and consisting of values from main_data[item_name_column] . Defaults to dict(). None # format (str the input format of the dataset. Defaults to 'stata'. required Exceptions: Type Description ValueError description Source code in torch_choice/utils/easy_data_wrapper.py def __init__ ( self , main_data : pd . DataFrame , purchase_record_column : str , item_name_column : str , choice_column : str , user_index_column : Optional [ str ] = None , session_index_column : Optional [ str ] = None , # Option 1: feed in data-frames of observables. user_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , session_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , price_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , # Option 2: derive observables from columns of main_data. user_observable_columns : Optional [ List [ str ]] = None , item_observable_columns : Optional [ List [ str ]] = None , session_observable_columns : Optional [ List [ str ]] = None , price_observable_columns : Optional [ List [ str ]] = None , device : str = 'cpu' ): \"\"\"The initialization method of EasyDatasetWrapper. Args: main_data (pd.DataFrame): the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: purchase_record_column (str): the column in main_data identifies the index of purchasing records. item_name_column (str): the column in main_data identifies the name of items. choice_column (str): the column in the main_data identifies the bought item, for all rows with the same value of `purchase_record_column`, there should be exactly one row with `choice_column` equal to 1, all other rows should be 0. user_index_column (Optional[str], optional): an optional column indicating the user in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. session_index_column (Optional[str], optional): an optional column indicating the session in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. The keys of all of *_observable_data are the name of the observable data. user_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `user_index_column` and consisting of values from `main_data[user_index_column]`. Defaults to dict(). item_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). session_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `session_index_column` and consisting of values from `main_data[session_index_column]`. Defaults to dict(). price_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of `session_index_column` and consists of values from `main_data[session_index_column]` and (2) a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). Another method to include observables is via *_observable_columns keywords, which takes column name(s) of the main_data data-frame. The data wrapper will derive observable data from the main_data data-frame. For example, with `user_observable_columns = ['feature_A', 'feature_B']`, this wrapper will create two user-specific observable tensors derived from main_data['feature_A'] and main_data['feature_B']. # format (str, optional): the input format of the dataset. Defaults to 'stata'. Raises: ValueError: _description_ \"\"\" # read in data. self . main_data = main_data self . purchase_record_column = purchase_record_column # in alphabetical order of purchase record indices. # this is kept internally. self . purchase_record_index = main_data [ purchase_record_column ] . unique () self . item_name_column = item_name_column self . choice_column = choice_column self . user_index_column = user_index_column self . session_index_column = session_index_column self . device = device # encode item name, user index, session index. self . encode () # re-format observable data-frames and set correct index. self . align_observable_data ( item_observable_data , user_observable_data , session_observable_data , price_observable_data ) # derive observables from columns of the main data-frame. self . derive_observable_from_main_data ( item_observable_columns , user_observable_columns , session_observable_columns , price_observable_columns ) self . observable_data_to_observable_tensors () self . create_choice_dataset () print ( 'Finished Creating Choice Dataset.' ) align_observable_data ( self , item_observable_data , user_observable_data , session_observable_data , price_observable_data ) This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Parameters: Name Type Description Default item_observable_data Optional[Dict[str, pd.DataFrame]] description required user_observable_data Optional[Dict[str, pd.DataFrame]] description required session_observable_data Optional[Dict[str, pd.DataFrame]] description required price_observable_data Optional[Dict[str, pd.DataFrame]] description required Source code in torch_choice/utils/easy_data_wrapper.py def align_observable_data ( self , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]], user_observable_data : Optional [ Dict [ str , pd . DataFrame ]], session_observable_data : Optional [ Dict [ str , pd . DataFrame ]], price_observable_data : Optional [ Dict [ str , pd . DataFrame ]]) -> None : \"\"\"This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Args: item_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ user_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ session_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ price_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ \"\"\" self . item_observable_data = dict () if item_observable_data is not None : for key , val in item_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided item observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ], f \"item { item } is not in the { self . item_name_column } column of the item observable data-frame.\" self . item_observable_data [ 'item_' + key ] = val . set_index ( self . item_name_column ) . loc [ self . item_name_encoder . classes_ ] self . user_observable_data = dict () if user_observable_data is not None : for key , val in user_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . user_index_column is not None , \"user observable data is provided but user index column is not provided.\" assert self . user_index_column in val . columns , f \" { self . user_index_column } is not a column of provided user observable data-frame.\" for user in self . user_name_encoder . classes_ : assert user in val [ self . user_index_column ] . values , f \"user { user } is not in the { self . user_index_column } column of the user observable data-frame.\" self . user_observable_data [ 'user_' + key ] = val . set_index ( self . user_index_column ) . loc [ self . user_name_encoder . classes_ ] self . session_observable_data = dict () if session_observable_data is not None : for key , val in session_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"session observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided session observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the session observable data-frame.\" self . session_observable_data [ 'session_' + key ] = val . set_index ( self . session_index_column ) . loc [ self . session_name_encoder . classes_ ] self . price_observable_data = dict () if price_observable_data is not None : for key , val in price_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"price observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided price observable data-frame.\" assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided price observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the price observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ] . values , f \"item { item } is not in the { self . item_name_column } column of the price observable data-frame.\" # need to re-index since some (session, item) pairs are allowed to be unavailable. # complete index = Cartesian product of all sessions and all items. complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + key ] = val . set_index ([ self . session_index_column , self . item_name_column ]) . reindex ( complete_index ) derive_observable_from_main_data ( self , item_observable_columns , user_observable_columns , session_observable_columns , price_observable_columns ) Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. Source code in torch_choice/utils/easy_data_wrapper.py def derive_observable_from_main_data ( self , item_observable_columns : Optional [ List [ str ]], user_observable_columns : Optional [ List [ str ]], session_observable_columns : Optional [ List [ str ]], price_observable_columns : Optional [ List [ str ]]) -> None : \"\"\" Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. \"\"\" if item_observable_columns is not None : for obs_col in item_observable_columns : # get the value of `obs_col` for each item. # note: values in self.main_data[self.item_name_column] are NOT encoded, they are raw values. self . item_observable_data [ 'item_' + obs_col ] = self . main_data . groupby ( self . item_name_column ) . first ()[[ obs_col ]] . loc [ self . item_name_encoder . classes_ ] if user_observable_columns is not None : for obs_col in user_observable_columns : # TODO: move to sanity check part. assert self . user_index_column is not None , \"user observable data is required but user index column is not provided.\" self . user_observable_data [ 'user_' + obs_col ] = self . main_data . groupby ( self . user_index_column ) . first ()[[ obs_col ]] . loc [ self . user_name_encoder . classes_ ] if session_observable_columns is not None : for obs_col in session_observable_columns : self . session_observable_data [ 'session_' + obs_col ] = self . main_data . groupby ( self . session_index_column ) . first ()[[ obs_col ]] . loc [ self . session_name_encoder . classes_ ] if price_observable_columns is not None : for obs_col in price_observable_columns : val = self . main_data . groupby ([ self . session_index_column , self . item_name_column ]) . first ()[[ obs_col ]] complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + obs_col ] = val . reindex ( complete_index ) encode ( self ) Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. Source code in torch_choice/utils/easy_data_wrapper.py def encode ( self ) -> None : \"\"\" Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. \"\"\" # encode item names. self . item_name_encoder = LabelEncoder () . fit ( self . main_data [ self . item_name_column ] . unique ()) # encode user indices. if self . user_index_column is not None : self . user_name_encoder = LabelEncoder () . fit ( self . main_data [ self . user_index_column ] . unique ()) # encode session indices. if self . session_index_column is not None : self . session_name_encoder = LabelEncoder () . fit ( self . main_data [ self . session_index_column ] . unique ()) get_item_availability_tensor ( self ) Get the item availability tensor from the main_data data-frame. Source code in torch_choice/utils/easy_data_wrapper.py def get_item_availability_tensor ( self ) -> torch . BoolTensor : \"\"\"Get the item availability tensor from the main_data data-frame.\"\"\" if self . session_index_column is None : raise ValueError ( f 'Item availability cannot be constructed without session index column.' ) A = self . main_data . pivot ( self . session_index_column , self . item_name_column , self . choice_column ) return torch . BoolTensor ( ~ np . isnan ( A . values )) observable_data_to_observable_tensors ( self ) Convert all self. _observable_data to self. _observable_tensors for PyTorch. Source code in torch_choice/utils/easy_data_wrapper.py def observable_data_to_observable_tensors ( self ) -> None : \"\"\"Convert all self.*_observable_data to self.*_observable_tensors for PyTorch.\"\"\" self . item_observable_tensors = dict () for key , val in self . item_observable_data . items (): assert all ( val . index == self . item_name_encoder . classes_ ), \"item observable data is not alighted with user name encoder.\" self . item_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . user_observable_tensors = dict () for key , val in self . user_observable_data . items (): assert all ( val . index == self . user_name_encoder . classes_ ), \"user observable data is not alighted with user name encoder.\" self . user_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . session_observable_tensors = dict () for key , val in self . session_observable_data . items (): assert all ( val . index == self . session_name_encoder . classes_ ), \"session observable data is not aligned with session name encoder.\" self . session_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . price_observable_tensors = dict () for key , val in self . price_observable_data . items (): tensor_slice = list () # if there are multiple columns (i.e., multiple observables) in the data-frame, we stack them together. for column in val . columns : df_slice = val . reset_index () . pivot ( index = self . session_index_column , columns = self . item_name_column , values = column ) tensor_slice . append ( torch . tensor ( df_slice . values , dtype = torch . float32 )) assert np . all ( df_slice . index == self . session_name_encoder . classes_ ) assert np . all ( df_slice . columns == self . item_name_encoder . classes_ ) # (num_sessions, num_items, num_params) self . price_observable_tensors [ key ] = torch . stack ( tensor_slice , dim =- 1 ) run_helper This is a template script for researchers to train the PyTorch-based model with minimal effort. The researcher only needs to initialize the dataset and the model, this training template comes with default hyper-parameters including batch size and learning rate. The researcher should experiment with different levels of hyper-parameter if the default setting doesn't converge well. run ( model , dataset , batch_size =- 1 , learning_rate = 0.01 , num_epochs = 5000 ) All in one script for the model training and result presentation. Source code in torch_choice/utils/run_helper.py def run ( model , dataset , batch_size =- 1 , learning_rate = 0.01 , num_epochs = 5000 ): \"\"\"All in one script for the model training and result presentation.\"\"\" assert isinstance ( model , ConditionalLogitModel ) or isinstance ( model , NestedLogitModel ), \\ f 'A model of type { type ( model ) } is not supported by this runner.' model = deepcopy ( model ) # do not modify the model outside. trained_model = deepcopy ( model ) # create another copy for returning. data_loader = data_utils . create_data_loader ( dataset , batch_size = batch_size , shuffle = True ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) print ( '=' * 20 , 'received model' , '=' * 20 ) print ( model ) print ( '=' * 20 , 'received dataset' , '=' * 20 ) print ( dataset ) print ( '=' * 20 , 'training the model' , '=' * 20 ) # fit the model. for e in range ( 1 , num_epochs + 1 ): # track the log-likelihood to minimize. ll , count = 0.0 , 0.0 for batch in data_loader : item_index = batch [ 'item' ] . item_index if isinstance ( model , NestedLogitModel ) else batch . item_index loss = model . negative_log_likelihood ( batch , item_index ) ll -= loss . detach () . item () # * len(batch) count += len ( batch ) optimizer . zero_grad () loss . backward () optimizer . step () # ll /= count if e % ( num_epochs // 10 ) == 0 : print ( f 'Epoch { e } : Log-likelihood= { ll } ' ) # current methods of computing standard deviation will corrupt the model, load weights into another model for returning. state_dict = deepcopy ( model . state_dict ()) trained_model . load_state_dict ( state_dict ) # get mean of estimation. mean_dict = dict () for k , v in model . named_parameters (): mean_dict [ k ] = v . clone () # estimate the standard error of the model. if isinstance ( model , ConditionalLogitModel ): def nll_loss ( model ): y_pred = model ( dataset ) return F . cross_entropy ( y_pred , dataset . item_index , reduction = 'sum' ) elif isinstance ( model , NestedLogitModel ): def nll_loss ( model ): d = dataset [ torch . arange ( len ( dataset ))] return model . negative_log_likelihood ( d , d [ 'item' ] . item_index ) std_dict = parameter_std ( model , nll_loss ) print ( '=' * 20 , 'model results' , '=' * 20 ) report = list () for coef_name , std in std_dict . items (): std = std . cpu () . detach () . numpy () mean = mean_dict [ coef_name ] . cpu () . detach () . numpy () coef_name = coef_name . replace ( 'coef_dict.' , '' ) . replace ( '.coef' , '' ) for i in range ( mean . size ): report . append ({ 'Coefficient' : coef_name + f '_ { i } ' , 'Estimation' : float ( mean [ i ]), 'Std. Err.' : float ( std [ i ])}) report = pd . DataFrame ( report ) . set_index ( 'Coefficient' ) print ( f 'Training Epochs: { num_epochs } \\n ' ) print ( f 'Learning Rate: { learning_rate } \\n ' ) print ( f 'Batch Size: { batch_size if batch_size != - 1 else len ( dataset ) } out of { len ( dataset ) } observations in total \\n ' ) print ( f 'Final Log-likelihood: { ll } \\n ' ) print ( 'Coefficients: \\n ' ) print ( report . to_markdown ()) return trained_model std parameter_std ( model_trained , loss_fn ) This method firstly computes the Hessian of loss_fn(model_trained) with respect to model_trained.parameters(), then computes the standard error from the Hessian. NOTE: the current implementation involving deletion of attributes in model, this is an unsafe workaround for now. See https://github.com/pytorch/pytorch/issues/50138 for details. Parameters: Name Type Description Default model_trained nn.Module a trained pytorch model, the std estimated from Hessian only works if the model has been trained to optimal. required loss_fn callable the negatigve log-likelihood function (loss function). required return_hessian bool request to return hessian matrix as well. required Returns: Type Description [dict] a dictionary maps from keys in model_train.state_dict() to standard errors of esimations of each parameters in model_train.parameters(), shapes of values of returned dictionary is the same as shapes in model_train.state_dict(). [torch.Tensor]: optionally return the Hessian of loss_fn(model_trained) w.r.t. model_trained.parameters() Source code in torch_choice/utils/std.py def parameter_std ( model_trained : nn . Module , loss_fn : callable ) -> Tuple [ dict , Optional [ torch . Tensor ]]: \"\"\"This method firstly computes the Hessian of loss_fn(model_trained) with respect to model_trained.parameters(), then computes the standard error from the Hessian. NOTE: the current implementation involving deletion of attributes in model, this is an unsafe workaround for now. See https://github.com/pytorch/pytorch/issues/50138 for details. Args: model_trained (nn.Module): a trained pytorch model, the std estimated from Hessian only works if the model has been trained to optimal. loss_fn (callable): the negatigve log-likelihood function (loss function). return_hessian (bool): request to return hessian matrix as well. Returns: [dict]: a dictionary maps from keys in model_train.state_dict() to standard errors of esimations of each parameters in model_train.parameters(), shapes of values of returned dictionary is the same as shapes in model_train.state_dict(). [torch.Tensor]: optionally return the Hessian of loss_fn(model_trained) w.r.t. model_trained.parameters() \"\"\" # Need to make this safe. model = copy ( model_trained ) state_dict = deepcopy ( model . state_dict ()) shape , start , end = dict (), dict (), dict () param_list = list () s = 0 # wrap state dict into a single one dimensional tensor. for k , v in state_dict . items (): num_params = state_dict [ k ] . numel () start [ k ], end [ k ] = ( s , s + num_params ) s += num_params shape [ k ] = v . shape param_list . append ( v . clone () . view ( - 1 ,)) all_params = torch . cat ( param_list ) def func ( input_tensor ): # unwrap parameters. for k in state_dict . keys (): src = input_tensor [ start [ k ]: end [ k ]] . view ( * shape [ k ]) exec ( f 'del model. { k } ' ) exec ( f 'model. { k } = src' ) return loss_fn ( model ) H = torch . autograd . functional . hessian ( func , all_params ) std_all = torch . sqrt ( torch . diag ( torch . inverse ( H ))) std_dict = dict () for k in state_dict . keys (): std_dict [ k ] = std_all [ start [ k ]: end [ k ]] . view ( * shape [ k ]) . cpu () return std_dict","title":"API Reference Torch-Choice"},{"location":"api_torch_choice/#api-reference-torch-choice","text":"","title":"API Reference: Torch Choice"},{"location":"api_torch_choice/#torch_choice.data","text":"","title":"data"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset","text":"The dataset object for management large scale consumer choice datasets. Please refer to the documentation and tutorials for more details on using ChoiceDataset . Author: Tianyu Du Update: Apr. 27, 2022","title":"choice_dataset"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset","text":"Source code in torch_choice/data/choice_dataset.py class ChoiceDataset ( torch . utils . data . Dataset ): def __init__ ( self , item_index : torch . LongTensor , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset. def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) def __contains__ ( self , key : str ) -> bool : return key in self . keys def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag @property def device ( self ) -> str : \"\"\"Returns the device of the dataset. Returns: str: the device of the dataset. \"\"\" for attr in self . __dict__ . values (): if torch . is_tensor ( attr ): return attr . device @property def num_users ( self ) -> int : \"\"\"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: int: the number of users involved in this dataset. \"\"\" # query from user_index if self . user_index is not None : return len ( torch . unique ( self . user_index )) else : return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_user_attribute(key) or self._is_taste_attribute(key): # return val.shape[0] # return 1 @property def num_items ( self ) -> int : \"\"\"Returns the number of items involved in this dataset. Returns: int: the number of items involved in this dataset. \"\"\" return len ( torch . unique ( self . item_index )) # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_item_attribute(key): # return val.shape[0] # elif self._is_taste_attribute(key) or self._is_price_attribute(key): # return val.shape[1] # return 1 @property def num_sessions ( self ) -> int : \"\"\"Returns the number of sessions involved in this dataset. Returns: int: the number of sessions involved in this dataset. \"\"\" return len ( torch . unique ( self . session_index )) # if self.session_index is None: # return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_session_attribute(key) or self._is_price_attribute(key): # return val.shape[0] # return 1 @property def x_dict ( self ) -> Dict [ object , torch . Tensor ]: \"\"\"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. \"\"\" out = dict () for key , val in self . __dict__ . items (): if self . _is_attribute ( key ): # only include attributes. out [ key ] = self . _expand_tensor ( key , val ) # reshape to (num_sessions, num_items, num_params). return out @classmethod def _from_dict ( cls , dictionary : Dict [ str , torch . tensor ]) -> \"ChoiceDataset\" : \"\"\"Creates an instance of ChoiceDataset from a dictionary of arguments. Args: dictionary (Dict[str, torch.tensor]): a dictionary with keys as argument names and values as arguments. Returns: ChoiceDataset: the created copy of dataset. \"\"\" dataset = cls ( ** dictionary ) for key , item in dictionary . items (): setattr ( dataset , key , item ) return dataset def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) def _check_device_consistency ( self ) -> None : \"\"\"Checks if all tensors in this dataset are on the same device. Raises: Exception: an exception is raised if not all tensors are on the same device. \"\"\" # assert all tensors are on the same device. devices = list () for val in self . __dict__ . values (): if torch . is_tensor ( val ): devices . append ( val . device ) if len ( set ( devices )) > 1 : raise Exception ( f 'Found tensors on different devices: { set ( devices ) } .' , 'Use dataset.to() method to align devices.' ) def _size_repr ( self , value : object ) -> List [ int ]: \"\"\"A helper method to get the string-representation of object sizes, this is helpful while constructing the string representation of the dataset. Args: value (object): an object to examine its size. Returns: List[int]: list of integers representing the size of the object, length of the list is equal to dimension of `value`. \"\"\" if torch . is_tensor ( value ): return list ( value . size ()) elif isinstance ( value , int ) or isinstance ( value , float ): return [ 1 ] elif isinstance ( value , list ) or isinstance ( value , tuple ): return [ len ( value )] else : return [] def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items ()] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" # ================================================================================================================== # methods for checking attribute categories. # ================================================================================================================== @staticmethod def _is_item_attribute ( key : str ) -> bool : return key . startswith ( 'item_' ) and ( key != 'item_availability' ) and ( key != 'item_index' ) @staticmethod def _is_user_attribute ( key : str ) -> bool : return key . startswith ( 'user_' ) and ( key != 'user_index' ) @staticmethod def _is_session_attribute ( key : str ) -> bool : return key . startswith ( 'session_' ) and ( key != 'session_index' ) @staticmethod def _is_taste_attribute ( key : str ) -> bool : return key . startswith ( 'taste_' ) @staticmethod def _is_price_attribute ( key : str ) -> bool : return key . startswith ( 'price_' ) def _is_attribute ( self , key : str ) -> bool : return self . _is_item_attribute ( key ) \\ or self . _is_user_attribute ( key ) \\ or self . _is_session_attribute ( key ) \\ or self . _is_taste_attribute ( key ) \\ or self . _is_price_attribute ( key ) def _expand_tensor ( self , key : str , val : torch . Tensor ) -> torch . Tensor : \"\"\"Expands attribute tensor to (num_sessions, num_items, num_params) shape for prediction tasks, this method won't reshape the tensor at all if the `key` (i.e., name of the tensor) suggests its not an attribute of any kind. Args: key (str): name of the attribute used to determine the raw shape of the tensor. For example, 'item_obs' means the raw tensor is in shape (num_items, num_params). val (torch.Tensor): the attribute tensor to be reshaped. Returns: torch.Tensor: the reshaped tensor with shape (num_sessions, num_items, num_params). \"\"\" if not self . _is_attribute ( key ): print ( f 'Warning: the input key { key } is not an attribute of the dataset, will NOT modify the provided tensor.' ) # don't expand non-attribute tensors, if any. return val num_params = val . shape [ - 1 ] if self . _is_user_attribute ( key ): # user_attribute (num_users, *) out = val [ self . user_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_item_attribute ( key ): # item_attribute (num_items, *) out = val . view ( 1 , self . num_items , num_params ) . expand ( len ( self ), - 1 , - 1 ) elif self . _is_session_attribute ( key ): # session_attribute (num_sessions, *) out = val [ self . session_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_taste_attribute ( key ): # taste_attribute (num_users, num_items, *) out = val [ self . user_index , :, :] elif self . _is_price_attribute ( key ): # price_attribute (num_sessions, num_items, *) out = val [ self . session_index , :, :] assert out . shape == ( len ( self ), self . num_items , num_params ) return out","title":"ChoiceDataset"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.device","text":"Returns the device of the dataset. Returns: Type Description str the device of the dataset.","title":"device"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_items","text":"Returns the number of items involved in this dataset. Returns: Type Description int the number of items involved in this dataset.","title":"num_items"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_sessions","text":"Returns the number of sessions involved in this dataset. Returns: Type Description int the number of sessions involved in this dataset.","title":"num_sessions"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_users","text":"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: Type Description int the number of users involved in this dataset.","title":"num_users"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.x_dict","text":"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Type Description Dict[object, torch.Tensor] a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values.","title":"x_dict"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__eq__","text":"Returns whether all tensor attributes of both ChoiceDatasets are equal. Source code in torch_choice/data/choice_dataset.py def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag","title":"__eq__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__getitem__","text":"Retrieves samples corresponding to the provided index or list of indices. Parameters: Name Type Description Default indices Union[int, torch.LongTensor] a single integer index or a tensor of indices. required Returns: Type Description ChoiceDataset a subset of the dataset. Source code in torch_choice/data/choice_dataset.py def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict )","title":"__getitem__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__init__","text":"Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called batch_size in the documentation. The batch_size corresponds to the file length in wide-format dataset, and often denoted using N . We call it batch_size to follow the convention in machine learning literature. A choice instance is a row of the dataset, so there are batch_size choice instances in each ChoiceDataset . The dataset consists of: (1) a collection of batch_size tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of observables associated with item, user, session, etc. Parameters: Name Type Description Default item_index torch.LongTensor a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the label tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. required label Optional[torch.LongTensor] a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the label argument as None in the initialization method, and the model will use item_index as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. None user_index Optional[torch.LongTensor] a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If None user index is provided, it's assumed that the choice instances are from the same user. user_index is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. None session_index Optional[torch.LongTensor] a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as None . In this case, the ChoiceDataset object will assume each choice instance to be in its own session. Defaults to None. None item_availability Optional[torch.BoolTensor] A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. None Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, ) 2. item observables must start with 'item_' and have shape (num_items, ) 3. session observables must start with 'session_' and have shape (num_sessions, ) 4. taste observables (those vary by user and item) must start with taste_ and have shape (num_users, num_items, ). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with price_ and have shape (num_sessions, num_items, *) Source code in torch_choice/data/choice_dataset.py def __init__ ( self , item_index : torch . LongTensor , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset.","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__len__","text":"Returns number of samples in this dataset. Returns: Type Description int length of the dataset. Source code in torch_choice/data/choice_dataset.py def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index )","title":"__len__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__repr__","text":"A method to get a string representation of the dataset. Returns: Type Description str the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items ()] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\"","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.apply_tensor","text":"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Parameters: Name Type Description Default func callable a callable function to be applied on tensors and tensor-values of dictionaries. required Returns: Type Description ChoiceDataset the modified dataset. Source code in torch_choice/data/choice_dataset.py def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self","title":"apply_tensor()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.clone","text":"Creates a copy of self. Returns: Type Description ChoiceDataset a copy of self. Source code in torch_choice/data/choice_dataset.py def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary )","title":"clone()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.to","text":"Moves all tensors in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union[str, torch.device] the destination device. required Returns: Type Description ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/choice_dataset.py def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device ))","title":"to()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset","text":"The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Author: Tianyu Du Update: Apr. 28, 2022","title":"joint_dataset"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset","text":"A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Source code in torch_choice/data/joint_dataset.py class JointDataset ( torch . utils . data . Dataset ): \"\"\"A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. \"\"\" def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) @property def device ( self ) -> str : \"\"\"Returns the device of datasets contained in the joint dataset. Returns: str: the device of the dataset. \"\"\" for d in self . datasets . values (): return d . device def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self","title":"JointDataset"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.device","text":"Returns the device of datasets contained in the joint dataset. Returns: Type Description str the device of the dataset.","title":"device"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__getitem__","text":"Queries samples from the dataset by index. Parameters: Name Type Description Default indices Union[int, torch.LongTensor] an integer or a 1D tensor of multiple indices. required Returns: Type Description Dict[str, ChoiceDataset] the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the datasets argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. Source code in torch_choice/data/joint_dataset.py def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ())","title":"__getitem__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__init__","text":"The initialize methods. Source code in torch_choice/data/joint_dataset.py def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__len__","text":"Get the number of samples in the joint dataset. Returns: Type Description int the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. Source code in torch_choice/data/joint_dataset.py def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d )","title":"__len__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__repr__","text":"A method to get a string representation of the dataset. Returns: Type Description str the string representation of the dataset. Source code in torch_choice/data/joint_dataset.py def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out )","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.to","text":"Moves all datasets in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union[str, torch.device] the destination device. required Returns: Type Description ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/joint_dataset.py def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self","title":"to()"},{"location":"api_torch_choice/#torch_choice.data.utils","text":"","title":"utils"},{"location":"api_torch_choice/#torch_choice.data.utils.pivot3d","text":"Creates a tensor of shape (df[dim0].nunique(), df[dim1].nunique(), len(values)) from the provided data frame. Example, if dim0 is the column of session ID, dim1 is the column of alternative names, then out[t, i, k] is the feature values[k] of item i in session t. The returned tensor has shape (num_sessions, num_items, num_params), which fits the purpose of conditioanl logit models. Source code in torch_choice/data/utils.py def pivot3d ( df : pd . DataFrame , dim0 : str , dim1 : str , values : Union [ str , List [ str ]]) -> torch . Tensor : \"\"\" Creates a tensor of shape (df[dim0].nunique(), df[dim1].nunique(), len(values)) from the provided data frame. Example, if dim0 is the column of session ID, dim1 is the column of alternative names, then out[t, i, k] is the feature values[k] of item i in session t. The returned tensor has shape (num_sessions, num_items, num_params), which fits the purpose of conditioanl logit models. \"\"\" if not isinstance ( values , list ): values = [ values ] dim1_list = sorted ( df [ dim1 ] . unique ()) tensor_slice = list () for value in values : layer = df . pivot ( index = dim0 , columns = dim1 , values = value ) tensor_slice . append ( torch . Tensor ( layer [ dim1_list ] . values )) tensor = torch . stack ( tensor_slice , dim =- 1 ) assert tensor . shape == ( df [ dim0 ] . nunique (), df [ dim1 ] . nunique (), len ( values )) return tensor","title":"pivot3d()"},{"location":"api_torch_choice/#torch_choice.model","text":"","title":"model"},{"location":"api_torch_choice/#torch_choice.model.coefficient","text":"The general class of learnable coefficients in various models, this class serves as the building blocks for models in this package. The weights (i.e., learnable parameters) in the Coefficient class are implemented using PyTorch and can be trained directly using optimizers from PyTorch. NOTE: torch-choice package users don't interact with classes in this file directly, please use conditional_logit_model.py and nested_logit_model.py instead. Author: Tianyu Du Update: Apr. 28, 2022","title":"coefficient"},{"location":"api_torch_choice/#torch_choice.model.coefficient.Coefficient","text":"Source code in torch_choice/model/coefficient.py class Coefficient ( nn . Module ): def __init__ ( self , variation : str , num_params : int , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\"A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Args: variation (str): the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. num_params (int): number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called `theta_user`. The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, `num_params` should be set to `K`, NOT `K*U`. num_items (int): the number of items in the prediction problem, this is required to reshape the parameter correctly. num_users (Optional[int], optional): number of users, this is only necessary if the coefficient varies by users. Defaults to None. \"\"\" super ( Coefficient , self ) . __init__ () self . variation = variation self . num_items = num_items self . num_users = num_users self . num_params = num_params # construct the trainable. if self . variation == 'constant' : # constant for all users and items. self . coef = nn . Parameter ( torch . randn ( num_params ), requires_grad = True ) elif self . variation == 'item' : # coef depends on item j but not on user i. # force coefficients for the first item class to be zero. self . coef = nn . Parameter ( torch . zeros ( num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'item-full' : # coef depends on item j but not on user i. # model coefficient for every item. self . coef = nn . Parameter ( torch . zeros ( num_items , num_params ), requires_grad = True ) elif self . variation == 'user' : # coef depends on the user. # we always model coefficient for all users. self . coef = nn . Parameter ( torch . zeros ( num_users , num_params ), requires_grad = True ) elif self . variation == 'user-item' : # coefficients of the first item is forced to be zero, model coefficients for N - 1 items only. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'user-item-full' : # construct coefficients for every items. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items , num_params ), requires_grad = True ) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of the coefficient. Returns: str: the string representation of the coefficient. \"\"\" return f 'Coefficient(variation= { self . variation } , num_items= { self . num_items } ,' \\ + f ' num_users= { self . num_users } , num_params= { self . num_params } ,' \\ + f ' { self . coef . numel () } trainable parameters in total).' def forward ( self , x : torch . Tensor , user_index : Optional [ torch . Tensor ] = None , manual_coef_value : Optional [ torch . Tensor ] = None ) -> torch . Tensor : \"\"\" The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Args: x (torch.Tensor): a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. user_index (Optional[torch.Tensor], optional): a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. manual_coef_value (Optional[torch.Tensor], optional): a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. \"\"\" if manual_coef_value is not None : assert manual_coef_value . numel () == self . coef . numel () # plugin the provided coefficient values, coef is a tensor. coef = manual_coef_value . reshape ( * self . coef . shape ) else : # use the learned coefficient values, coef is a nn.Parameter. coef = self . coef num_trips , num_items , num_feats = x . shape assert self . num_params == num_feats # cast coefficient tensor to (num_trips, num_items, self.num_params). if self . variation == 'constant' : coef = coef . view ( 1 , 1 , self . num_params ) . expand ( num_trips , num_items , - 1 ) elif self . variation == 'item' : # coef has shape (num_items-1, num_params) # force coefficient for the first item to be zero. zeros = torch . zeros ( 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 0 ) # (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'item-full' : # coef has shape (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'user' : # coef has shape (num_users, num_params) coef = coef [ user_index , :] # (num_trips, num_params) user-specific coefficients. coef = coef . view ( num_trips , 1 , self . num_params ) . expand ( - 1 , num_items , - 1 ) elif self . variation == 'user-item' : # (num_trips,) long tensor of user ID. # originally, coef has shape (num_users, num_items-1, num_params) # transform to (num_trips, num_items - 1, num_params), user-specific. coef = coef [ user_index , :, :] # coefs for the first item for all users are enforced to 0. zeros = torch . zeros ( num_trips , 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 1 ) # (num_trips, num_items, num_params) elif self . variation == 'user-item-full' : # originally, coef has shape (num_users, num_items, num_params) coef = coef [ user_index , :, :] # (num_trips, num_items, num_params) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) assert coef . shape == ( num_trips , num_items , num_feats ) == x . shape # compute the utility of each item in each trip, take summation along the feature dimension, the same as taking # the inner product. return ( x * coef ) . sum ( dim =- 1 )","title":"Coefficient"},{"location":"api_torch_choice/#torch_choice.model.coefficient.Coefficient.__init__","text":"A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Parameters: Name Type Description Default variation str the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. required num_params int number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called theta_user . The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, num_params should be set to K , NOT K*U . required num_items int the number of items in the prediction problem, this is required to reshape the parameter correctly. None num_users Optional[int] number of users, this is only necessary if the coefficient varies by users. Defaults to None. None Source code in torch_choice/model/coefficient.py def __init__ ( self , variation : str , num_params : int , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\"A generic coefficient object storing trainable parameters. This class corresponds to those variables typically in Greek letters in the model's utility representation. Args: variation (str): the degree of variation of this coefficient. For example, the coefficient can vary by users or items. Currently, we support variations 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. For detailed explanation of these variations, please refer to the documentation of ConditionalLogitModel. num_params (int): number of parameters in this coefficient. Note that this number is the number of parameters per class, not the total number of parameters. For example, suppose we have U users and you want to initiate an user-specific coefficient called `theta_user`. The coefficient enters the utility form while being multiplied with some K-dimension observables. Then, for each user, there are K parameters to be multiplied with the K-dimensional observable. However, the total number of parameters is K * U (K for each of U users). In this case, `num_params` should be set to `K`, NOT `K*U`. num_items (int): the number of items in the prediction problem, this is required to reshape the parameter correctly. num_users (Optional[int], optional): number of users, this is only necessary if the coefficient varies by users. Defaults to None. \"\"\" super ( Coefficient , self ) . __init__ () self . variation = variation self . num_items = num_items self . num_users = num_users self . num_params = num_params # construct the trainable. if self . variation == 'constant' : # constant for all users and items. self . coef = nn . Parameter ( torch . randn ( num_params ), requires_grad = True ) elif self . variation == 'item' : # coef depends on item j but not on user i. # force coefficients for the first item class to be zero. self . coef = nn . Parameter ( torch . zeros ( num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'item-full' : # coef depends on item j but not on user i. # model coefficient for every item. self . coef = nn . Parameter ( torch . zeros ( num_items , num_params ), requires_grad = True ) elif self . variation == 'user' : # coef depends on the user. # we always model coefficient for all users. self . coef = nn . Parameter ( torch . zeros ( num_users , num_params ), requires_grad = True ) elif self . variation == 'user-item' : # coefficients of the first item is forced to be zero, model coefficients for N - 1 items only. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items - 1 , num_params ), requires_grad = True ) elif self . variation == 'user-item-full' : # construct coefficients for every items. self . coef = nn . Parameter ( torch . zeros ( num_users , num_items , num_params ), requires_grad = True ) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.model.coefficient.Coefficient.__repr__","text":"Returns a string representation of the coefficient. Returns: Type Description str the string representation of the coefficient. Source code in torch_choice/model/coefficient.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of the coefficient. Returns: str: the string representation of the coefficient. \"\"\" return f 'Coefficient(variation= { self . variation } , num_items= { self . num_items } ,' \\ + f ' num_users= { self . num_users } , num_params= { self . num_params } ,' \\ + f ' { self . coef . numel () } trainable parameters in total).'","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.model.coefficient.Coefficient.forward","text":"The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Parameters: Name Type Description Default x torch.Tensor a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. required user_index Optional[torch.Tensor] a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. None manual_coef_value Optional[torch.Tensor] a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. None Returns: Type Description torch.Tensor a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. Source code in torch_choice/model/coefficient.py def forward ( self , x : torch . Tensor , user_index : Optional [ torch . Tensor ] = None , manual_coef_value : Optional [ torch . Tensor ] = None ) -> torch . Tensor : \"\"\" The forward function of the coefficient, which computes the utility from purchasing each item in each session. The output shape will be (num_sessions, num_items). Args: x (torch.Tensor): a tensor of shape (num_sessions, num_items, num_params). Please note that the Coefficient class will NOT reshape input tensors itself, this reshaping needs to be done in the model class. user_index (Optional[torch.Tensor], optional): a tensor of shape (num_sessions,) contain IDs of the user involved in that session. If set to None, assume the same user is making all decisions. Defaults to None. manual_coef_value (Optional[torch.Tensor], optional): a tensor with the same number of entries as self.coef. If provided, the forward function uses provided values as coefficient and return the predicted utility, this feature is useful when the researcher wishes to manually specify values for coefficients and examine prediction with specified coefficient values. If not provided, forward function is executed using values from self.coef. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_sessions, num_items) whose (t, i) entry represents the utility of purchasing item i in session t. \"\"\" if manual_coef_value is not None : assert manual_coef_value . numel () == self . coef . numel () # plugin the provided coefficient values, coef is a tensor. coef = manual_coef_value . reshape ( * self . coef . shape ) else : # use the learned coefficient values, coef is a nn.Parameter. coef = self . coef num_trips , num_items , num_feats = x . shape assert self . num_params == num_feats # cast coefficient tensor to (num_trips, num_items, self.num_params). if self . variation == 'constant' : coef = coef . view ( 1 , 1 , self . num_params ) . expand ( num_trips , num_items , - 1 ) elif self . variation == 'item' : # coef has shape (num_items-1, num_params) # force coefficient for the first item to be zero. zeros = torch . zeros ( 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 0 ) # (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'item-full' : # coef has shape (num_items, num_params) coef = coef . view ( 1 , self . num_items , self . num_params ) . expand ( num_trips , - 1 , - 1 ) elif self . variation == 'user' : # coef has shape (num_users, num_params) coef = coef [ user_index , :] # (num_trips, num_params) user-specific coefficients. coef = coef . view ( num_trips , 1 , self . num_params ) . expand ( - 1 , num_items , - 1 ) elif self . variation == 'user-item' : # (num_trips,) long tensor of user ID. # originally, coef has shape (num_users, num_items-1, num_params) # transform to (num_trips, num_items - 1, num_params), user-specific. coef = coef [ user_index , :, :] # coefs for the first item for all users are enforced to 0. zeros = torch . zeros ( num_trips , 1 , self . num_params ) . to ( coef . device ) coef = torch . cat (( zeros , coef ), dim = 1 ) # (num_trips, num_items, num_params) elif self . variation == 'user-item-full' : # originally, coef has shape (num_users, num_items, num_params) coef = coef [ user_index , :, :] # (num_trips, num_items, num_params) else : raise ValueError ( f 'Unsupported type of variation: { self . variation } .' ) assert coef . shape == ( num_trips , num_items , num_feats ) == x . shape # compute the utility of each item in each trip, take summation along the feature dimension, the same as taking # the inner product. return ( x * coef ) . sum ( dim =- 1 )","title":"forward()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model","text":"Conditional Logit Model. Author: Tianyu Du Date: Aug. 8, 2021 Update: Apr. 28, 2022","title":"conditional_logit_model"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel","text":"The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: !!! note \"unless the -full flag is specified (which means we want to explicitly model coefficients\" for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. constant: constant over all users and items, user: user-specific parameters but constant across all items, item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. item-full: item-specific parameters but constant across all users, explicitly model for all items. user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full: parameters that are specific to both user and item, explicitly model for all items. Source code in torch_choice/model/conditional_logit_model.py class ConditionalLogitModel ( nn . Module ): \"\"\"The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: NOTE: unless the `-full` flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. - constant: constant over all users and items, - user: user-specific parameters but constant across all items, - item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. - item-full: item-specific parameters but constant across all users, explicitly model for all items. - user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - user-item-full: parameters that are specific to both user and item, explicitly model for all items. \"\"\" def __init__ ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\" Args: num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. \"\"\" super ( ConditionalLogitModel , self ) . __init__ () if num_param_dict is None : num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () self . variable_types = list ( deepcopy ( num_param_dict ) . keys ()) self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. if 'intercept' in self . coef_variation_dict . keys () and 'intercept' not in self . num_param_dict . keys (): warnings . warn ( \"'intercept' key found in coef_variation_dict but not in num_param_dict, num_param_dict['intercept'] has been set to 1.\" ) self . num_param_dict [ 'intercept' ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict if 'intercept' in self . coef_variation_dict . keys (): # intercept term has no input tensor, which has only 1 feature. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): total_utility += coef ( x_dict [ var_type ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavilable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll # NOTE: the method for computing Hessian and standard deviation has been moved to std.py. # @staticmethod # def flatten_coef_dict(coef_dict: Dict[str, Union[torch.Tensor, torch.nn.Parameter]]) -> Tuple[torch.Tensor, dict]: # \"\"\"Flattens the coef_dict into a 1-dimension tensor, used for hessian computation. # Args: # coef_dict (Dict[str, Union[torch.Tensor, torch.nn.Parameter]]): a dictionary holding learnable parameters. # Returns: # Tuple[torch.Tensor, dict]: 1. the flattened tensors with shape (num_params,), 2. an indexing dictionary # used for reconstructing the original coef_dict from the flatten tensor. # \"\"\" # type2idx = dict() # param_list = list() # start = 0 # for var_type in coef_dict.keys(): # num_params = coef_dict[var_type].coef.numel() # # track which portion of all_param tensor belongs to this variable type. # type2idx[var_type] = (start, start + num_params) # start += num_params # # use reshape instead of view to make a copy. # param_list.append(coef_dict[var_type].coef.clone().reshape(-1,)) # all_param = torch.cat(param_list) # (self.num_params(), ) # return all_param, type2idx # @staticmethod # def unwrap_coef_dict(param: torch.Tensor, type2idx: Dict[str, Tuple[int, int]]) -> Dict[str, torch.Tensor]: # \"\"\"Rebuilds coef_dict from output of self.flatten_coef_dict method. # Args: # param (torch.Tensor): the flattened coef_dict from self.flatten_coef_dict. # type2idx (Dict[str, Tuple[int, int]]): the indexing dictionary from self.flatten_coef_dict. # Returns: # Dict[str, torch.Tensor]: the re-constructed coefficient dictionary. # \"\"\" # coef_dict = dict() # for var_type in type2idx.keys(): # start, end = type2idx[var_type] # # no need to reshape here, Coefficient handles it. # coef_dict[var_type] = param[start:end] # return coef_dict # def compute_hessian(self, x_dict, availability, user_index, y) -> torch.Tensor: # \"\"\"Computes the Hessian of negative log-likelihood (total cross-entropy loss) with respect # to all parameters in this model. The Hessian can be later used for constructing the standard deviation of # parameters. # Args: # x_dict ,availability, user_index: see definitions in self.forward method. # y (torch.LongTensor): a tensor with shape (num_trips,) of IDs of items actually purchased. # Returns: # torch.Tensor: a (self.num_params, self.num_params) tensor of the Hessian matrix. # \"\"\" # all_coefs, type2idx = self.flatten_coef_dict(self.coef_dict) # def compute_nll(P: torch.Tensor) -> float: # coef_dict = self.unwrap_coef_dict(P, type2idx) # y_pred = self._forward(x_dict=x_dict, # availability=availability, # user_index=user_index, # manual_coef_value_dict=coef_dict) # # the reduction needs to be 'sum' to obtain NLL. # loss = F.cross_entropy(y_pred, y, reduction='sum') # return loss # H = torch.autograd.functional.hessian(compute_nll, all_coefs) # assert H.shape == (self.num_params, self.num_params) # return H # def compute_std(self, x_dict, availability, user_index, y) -> Dict[str, torch.Tensor]: # \"\"\"Computes # Args:f # See definitions in self.compute_hessian. # Returns: # Dict[str, torch.Tensor]: a dictionary whose keys are the same as self.coef_dict.keys() # the values are standard errors of coefficients in each coefficient group. # \"\"\" # _, type2idx = self.flatten_coef_dict(self.coef_dict) # H = self.compute_hessian(x_dict, availability, user_index, y) # std_all = torch.sqrt(torch.diag(torch.inverse(H))) # std_dict = dict() # for var_type in type2idx.keys(): # # get std of variables belonging to each type. # start, end = type2idx[var_type] # std_dict[var_type] = std_all[start:end] # return std_dict","title":"ConditionalLogitModel"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.num_params","text":"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Type Description int the total number of learnable parameters.","title":"num_params"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__init__","text":"Parameters: Name Type Description Default num_items int number of items in the dataset. None num_users int number of users in the dataset. None coef_variation_dict Dict[str, str] variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with price_ , user_ , etc), or intercept if the researcher requires an intercept term. For each variable name X_var (e.g., user_income ) or intercept , the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. constant : the coefficient constant over all users and items: \\(X \beta\\) . user : user-specific parameters but constant across all items: \\(X \beta_{u}\\) . item : item-specific parameters but constant across all users, \\(X \beta_{i}\\) . Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. item-full : the same configuration as item , but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - user-item : parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full : parameters that are specific to both user and item, explicitly model for all items. required num_param_dict Optional[Dict[str, int]] variable type to number of parameters dictionary with keys exactly the same as the coef_variation_dict . Values of num_param_dict records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the coef_variation_dict dictionary and values of all ones. Default to be None. None Source code in torch_choice/model/conditional_logit_model.py def __init__ ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None ) -> None : \"\"\" Args: num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. \"\"\" super ( ConditionalLogitModel , self ) . __init__ () if num_param_dict is None : num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () self . variable_types = list ( deepcopy ( num_param_dict ) . keys ()) self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. if 'intercept' in self . coef_variation_dict . keys () and 'intercept' not in self . num_param_dict . keys (): warnings . warn ( \"'intercept' key found in coef_variation_dict but not in num_param_dict, num_param_dict['intercept'] has been set to 1.\" ) self . num_param_dict [ 'intercept' ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__repr__","text":"Return a string representation of the model. Returns: Type Description str the string representation of the model. Source code in torch_choice/model/conditional_logit_model.py def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst )","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.forward","text":"Forward pass of the model. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object. required manual_coef_value_dict Optional[Dict[str, torch.Tensor]] a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. None Returns: Type Description torch.Tensor a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. Source code in torch_choice/model/conditional_logit_model.py def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict if 'intercept' in self . coef_variation_dict . keys (): # intercept term has no input tensor, which has only 1 feature. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): total_utility += coef ( x_dict [ var_type ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavilable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility","title":"forward()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.negative_log_likelihood","text":"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data. required y torch.Tensor the label. required is_train bool whether to trace the gradient. Defaults to True. True Returns: Type Description torch.Tensor the negative log-likelihood. Source code in torch_choice/model/conditional_logit_model.py def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll","title":"negative_log_likelihood()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.summary","text":"Print out the current model parameter. Source code in torch_choice/model/conditional_logit_model.py def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef )","title":"summary()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model","text":"Implementation of the nested logit model, see page 86 of the book \"discrete choice methods with simulation\" by Train. for more details. Author: Tianyu Du Update; Apr. 28, 2022","title":"nested_logit_model"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel","text":"Source code in torch_choice/model/nested_logit_model.py class NestedLogitModel ( nn . Module ): def __init__ ( self , category_to_item : Dict [ object , List [ int ]], category_coef_variation_dict : Dict [ str , str ], category_num_param_dict : Dict [ str , int ], item_coef_variation_dict : Dict [ str , str ], item_num_param_dict : Dict [ str , int ], num_users : Optional [ int ] = None , shared_lambda : bool = False ) -> None : \"\"\"Initialization method of the nested logit model. Args: category_to_item (Dict[object, List[int]]): a dictionary maps a category ID to a list of items IDs of the queried category. category_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. category_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as category_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as category_num_param_dict but for item features. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. \"\"\" super ( NestedLogitModel , self ) . __init__ () self . category_to_item = category_to_item self . category_coef_variation_dict = category_coef_variation_dict self . category_num_param_dict = category_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . categories = list ( category_to_item . keys ()) self . num_categories = len ( self . categories ) self . num_items = sum ( len ( items ) for items in category_to_item . values ()) # category coefficients. self . category_coef_dict = self . _build_coef_dict ( self . category_coef_variation_dict , self . category_num_param_dict , self . num_categories ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_categories ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def _build_coef_dict ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Dict [ str , int ], num_items : int ) -> nn . ModuleDict : \"\"\"Builds a coefficient dictionary containing all trainable components of the model, mapping coefficient names to the corresponding Coefficient Module. num_items could be the actual number of items or the number of categories depends on the use case. NOTE: torch-choice users don't directly interact with this method. Args: coef_variation_dict (Dict[str, str]): a dictionary mapping coefficient names (e.g., theta_user) to the level of variation (e.g., 'user'). num_param_dict (Dict[str, int]): a dictionary mapping coefficient names to the number of parameters in this coefficient. Be aware that, for example, if there is one K-dimensional coefficient for every user, then the `num_param` should be K instead of K x number of users. num_items (int): the total number of items in the prediction problem. `num_items` should be the number of categories if _build_coef_dict() is used for category-level prediction. Returns: nn.ModuleDict: a PyTorch ModuleDict object mapping from coefficient names to training Coefficient. \"\"\" coef_dict = dict () for var_type , variation in coef_variation_dict . items (): num_params = num_param_dict [ var_type ] coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = num_items , num_users = self . num_users , num_params = num_params ) return nn . ModuleDict ( coef_dict ) # def _check_input_shapes(self, category_x_dict, item_x_dict, user_index, item_availability) -> None: # T = list(category_x_dict.values())[0].shape[0] # batch size. # for var_type, x_category in category_x_dict.items(): # x_item = item_x_dict[var_type] # assert len(x_item.shape) == len(x_item.shape) == 3 # assert x_category.shape[0] == x_item.shape[0] # assert x_category.shape == (T, self.num_categories, self.category_num_param_dict[var_type]) # assert x_item.shape == (T, self.num_items, self.item_num_param_dict[var_type]) # if (user_index is not None) and (self.num_users is not None): # assert user_index.shape == (T,) # if item_availability is not None: # assert item_availability.shape == (T, self.num_items) def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'category' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) def _forward ( self , category_x_dict : Dict [ str , torch . Tensor ], item_x_dict : Dict [ str , torch . Tensor ], user_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None ) -> torch . Tensor : \"\"\"\"Computes log P[t, i] = the log probability for the user involved in trip t to choose item i. Let n denote the ID of the user involved in trip t, then P[t, i] = P_{ni} on page 86 of the book \"discrete choice methods with simulation\" by Train. Args: x_category (torch.Tensor): a tensor with shape (num_trips, num_categories, *) including features of all categories in each trip. x_item (torch.Tensor): a tensor with shape (num_trips, num_items, *) including features of all items in each trip. user_index (torch.LongTensor): a tensor of shape (num_trips,) indicating which user is making decision in each trip. Setting user_index = None assumes the same user is making decisions in all trips. item_availability (torch.BoolTensor): a boolean tensor with shape (num_trips, num_items) indicating the aviliability of items in each trip. If item_availability[t, i] = False, the utility of choosing item i in trip t, V[t, i], will be set to -inf. Given the decomposition V[t, i] = W[t, k(i)] + Y[t, i] + eps, V[t, i] is set to -inf by setting Y[t, i] = -inf for unavilable items. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" if self . shared_lambda : self . lambdas = self . lambda_weight . expand ( self . num_categories ) else : self . lambdas = self . lambda_weight # if not self._clamp_called_flag: # warnings.warn('Did you forget to call clamp_lambdas() after optimizer.step()?') # The overall utility of item can be decomposed into V[item] = W[category] + Y[item] + eps. T = list ( item_x_dict . values ())[ 0 ] . shape [ 0 ] device = list ( item_x_dict . values ())[ 0 ] . device # compute category-specific utility with shape (T, num_categories). W = torch . zeros ( T , self . num_categories ) . to ( device ) if 'intercept' in self . category_coef_variation_dict . keys (): category_x_dict [ 'intercept' ] = torch . ones (( T , self . num_categories , 1 )) . to ( device ) for var_type , coef in self . category_coef_dict . items (): W += coef ( category_x_dict [ var_type ], user_index ) # compute item-specific utility (T, num_items). Y = torch . zeros ( T , self . num_items ) . to ( device ) for var_type , coef in self . item_coef_dict . items (): Y += coef ( item_x_dict [ var_type ], user_index ) if item_availability is not None : Y [ ~ item_availability ] = torch . finfo ( Y . dtype ) . min / 2 # ============================================================================= # compute the inclusive value of each category. inclusive_value = dict () for k , Bk in self . category_to_item . items (): # for nest k, divide the Y of all items in Bk by lambda_k. Y [:, Bk ] /= self . lambdas [ k ] # compute inclusive value for category k. # mask out unavilable items. inclusive_value [ k ] = torch . logsumexp ( Y [:, Bk ], dim = 1 , keepdim = False ) # (T,) # boardcast inclusive value from (T, num_categories) to (T, num_items). # for trip t, I[t, i] is the inclusive value of the category item i belongs to. I = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . category_to_item . items (): I [:, Bk ] = inclusive_value [ k ] . view ( - 1 , 1 ) # (T, |Bk|) # logP_item[t, i] = log P(ni|Bk), where Bk is the category item i is in, n is the user in trip t. logP_item = Y - I # (T, num_items) # ============================================================================= # logP_category[t, i] = log P(Bk), for item i in trip t, the probability of choosing the nest/bucket # item i belongs to. logP_category has shape (T, num_items) # logit[t, i] = W[n, k] + lambda[k] I[n, k], where n is the user involved in trip t, k is # the category item i belongs to. logit = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . category_to_item . items (): logit [:, Bk ] = ( W [:, k ] + self . lambdas [ k ] * inclusive_value [ k ]) . view ( - 1 , 1 ) # (T, |Bk|) # only count each category once in the logsumexp within the category level model. cols = [ x [ 0 ] for x in self . category_to_item . values ()] logP_category = logit - torch . logsumexp ( logit [:, cols ], dim = 1 , keepdim = True ) # ============================================================================= # compute the joint log P_{ni} as in the textbook. logP = logP_item + logP_category self . _clamp_called_flag = False return logP def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll # def clamp_lambdas(self): # \"\"\" # Restrict values of lambdas to 0 < lambda <= 1 to guarantee the utility maximization property # of the model. # This method should be called everytime after optimizer.step(). # We add a self_clamp_called_flag to remind researchers if this method is not called. # \"\"\" # for k in range(len(self.lambdas)): # self.lambdas[k] = torch.clamp(self.lambdas[k], 1e-5, 1) # self._clam_called_flag = True # @staticmethod # def add_constant(x: torch.Tensor, where: str='prepend') -> torch.Tensor: # \"\"\"A helper function used to add constant to feature tensor, # x has shape (batch_size, num_classes, num_parameters), # returns a tensor of shape (*, num_parameters+1). # \"\"\" # batch_size, num_classes, num_parameters = x.shape # ones = torch.ones((batch_size, num_classes, 1)) # if where == 'prepend': # new = torch.cat((ones, x), dim=-1) # elif where == 'append': # new = torch.cat((x, ones), dim=-1) # else: # raise Exception # return new","title":"NestedLogitModel"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.num_params","text":"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Type Description int the total number of learnable parameters.","title":"num_params"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.__init__","text":"Initialization method of the nested logit model. Parameters: Name Type Description Default category_to_item Dict[object, List[int]] a dictionary maps a category ID to a list of items IDs of the queried category. required category_coef_variation_dict Dict[str, str] a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. required category_num_param_dict Dict[str, int] a dictionary maps a variable type name to the number of parameters in this variable group. required item_coef_variation_dict Dict[str, str] the same as category_coef_variation_dict but for item features. required item_num_param_dict Dict[str, int] the same as category_num_param_dict but for item features. required num_users Optional[int] number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. None shared_lambda bool a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. False Source code in torch_choice/model/nested_logit_model.py def __init__ ( self , category_to_item : Dict [ object , List [ int ]], category_coef_variation_dict : Dict [ str , str ], category_num_param_dict : Dict [ str , int ], item_coef_variation_dict : Dict [ str , str ], item_num_param_dict : Dict [ str , int ], num_users : Optional [ int ] = None , shared_lambda : bool = False ) -> None : \"\"\"Initialization method of the nested logit model. Args: category_to_item (Dict[object, List[int]]): a dictionary maps a category ID to a list of items IDs of the queried category. category_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. category_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as category_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as category_num_param_dict but for item features. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all categories. The lambda enters the category-level selection as the following Utility of choosing category k = lambda * inclusive value of category k + linear combination of some other category level features If set to True, a single lambda will be learned for all categories, otherwise, the model learns an individual lambda for each category. Defaults to False. \"\"\" super ( NestedLogitModel , self ) . __init__ () self . category_to_item = category_to_item self . category_coef_variation_dict = category_coef_variation_dict self . category_num_param_dict = category_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . categories = list ( category_to_item . keys ()) self . num_categories = len ( self . categories ) self . num_items = sum ( len ( items ) for items in category_to_item . values ()) # category coefficients. self . category_coef_dict = self . _build_coef_dict ( self . category_coef_variation_dict , self . category_num_param_dict , self . num_categories ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_categories ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward","text":"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method.","title":"forward()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward--todo-the-conditionalogitmodel-returns-predicted-utility-the-nestedlogitmodel-behaves-the-same","text":"Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data batch. required Returns: Type Description torch.Tensor a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. Source code in torch_choice/model/nested_logit_model.py def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'category' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability )","title":"TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same?"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.log_likelihood","text":"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: Type Description _type_ the log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args )","title":"log_likelihood()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.negative_log_likelihood","text":"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Parameters: Name Type Description Default batch ChoiceDataset the ChoiceDataset object containing the data. required y torch.LongTensor the label. required is_train bool which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, is_train should be set to True. If we merely need a performance metric, then is_train can be set to False for better performance. Defaults to True. True Returns: Type Description torch.scalar_tensor the negative log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll","title":"negative_log_likelihood()"},{"location":"api_torch_choice/#torch_choice.utils","text":"","title":"utils"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper","text":"This is a helper class for creating ChoiceDataset class, we only assume very basic python knowledge to use this utility.","title":"easy_data_wrapper"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper","text":"An easy-to-use interface for creating ChoiceDataset object, please refer to the doc-string of the __init__ method for more details. You feed it with a couple of pandas data-frames and necessary information, this EasyDatasetWrapper would create the ChoiceDataset object for you. Source code in torch_choice/utils/easy_data_wrapper.py class EasyDatasetWrapper (): \"\"\"An easy-to-use interface for creating ChoiceDataset object, please refer to the doc-string of the `__init__` method for more details. You feed it with a couple of pandas data-frames and necessary information, this EasyDatasetWrapper would create the ChoiceDataset object for you. \"\"\" def __init__ ( self , main_data : pd . DataFrame , purchase_record_column : str , item_name_column : str , choice_column : str , user_index_column : Optional [ str ] = None , session_index_column : Optional [ str ] = None , # Option 1: feed in data-frames of observables. user_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , session_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , price_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , # Option 2: derive observables from columns of main_data. user_observable_columns : Optional [ List [ str ]] = None , item_observable_columns : Optional [ List [ str ]] = None , session_observable_columns : Optional [ List [ str ]] = None , price_observable_columns : Optional [ List [ str ]] = None , device : str = 'cpu' ): \"\"\"The initialization method of EasyDatasetWrapper. Args: main_data (pd.DataFrame): the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: purchase_record_column (str): the column in main_data identifies the index of purchasing records. item_name_column (str): the column in main_data identifies the name of items. choice_column (str): the column in the main_data identifies the bought item, for all rows with the same value of `purchase_record_column`, there should be exactly one row with `choice_column` equal to 1, all other rows should be 0. user_index_column (Optional[str], optional): an optional column indicating the user in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. session_index_column (Optional[str], optional): an optional column indicating the session in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. The keys of all of *_observable_data are the name of the observable data. user_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `user_index_column` and consisting of values from `main_data[user_index_column]`. Defaults to dict(). item_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). session_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `session_index_column` and consisting of values from `main_data[session_index_column]`. Defaults to dict(). price_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of `session_index_column` and consists of values from `main_data[session_index_column]` and (2) a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). Another method to include observables is via *_observable_columns keywords, which takes column name(s) of the main_data data-frame. The data wrapper will derive observable data from the main_data data-frame. For example, with `user_observable_columns = ['feature_A', 'feature_B']`, this wrapper will create two user-specific observable tensors derived from main_data['feature_A'] and main_data['feature_B']. # format (str, optional): the input format of the dataset. Defaults to 'stata'. Raises: ValueError: _description_ \"\"\" # read in data. self . main_data = main_data self . purchase_record_column = purchase_record_column # in alphabetical order of purchase record indices. # this is kept internally. self . purchase_record_index = main_data [ purchase_record_column ] . unique () self . item_name_column = item_name_column self . choice_column = choice_column self . user_index_column = user_index_column self . session_index_column = session_index_column self . device = device # encode item name, user index, session index. self . encode () # re-format observable data-frames and set correct index. self . align_observable_data ( item_observable_data , user_observable_data , session_observable_data , price_observable_data ) # derive observables from columns of the main data-frame. self . derive_observable_from_main_data ( item_observable_columns , user_observable_columns , session_observable_columns , price_observable_columns ) self . observable_data_to_observable_tensors () self . create_choice_dataset () print ( 'Finished Creating Choice Dataset.' ) def encode ( self ) -> None : \"\"\" Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. \"\"\" # encode item names. self . item_name_encoder = LabelEncoder () . fit ( self . main_data [ self . item_name_column ] . unique ()) # encode user indices. if self . user_index_column is not None : self . user_name_encoder = LabelEncoder () . fit ( self . main_data [ self . user_index_column ] . unique ()) # encode session indices. if self . session_index_column is not None : self . session_name_encoder = LabelEncoder () . fit ( self . main_data [ self . session_index_column ] . unique ()) def align_observable_data ( self , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]], user_observable_data : Optional [ Dict [ str , pd . DataFrame ]], session_observable_data : Optional [ Dict [ str , pd . DataFrame ]], price_observable_data : Optional [ Dict [ str , pd . DataFrame ]]) -> None : \"\"\"This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Args: item_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ user_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ session_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ price_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ \"\"\" self . item_observable_data = dict () if item_observable_data is not None : for key , val in item_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided item observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ], f \"item { item } is not in the { self . item_name_column } column of the item observable data-frame.\" self . item_observable_data [ 'item_' + key ] = val . set_index ( self . item_name_column ) . loc [ self . item_name_encoder . classes_ ] self . user_observable_data = dict () if user_observable_data is not None : for key , val in user_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . user_index_column is not None , \"user observable data is provided but user index column is not provided.\" assert self . user_index_column in val . columns , f \" { self . user_index_column } is not a column of provided user observable data-frame.\" for user in self . user_name_encoder . classes_ : assert user in val [ self . user_index_column ] . values , f \"user { user } is not in the { self . user_index_column } column of the user observable data-frame.\" self . user_observable_data [ 'user_' + key ] = val . set_index ( self . user_index_column ) . loc [ self . user_name_encoder . classes_ ] self . session_observable_data = dict () if session_observable_data is not None : for key , val in session_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"session observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided session observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the session observable data-frame.\" self . session_observable_data [ 'session_' + key ] = val . set_index ( self . session_index_column ) . loc [ self . session_name_encoder . classes_ ] self . price_observable_data = dict () if price_observable_data is not None : for key , val in price_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"price observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided price observable data-frame.\" assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided price observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the price observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ] . values , f \"item { item } is not in the { self . item_name_column } column of the price observable data-frame.\" # need to re-index since some (session, item) pairs are allowed to be unavailable. # complete index = Cartesian product of all sessions and all items. complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + key ] = val . set_index ([ self . session_index_column , self . item_name_column ]) . reindex ( complete_index ) def derive_observable_from_main_data ( self , item_observable_columns : Optional [ List [ str ]], user_observable_columns : Optional [ List [ str ]], session_observable_columns : Optional [ List [ str ]], price_observable_columns : Optional [ List [ str ]]) -> None : \"\"\" Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. \"\"\" if item_observable_columns is not None : for obs_col in item_observable_columns : # get the value of `obs_col` for each item. # note: values in self.main_data[self.item_name_column] are NOT encoded, they are raw values. self . item_observable_data [ 'item_' + obs_col ] = self . main_data . groupby ( self . item_name_column ) . first ()[[ obs_col ]] . loc [ self . item_name_encoder . classes_ ] if user_observable_columns is not None : for obs_col in user_observable_columns : # TODO: move to sanity check part. assert self . user_index_column is not None , \"user observable data is required but user index column is not provided.\" self . user_observable_data [ 'user_' + obs_col ] = self . main_data . groupby ( self . user_index_column ) . first ()[[ obs_col ]] . loc [ self . user_name_encoder . classes_ ] if session_observable_columns is not None : for obs_col in session_observable_columns : self . session_observable_data [ 'session_' + obs_col ] = self . main_data . groupby ( self . session_index_column ) . first ()[[ obs_col ]] . loc [ self . session_name_encoder . classes_ ] if price_observable_columns is not None : for obs_col in price_observable_columns : val = self . main_data . groupby ([ self . session_index_column , self . item_name_column ]) . first ()[[ obs_col ]] complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + obs_col ] = val . reindex ( complete_index ) def observable_data_to_observable_tensors ( self ) -> None : \"\"\"Convert all self.*_observable_data to self.*_observable_tensors for PyTorch.\"\"\" self . item_observable_tensors = dict () for key , val in self . item_observable_data . items (): assert all ( val . index == self . item_name_encoder . classes_ ), \"item observable data is not alighted with user name encoder.\" self . item_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . user_observable_tensors = dict () for key , val in self . user_observable_data . items (): assert all ( val . index == self . user_name_encoder . classes_ ), \"user observable data is not alighted with user name encoder.\" self . user_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . session_observable_tensors = dict () for key , val in self . session_observable_data . items (): assert all ( val . index == self . session_name_encoder . classes_ ), \"session observable data is not aligned with session name encoder.\" self . session_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . price_observable_tensors = dict () for key , val in self . price_observable_data . items (): tensor_slice = list () # if there are multiple columns (i.e., multiple observables) in the data-frame, we stack them together. for column in val . columns : df_slice = val . reset_index () . pivot ( index = self . session_index_column , columns = self . item_name_column , values = column ) tensor_slice . append ( torch . tensor ( df_slice . values , dtype = torch . float32 )) assert np . all ( df_slice . index == self . session_name_encoder . classes_ ) assert np . all ( df_slice . columns == self . item_name_encoder . classes_ ) # (num_sessions, num_items, num_params) self . price_observable_tensors [ key ] = torch . stack ( tensor_slice , dim =- 1 ) def create_choice_dataset ( self ) -> None : print ( 'Creating choice dataset from stata format data-frames...' ) # get choice set in each purchase record. choice_set_size = self . main_data . groupby ( self . purchase_record_column )[ self . item_name_column ] . nunique () s = choice_set_size . value_counts () # choice set size might be different in different purchase records due to unavailability of items. rep = dict ( zip ([ f 'size { x } ' for x in s . index ], [ f 'occurrence { x } ' for x in s . values ])) if len ( np . unique ( choice_set_size )) > 1 : print ( f 'Note: choice sets of different sizes found in different purchase records: { rep } ' ) self . item_availability = self . get_item_availability_tensor () else : # None means all items are available. self . item_availability = None # get the name of item bought in each purchase record. assert all ( self . main_data [ self . main_data [ self . choice_column ] == 1 ] . groupby ( self . purchase_record_column ) . size () == 1 ) item_bought = self . main_data [ self . main_data [ self . choice_column ] == 1 ] . set_index ( self . purchase_record_column ) . loc [ self . purchase_record_index , self . item_name_column ] . values # encode names of item bought. self . item_index = self . item_name_encoder . transform ( item_bought ) # user index if self . user_index_column is None : # no user index is supplied. self . user_index = None else : # get the user index of each purchase record. self . user_index = self . main_data . groupby ( self . purchase_record_column )[ self . user_index_column ] . first () . loc [ self . purchase_record_index ] . values # encode user indices. self . user_index = self . user_name_encoder . transform ( self . user_index ) # session index if self . session_index_column is None : # print('Note: no session index provided, assign each case/purchase record to a unique session index.') self . session_index = None else : # get session index of each purchase record. self . session_index = self . main_data . groupby ( self . purchase_record_column )[ self . session_index_column ] . first () . loc [ self . purchase_record_index ] . values self . session_index = self . session_name_encoder . transform ( self . session_index ) self . choice_dataset = ChoiceDataset ( item_index = torch . LongTensor ( self . item_index ), user_index = torch . LongTensor ( self . user_index ) if self . user_index is not None else None , session_index = torch . LongTensor ( self . session_index ) if self . session_index is not None else None , item_availability = self . item_availability , # keyword arguments for observables. ** self . item_observable_tensors , ** self . user_observable_tensors , ** self . session_observable_tensors , ** self . price_observable_tensors ) self . choice_dataset . to ( self . device ) def get_item_availability_tensor ( self ) -> torch . BoolTensor : \"\"\"Get the item availability tensor from the main_data data-frame.\"\"\" if self . session_index_column is None : raise ValueError ( f 'Item availability cannot be constructed without session index column.' ) A = self . main_data . pivot ( self . session_index_column , self . item_name_column , self . choice_column ) return torch . BoolTensor ( ~ np . isnan ( A . values )) def __len__ ( self ): return len ( self . purchase_record_index ) def summary ( self ): print ( f '* purchase record index range:' , self . purchase_record_index [: 3 ], '...' , self . purchase_record_index [ - 3 :]) print ( f '* Space of { len ( self . item_name_encoder . classes_ ) } items: \\n ' , pd . DataFrame ( data = { 'item name' : self . item_name_encoder . classes_ }, index = np . arange ( len ( self . item_name_encoder . classes_ ))) . T ) print ( f '* Number of purchase records/cases: { len ( self ) } .' ) print ( '* Preview of main data frame:' ) print ( self . main_data ) print ( '* Preview of ChoiceDataset:' ) print ( self . choice_dataset )","title":"EasyDatasetWrapper"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.__init__","text":"The initialization method of EasyDatasetWrapper. Parameters: Name Type Description Default main_data pd.DataFrame the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: required purchase_record_column str the column in main_data identifies the index of purchasing records. required item_name_column str the column in main_data identifies the name of items. required choice_column str the column in the main_data identifies the bought item, for all rows with the same value of purchase_record_column , there should be exactly one row with choice_column equal to 1, all other rows should be 0. required user_index_column Optional[str] an optional column indicating the user in each purchasing records, values should be constant across all rows with the same purchase_record_column . Defaults to None. None session_index_column Optional[str] an optional column indicating the session in each purchasing records, values should be constant across all rows with the same purchase_record_column . Defaults to None. None user_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of user_index_column and consisting of values from main_data[user_index_column] . Defaults to dict(). None item_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of item_name_column and consisting of values from main_data[item_name_column] . Defaults to dict(). None session_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of session_index_column and consisting of values from main_data[session_index_column] . Defaults to dict(). None price_observable_data Optional[Dict[str, pd.DataFrame]] a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of session_index_column and consists of values from main_data[session_index_column] and (2) a column named by the value of item_name_column and consisting of values from main_data[item_name_column] . Defaults to dict(). None # format (str the input format of the dataset. Defaults to 'stata'. required Exceptions: Type Description ValueError description Source code in torch_choice/utils/easy_data_wrapper.py def __init__ ( self , main_data : pd . DataFrame , purchase_record_column : str , item_name_column : str , choice_column : str , user_index_column : Optional [ str ] = None , session_index_column : Optional [ str ] = None , # Option 1: feed in data-frames of observables. user_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , session_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , price_observable_data : Optional [ Dict [ str , pd . DataFrame ]] = None , # Option 2: derive observables from columns of main_data. user_observable_columns : Optional [ List [ str ]] = None , item_observable_columns : Optional [ List [ str ]] = None , session_observable_columns : Optional [ List [ str ]] = None , price_observable_columns : Optional [ List [ str ]] = None , device : str = 'cpu' ): \"\"\"The initialization method of EasyDatasetWrapper. Args: main_data (pd.DataFrame): the main dataset holding all purchase records in a \"long-format\", each row of the dataset is an item in a purchase record. The main_data data-frame should contains the following columns: purchase_record_column (str): the column in main_data identifies the index of purchasing records. item_name_column (str): the column in main_data identifies the name of items. choice_column (str): the column in the main_data identifies the bought item, for all rows with the same value of `purchase_record_column`, there should be exactly one row with `choice_column` equal to 1, all other rows should be 0. user_index_column (Optional[str], optional): an optional column indicating the user in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. session_index_column (Optional[str], optional): an optional column indicating the session in each purchasing records, values should be constant across all rows with the same `purchase_record_column`. Defaults to None. The keys of all of *_observable_data are the name of the observable data. user_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `user_index_column` and consisting of values from `main_data[user_index_column]`. Defaults to dict(). item_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). session_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains a column named by the value of `session_index_column` and consisting of values from `main_data[session_index_column]`. Defaults to dict(). price_observable_data (Optional[Dict[str, pd.DataFrame]], optional): a dictionary with keys as the name of each observable. The values should be a pandas data-frame contains (1) a column named by the value of `session_index_column` and consists of values from `main_data[session_index_column]` and (2) a column named by the value of `item_name_column` and consisting of values from `main_data[item_name_column]`. Defaults to dict(). Another method to include observables is via *_observable_columns keywords, which takes column name(s) of the main_data data-frame. The data wrapper will derive observable data from the main_data data-frame. For example, with `user_observable_columns = ['feature_A', 'feature_B']`, this wrapper will create two user-specific observable tensors derived from main_data['feature_A'] and main_data['feature_B']. # format (str, optional): the input format of the dataset. Defaults to 'stata'. Raises: ValueError: _description_ \"\"\" # read in data. self . main_data = main_data self . purchase_record_column = purchase_record_column # in alphabetical order of purchase record indices. # this is kept internally. self . purchase_record_index = main_data [ purchase_record_column ] . unique () self . item_name_column = item_name_column self . choice_column = choice_column self . user_index_column = user_index_column self . session_index_column = session_index_column self . device = device # encode item name, user index, session index. self . encode () # re-format observable data-frames and set correct index. self . align_observable_data ( item_observable_data , user_observable_data , session_observable_data , price_observable_data ) # derive observables from columns of the main data-frame. self . derive_observable_from_main_data ( item_observable_columns , user_observable_columns , session_observable_columns , price_observable_columns ) self . observable_data_to_observable_tensors () self . create_choice_dataset () print ( 'Finished Creating Choice Dataset.' )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.align_observable_data","text":"This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Parameters: Name Type Description Default item_observable_data Optional[Dict[str, pd.DataFrame]] description required user_observable_data Optional[Dict[str, pd.DataFrame]] description required session_observable_data Optional[Dict[str, pd.DataFrame]] description required price_observable_data Optional[Dict[str, pd.DataFrame]] description required Source code in torch_choice/utils/easy_data_wrapper.py def align_observable_data ( self , item_observable_data : Optional [ Dict [ str , pd . DataFrame ]], user_observable_data : Optional [ Dict [ str , pd . DataFrame ]], session_observable_data : Optional [ Dict [ str , pd . DataFrame ]], price_observable_data : Optional [ Dict [ str , pd . DataFrame ]]) -> None : \"\"\"This method converts observables in the dictionary format (observable name --> observable data frame), for each data frame of observables, this method set the appropriate corresponding index and subsets/permutes the data frames to have the same order as in the encoder. Args: item_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ user_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ session_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ price_observable_data (Optional[Dict[str, pd.DataFrame]]): _description_ \"\"\" self . item_observable_data = dict () if item_observable_data is not None : for key , val in item_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided item observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ], f \"item { item } is not in the { self . item_name_column } column of the item observable data-frame.\" self . item_observable_data [ 'item_' + key ] = val . set_index ( self . item_name_column ) . loc [ self . item_name_encoder . classes_ ] self . user_observable_data = dict () if user_observable_data is not None : for key , val in user_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . user_index_column is not None , \"user observable data is provided but user index column is not provided.\" assert self . user_index_column in val . columns , f \" { self . user_index_column } is not a column of provided user observable data-frame.\" for user in self . user_name_encoder . classes_ : assert user in val [ self . user_index_column ] . values , f \"user { user } is not in the { self . user_index_column } column of the user observable data-frame.\" self . user_observable_data [ 'user_' + key ] = val . set_index ( self . user_index_column ) . loc [ self . user_name_encoder . classes_ ] self . session_observable_data = dict () if session_observable_data is not None : for key , val in session_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"session observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided session observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the session observable data-frame.\" self . session_observable_data [ 'session_' + key ] = val . set_index ( self . session_index_column ) . loc [ self . session_name_encoder . classes_ ] self . price_observable_data = dict () if price_observable_data is not None : for key , val in price_observable_data . items (): # key: observable name. # val: data-frame of observable data. assert self . session_index_column is not None , \"price observable data is provided but session index column is not provided.\" assert self . session_index_column in val . columns , f \" { self . session_index_column } is not a column of provided price observable data-frame.\" assert self . item_name_column in val . columns , f \" { self . item_name_column } is not a column of provided price observable data-frame.\" for session in self . session_name_encoder . classes_ : assert session in val [ self . session_index_column ] . values , f \"session { session } is not in the { self . session_index_column } column of the price observable data-frame.\" for item in self . item_name_encoder . classes_ : assert item in val [ self . item_name_column ] . values , f \"item { item } is not in the { self . item_name_column } column of the price observable data-frame.\" # need to re-index since some (session, item) pairs are allowed to be unavailable. # complete index = Cartesian product of all sessions and all items. complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + key ] = val . set_index ([ self . session_index_column , self . item_name_column ]) . reindex ( complete_index )","title":"align_observable_data()"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.derive_observable_from_main_data","text":"Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. Source code in torch_choice/utils/easy_data_wrapper.py def derive_observable_from_main_data ( self , item_observable_columns : Optional [ List [ str ]], user_observable_columns : Optional [ List [ str ]], session_observable_columns : Optional [ List [ str ]], price_observable_columns : Optional [ List [ str ]]) -> None : \"\"\" Generates data-frames of observables using certain columns in the main dataset. This is a complementary method for programers to supply variables. \"\"\" if item_observable_columns is not None : for obs_col in item_observable_columns : # get the value of `obs_col` for each item. # note: values in self.main_data[self.item_name_column] are NOT encoded, they are raw values. self . item_observable_data [ 'item_' + obs_col ] = self . main_data . groupby ( self . item_name_column ) . first ()[[ obs_col ]] . loc [ self . item_name_encoder . classes_ ] if user_observable_columns is not None : for obs_col in user_observable_columns : # TODO: move to sanity check part. assert self . user_index_column is not None , \"user observable data is required but user index column is not provided.\" self . user_observable_data [ 'user_' + obs_col ] = self . main_data . groupby ( self . user_index_column ) . first ()[[ obs_col ]] . loc [ self . user_name_encoder . classes_ ] if session_observable_columns is not None : for obs_col in session_observable_columns : self . session_observable_data [ 'session_' + obs_col ] = self . main_data . groupby ( self . session_index_column ) . first ()[[ obs_col ]] . loc [ self . session_name_encoder . classes_ ] if price_observable_columns is not None : for obs_col in price_observable_columns : val = self . main_data . groupby ([ self . session_index_column , self . item_name_column ]) . first ()[[ obs_col ]] complete_index = pd . MultiIndex . from_product ([ self . session_name_encoder . classes_ , self . item_name_encoder . classes_ ], names = [ self . session_index_column , self . item_name_column ]) self . price_observable_data [ 'price_' + obs_col ] = val . reindex ( complete_index )","title":"derive_observable_from_main_data()"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.encode","text":"Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. Source code in torch_choice/utils/easy_data_wrapper.py def encode ( self ) -> None : \"\"\" Encodes item names, user names, and session names to {0, 1, 2, ...} integers, item/user/session are encoded in alphabetical order. \"\"\" # encode item names. self . item_name_encoder = LabelEncoder () . fit ( self . main_data [ self . item_name_column ] . unique ()) # encode user indices. if self . user_index_column is not None : self . user_name_encoder = LabelEncoder () . fit ( self . main_data [ self . user_index_column ] . unique ()) # encode session indices. if self . session_index_column is not None : self . session_name_encoder = LabelEncoder () . fit ( self . main_data [ self . session_index_column ] . unique ())","title":"encode()"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.get_item_availability_tensor","text":"Get the item availability tensor from the main_data data-frame. Source code in torch_choice/utils/easy_data_wrapper.py def get_item_availability_tensor ( self ) -> torch . BoolTensor : \"\"\"Get the item availability tensor from the main_data data-frame.\"\"\" if self . session_index_column is None : raise ValueError ( f 'Item availability cannot be constructed without session index column.' ) A = self . main_data . pivot ( self . session_index_column , self . item_name_column , self . choice_column ) return torch . BoolTensor ( ~ np . isnan ( A . values ))","title":"get_item_availability_tensor()"},{"location":"api_torch_choice/#torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper.observable_data_to_observable_tensors","text":"Convert all self. _observable_data to self. _observable_tensors for PyTorch. Source code in torch_choice/utils/easy_data_wrapper.py def observable_data_to_observable_tensors ( self ) -> None : \"\"\"Convert all self.*_observable_data to self.*_observable_tensors for PyTorch.\"\"\" self . item_observable_tensors = dict () for key , val in self . item_observable_data . items (): assert all ( val . index == self . item_name_encoder . classes_ ), \"item observable data is not alighted with user name encoder.\" self . item_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . user_observable_tensors = dict () for key , val in self . user_observable_data . items (): assert all ( val . index == self . user_name_encoder . classes_ ), \"user observable data is not alighted with user name encoder.\" self . user_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . session_observable_tensors = dict () for key , val in self . session_observable_data . items (): assert all ( val . index == self . session_name_encoder . classes_ ), \"session observable data is not aligned with session name encoder.\" self . session_observable_tensors [ key ] = torch . tensor ( val . values , dtype = torch . float32 ) self . price_observable_tensors = dict () for key , val in self . price_observable_data . items (): tensor_slice = list () # if there are multiple columns (i.e., multiple observables) in the data-frame, we stack them together. for column in val . columns : df_slice = val . reset_index () . pivot ( index = self . session_index_column , columns = self . item_name_column , values = column ) tensor_slice . append ( torch . tensor ( df_slice . values , dtype = torch . float32 )) assert np . all ( df_slice . index == self . session_name_encoder . classes_ ) assert np . all ( df_slice . columns == self . item_name_encoder . classes_ ) # (num_sessions, num_items, num_params) self . price_observable_tensors [ key ] = torch . stack ( tensor_slice , dim =- 1 )","title":"observable_data_to_observable_tensors()"},{"location":"api_torch_choice/#torch_choice.utils.run_helper","text":"This is a template script for researchers to train the PyTorch-based model with minimal effort. The researcher only needs to initialize the dataset and the model, this training template comes with default hyper-parameters including batch size and learning rate. The researcher should experiment with different levels of hyper-parameter if the default setting doesn't converge well.","title":"run_helper"},{"location":"api_torch_choice/#torch_choice.utils.run_helper.run","text":"All in one script for the model training and result presentation. Source code in torch_choice/utils/run_helper.py def run ( model , dataset , batch_size =- 1 , learning_rate = 0.01 , num_epochs = 5000 ): \"\"\"All in one script for the model training and result presentation.\"\"\" assert isinstance ( model , ConditionalLogitModel ) or isinstance ( model , NestedLogitModel ), \\ f 'A model of type { type ( model ) } is not supported by this runner.' model = deepcopy ( model ) # do not modify the model outside. trained_model = deepcopy ( model ) # create another copy for returning. data_loader = data_utils . create_data_loader ( dataset , batch_size = batch_size , shuffle = True ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) print ( '=' * 20 , 'received model' , '=' * 20 ) print ( model ) print ( '=' * 20 , 'received dataset' , '=' * 20 ) print ( dataset ) print ( '=' * 20 , 'training the model' , '=' * 20 ) # fit the model. for e in range ( 1 , num_epochs + 1 ): # track the log-likelihood to minimize. ll , count = 0.0 , 0.0 for batch in data_loader : item_index = batch [ 'item' ] . item_index if isinstance ( model , NestedLogitModel ) else batch . item_index loss = model . negative_log_likelihood ( batch , item_index ) ll -= loss . detach () . item () # * len(batch) count += len ( batch ) optimizer . zero_grad () loss . backward () optimizer . step () # ll /= count if e % ( num_epochs // 10 ) == 0 : print ( f 'Epoch { e } : Log-likelihood= { ll } ' ) # current methods of computing standard deviation will corrupt the model, load weights into another model for returning. state_dict = deepcopy ( model . state_dict ()) trained_model . load_state_dict ( state_dict ) # get mean of estimation. mean_dict = dict () for k , v in model . named_parameters (): mean_dict [ k ] = v . clone () # estimate the standard error of the model. if isinstance ( model , ConditionalLogitModel ): def nll_loss ( model ): y_pred = model ( dataset ) return F . cross_entropy ( y_pred , dataset . item_index , reduction = 'sum' ) elif isinstance ( model , NestedLogitModel ): def nll_loss ( model ): d = dataset [ torch . arange ( len ( dataset ))] return model . negative_log_likelihood ( d , d [ 'item' ] . item_index ) std_dict = parameter_std ( model , nll_loss ) print ( '=' * 20 , 'model results' , '=' * 20 ) report = list () for coef_name , std in std_dict . items (): std = std . cpu () . detach () . numpy () mean = mean_dict [ coef_name ] . cpu () . detach () . numpy () coef_name = coef_name . replace ( 'coef_dict.' , '' ) . replace ( '.coef' , '' ) for i in range ( mean . size ): report . append ({ 'Coefficient' : coef_name + f '_ { i } ' , 'Estimation' : float ( mean [ i ]), 'Std. Err.' : float ( std [ i ])}) report = pd . DataFrame ( report ) . set_index ( 'Coefficient' ) print ( f 'Training Epochs: { num_epochs } \\n ' ) print ( f 'Learning Rate: { learning_rate } \\n ' ) print ( f 'Batch Size: { batch_size if batch_size != - 1 else len ( dataset ) } out of { len ( dataset ) } observations in total \\n ' ) print ( f 'Final Log-likelihood: { ll } \\n ' ) print ( 'Coefficients: \\n ' ) print ( report . to_markdown ()) return trained_model","title":"run()"},{"location":"api_torch_choice/#torch_choice.utils.std","text":"","title":"std"},{"location":"api_torch_choice/#torch_choice.utils.std.parameter_std","text":"This method firstly computes the Hessian of loss_fn(model_trained) with respect to model_trained.parameters(), then computes the standard error from the Hessian. NOTE: the current implementation involving deletion of attributes in model, this is an unsafe workaround for now. See https://github.com/pytorch/pytorch/issues/50138 for details. Parameters: Name Type Description Default model_trained nn.Module a trained pytorch model, the std estimated from Hessian only works if the model has been trained to optimal. required loss_fn callable the negatigve log-likelihood function (loss function). required return_hessian bool request to return hessian matrix as well. required Returns: Type Description [dict] a dictionary maps from keys in model_train.state_dict() to standard errors of esimations of each parameters in model_train.parameters(), shapes of values of returned dictionary is the same as shapes in model_train.state_dict(). [torch.Tensor]: optionally return the Hessian of loss_fn(model_trained) w.r.t. model_trained.parameters() Source code in torch_choice/utils/std.py def parameter_std ( model_trained : nn . Module , loss_fn : callable ) -> Tuple [ dict , Optional [ torch . Tensor ]]: \"\"\"This method firstly computes the Hessian of loss_fn(model_trained) with respect to model_trained.parameters(), then computes the standard error from the Hessian. NOTE: the current implementation involving deletion of attributes in model, this is an unsafe workaround for now. See https://github.com/pytorch/pytorch/issues/50138 for details. Args: model_trained (nn.Module): a trained pytorch model, the std estimated from Hessian only works if the model has been trained to optimal. loss_fn (callable): the negatigve log-likelihood function (loss function). return_hessian (bool): request to return hessian matrix as well. Returns: [dict]: a dictionary maps from keys in model_train.state_dict() to standard errors of esimations of each parameters in model_train.parameters(), shapes of values of returned dictionary is the same as shapes in model_train.state_dict(). [torch.Tensor]: optionally return the Hessian of loss_fn(model_trained) w.r.t. model_trained.parameters() \"\"\" # Need to make this safe. model = copy ( model_trained ) state_dict = deepcopy ( model . state_dict ()) shape , start , end = dict (), dict (), dict () param_list = list () s = 0 # wrap state dict into a single one dimensional tensor. for k , v in state_dict . items (): num_params = state_dict [ k ] . numel () start [ k ], end [ k ] = ( s , s + num_params ) s += num_params shape [ k ] = v . shape param_list . append ( v . clone () . view ( - 1 ,)) all_params = torch . cat ( param_list ) def func ( input_tensor ): # unwrap parameters. for k in state_dict . keys (): src = input_tensor [ start [ k ]: end [ k ]] . view ( * shape [ k ]) exec ( f 'del model. { k } ' ) exec ( f 'model. { k } = src' ) return loss_fn ( model ) H = torch . autograd . functional . hessian ( func , all_params ) std_all = torch . sqrt ( torch . diag ( torch . inverse ( H ))) std_dict = dict () for k in state_dict . keys (): std_dict [ k ] = std_all [ start [ k ]: end [ k ]] . view ( * shape [ k ]) . cpu () return std_dict","title":"parameter_std()"},{"location":"conditional_logit_model_mode_canada/","text":"Tutorial: Conditional Logit Model on ModeCanada Dataset Author: Tianyu Du (tianyudu@stanford.edu) Update: May. 3, 2022 Reference: This tutorial is modified from the Random utility model and the multinomial logit model in th documentation of mlogit package in R. Please note that the dataset involved in this example is fairly small (2,779 choice records), so we don't expect the performance to be faster than the R implementation. We provide this tutorial mainly to check the correctness of our prediction. The fully potential of PyTorch is better exploited on much larger dataset. The executable Jupyter notebook for this tutorial is located at Random Utility Model (RUM) 1: Conditional Logit Model . Let's first import essential Python packages. from time import time import numpy as np import pandas as pd import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , utils from torch_choice.model import ConditionalLogitModel from torch_choice.utils.run_helper import run This tutorial will run both with and without graphic processing unit (GPU). However, our package is much faster with GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) device = 'cuda' else : print ( 'Running tutorial on CPU.' ) device = 'cpu' Running tutorial on CPU. Load Dataset We have included the ModeCanada dataset in our package, which is located at ./public_datasets/ . The ModeCanada dataset contains individuals' choice on traveling methods. The raw dataset is in a long-format, in which the case variable identifies each choice. Using the terminology mentioned in the data management tutorial, each choice is called a purchasing record (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as \\(B\\) . For example, the first four row below (with case == 109 ) corresponds to the first choice, the alt column lists all alternatives/items available. The choice column identifies which alternative/item is chosen. The second row in the data snapshot below, we have choice == 1 and alt == 'air' for case == 109 . This indicates the travelling mode chosen in case = 109 was air . Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial. We focus on cases when four alternatives were available by filtering noalt == 4 . df = pd . read_csv ( './public_datasets/ModeCanada.csv' ) df = df . query ( 'noalt == 4' ) . reset_index ( drop = True ) df . sort_values ( by = 'case' , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 304 109 train 0 377 58.25 215 74 4 45 0 4 1 305 109 air 1 377 142.80 56 85 9 45 0 4 2 306 109 bus 0 377 27.52 301 63 8 45 0 4 3 307 109 car 0 377 71.63 262 0 0 45 0 4 4 308 110 train 0 377 58.25 215 74 4 70 0 4 Since there are 4 rows corresponding to each purchasing record , the length of the long-format data is \\(4 \\times B\\) . Please refer to the data management tutorial for notations. df . shape (11116, 12) Construct the item_index tensor The first thing is to construct the item_index tensor identifying which item (i.e., travel mode) was chosen in each purchasing record. We can now construct the item_index array containing which item was chosen in each purchasing record. item_index = df [ df [ 'choice' ] == 1 ] . sort_values ( by = 'case' )[ 'alt' ] . reset_index ( drop = True ) print ( item_index ) 0 air 1 air 2 air 3 air 4 air ... 2774 car 2775 car 2776 car 2777 car 2778 car Name: alt, Length: 2779, dtype: object Since we will be training our model using PyTorch , we need to encode {'air', 'bus', 'car', 'train'} into integer values. Travel Mode Name Encoded Integer Values air 0 bus 1 car 2 train 3 The generated item_index would be a tensor of shape 2,778 (i.e., number of purchasing records in this dataset) with values {0, 1, 2, 3} . item_names = [ 'air' , 'bus' , 'car' , 'train' ] num_items = 4 encoder = dict ( zip ( item_names , range ( num_items ))) print ( f \" { encoder =:} \" ) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) print ( f \" { item_index =:} \" ) encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3} item_index=tensor([0, 0, 0, ..., 2, 2, 2]) Construct Observables Then let's constrct tensors for observables. As mentioned in the data management tutorial, the session is capturing the temporal dimension of our data. Since we have different values cost , freq and ovt for each purchasing record and for each item, it's natural to say each purchasing record has its own session. Consequently, these three variables are price observables since they vary by both item and session. The tensor holding these observables has shape \\((\\text{numer of purchasing records}, \\text{number of items}, 3)\\) We do the same for variable ivt , we put ivt into a separate tensor because we want to model its coefficient differently later. price_cost_freq_ovt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = [ 'cost' , 'freq' , 'ovt' ]) print ( f ' { price_cost_freq_ovt . shape =:} ' ) price_ivt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = 'ivt' ) print ( f ' { price_ivt . shape =:} ' ) price_cost_freq_ovt.shape=torch.Size([2779, 4, 3]) price_ivt.shape=torch.Size([2779, 4, 1]) In contrast, the income variable varies only by session (i.e., purchasing record), but not by item. income is therefore naturally a session variable. session_income = df . groupby ( 'case' )[ 'income' ] . first () session_income = torch . Tensor ( session_income . values ) . view ( - 1 , 1 ) print ( f ' { session_income . shape =:} ' ) session_income.shape=torch.Size([2779, 1]) To summarize, the ChoiceDataset constructed contains 2779 choice records. Since the original dataset did not reveal the identity of each decision maker, we consider all 2779 choices were made by a single user but in 2779 different sessions to handle variations. In this case, the cost , freq and ovt are observables depending on both sessions and items, we created a price_cost_freq_ovt tensor with shape (num_sessions, num_items, 3) = (2779, 4, 3) to contain these variables. In contrast, the income information depends only on session but not on items, hence we create the session_income tensor to store it. Because we wish to fit item-specific coefficients for the ivt variable, which varies by both sessions and items as well, we create another price_ivt tensor in addition to the price_cost_freq_ovt tensor. Lastly, we put all tensors we created to a single ChoiceDataset object, and move the dataset to the appropriate device. dataset = ChoiceDataset ( item_index = item_index , price_cost_freq_ovt = price_cost_freq_ovt , session_income = session_income , price_ivt = price_ivt ) . to ( device ) You can print(dataset) to check shapes of tensors contained in the ChoiceDataset . print ( dataset ) ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) Create the Model We now construct the ConditionalLogitModel to fit the dataset we constructed above. To start with, we aim to estimate the following model formulation: \\[ U_{uit} = \\beta^0_i + \\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{price:ivt} + \\epsilon_{uit} \\] We now initialize the ConditionalLogitModel to predict choices from the dataset. Please see the documentation for a complete description of the ConditionalLogitModel class. At it's core, the ConditionalLogitModel constructor requires the following four components. Define variation of each \\(\\beta\\) using coef_variation_dict The keyword coef_variation_dict is a dictionary with variable names (defined above while constructing the dataset) as keys and values from {constant, user, item, item-full} . For instance, since we wish to have constant coefficients for cost , freq and ovt observables, and these three observables are stored in the price_cost_freq_ovt tensor of the choice dataset, we set coef_variation_dict['price_cost_freq_ovt'] = 'constant' (corresponding to the \\(\\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it}\\) term above). The models allows for the option of zeroing coefficient for one item. The variation of \\(\\beta^3\\) above is specified as item-full which indicates 4 values of \\(\\beta^3\\) is learned (one for each item). In contrast, \\(\\beta^0, \\beta^2\\) are specified to have variation item instead of item-full . In this case, the \\(\\beta\\) correspond to the first item (i.e., the baseline item, which is encoded as 0 in the label tensor, air in our example) is force to be zero. The researcher needs to declare intercept explicitly for the model to fit an intercept as well, otherwise the model assumes zero intercept term. Define the dimension of each \\(\\beta\\) using num_param_dict The num_param_dict is a dictionary with keys exactly the same as the coef_variation_dict . Each of dictionary values tells the dimension of the corresponding observables, hence the dimension of the coefficient. For example, the price_cost_freq_ovt consists of three observables and we set the corresponding to three. Even the model can infer num_param_dict['intercept'] = 1 , but we recommend the research to include it for completeness. Number of items The num_items keyword informs the model how many alternatives users are choosing from. Number of users The num_users keyword is an optional integer informing the model how many users there are in the dataset. However, in this example we implicitly assume there is only one user making all the decisions and we do not have any user_obs involved, hence num_users argument is not supplied. model = ConditionalLogitModel ( coef_variation_dict = { 'price_cost_freq_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_param_dict = { 'price_cost_freq_ovt' : 3 , 'session_income' : 1 , 'price_ivt' : 1 , 'intercept' : 1 }, num_items = 4 ) Then we move the model to the appropriate device. model = model . to ( device ) One can print the ConditionalLogitModel object to obtain a summary of the model. print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total). (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total). (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt] with 3 parameters, with constant level variation. X[session_income] with 1 parameters, with item level variation. X[price_ivt] with 1 parameters, with item-full level variation. X[intercept] with 1 parameters, with item level variation. Train the Model We provide an easy-to-use helper function run() imported from torch_choice.utils.run_helper to fit the model with a particular dataset. We provide an easy-to-use model runner for both ConditionalLogitModel and NestedLogitModel (see later) instances. The run() mehtod supports mini-batch updating as well, for small datasets like the one we are dealing right now, we can use batch_size = -1 to conduct full-batch gradient update. start_time = time () run ( model , dataset , num_epochs = 50000 , learning_rate = 0.01 , batch_size =- 1 ) print ( 'Time taken:' , time () - start_time ) ==================== received model ==================== ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total). (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total). (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt] with 3 parameters, with constant level variation. X[session_income] with 1 parameters, with item level variation. X[price_ivt] with 1 parameters, with item-full level variation. X[intercept] with 1 parameters, with item level variation. ==================== received dataset ==================== ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) ==================== training the model ==================== Epoch 5000: Log-likelihood=-1875.552490234375 Epoch 10000: Log-likelihood=-1892.94775390625 Epoch 15000: Log-likelihood=-1877.9156494140625 Epoch 20000: Log-likelihood=-1881.0845947265625 Epoch 25000: Log-likelihood=-1884.7335205078125 Epoch 30000: Log-likelihood=-1874.423828125 Epoch 35000: Log-likelihood=-1875.3016357421875 Epoch 40000: Log-likelihood=-1874.3779296875 Epoch 45000: Log-likelihood=-1875.703125 Epoch 50000: Log-likelihood=-1899.8175048828125 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.01 Batch Size: 2779 out of 2779 observations in total Final Log-likelihood: -1899.8175048828125 Coefficients: | Coefficient | Estimation | Std. Err. | |:----------------------|-------------:|------------:| | price_cost_freq_ovt_0 | -0.0342194 | 0.00731707 | | price_cost_freq_ovt_1 | 0.092262 | 0.00520946 | | price_cost_freq_ovt_2 | -0.0439827 | 0.00342765 | | session_income_0 | -0.0901207 | 0.0205214 | | session_income_1 | -0.0272581 | 0.00385396 | | session_income_2 | -0.0390468 | 0.00428838 | | price_ivt_0 | 0.0592097 | 0.0102933 | | price_ivt_1 | -0.00753696 | 0.00496264 | | price_ivt_2 | -0.00604297 | 0.00193414 | | price_ivt_3 | -0.00207518 | 0.00123286 | | intercept_0 | 0.700786 | 1.39368 | | intercept_1 | 1.85016 | 0.728283 | | intercept_2 | 3.2782 | 0.648064 | Time taken: 179.84411025047302 Parameter Estimation from R The following is the R-output from the mlogit implementation, the estimation, standard error, and log-likelihood from our torch_choice implementation is the same as the result from mlogit implementation. We see that the final log-likelihood of models estimated using two packages are all around -1874 . The run() method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters. Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages. Coefficient Name in Python Estimation Std. Err. Coeffcient Name in R R Estimation R Std. Err. price_cost_freq_ovt_0 -0.0342194 0.00731707 cost -0.0333389 0.0070955 price_cost_freq_ovt_1 0.092262 0.00520946 freq 0.0925297 0.0050976 price_cost_freq_ovt_2 -0.0439827 0.00342765 ovt -0.0430036 0.0032247 session_income_0 -0.0901207 0.0205214 income:bus -0.0890867 0.0183471 session_income_1 -0.0272581 0.00385396 income:car -0.0279930 0.0038726 session_income_2 -0.0390468 0.00428838 ivt:train -0.0014504 0.0011875 price_ivt_0 0.0592097 0.0102933 ivt:air 0.0595097 0.0100727 price_ivt_1 -0.00753696 0.00496264 ivt:bus -0.0067835 0.0044334 price_ivt_2 -0.00604297 0.00193414 ivt:car -0.0064603 0.0018985 price_ivt_3 -0.00207518 0.00123286 ivt:train -0.0014504 0.0011875 intercept_0 0.700786 1.39368 (Intercept):bus 0.6983381 1.2802466 intercept_1 1.85016 0.728283 (Intercept):car 1.8441129 0.7085089 intercept_2 3.2782 0.648064 (Intercept):train 3.2741952 0.6244152 R Output install.packages ( \"mlogit\" ) library ( \"mlogit\" ) data ( \"ModeCanada\" , package = \"mlogit\" ) MC <- dfidx ( ModeCanada , subset = noalt == 4 ) ml.MC1 <- mlogit ( choice ~ cost + freq + ovt | income | ivt , MC , reflevel = 'air' ) summary ( ml.MC1 ) Call: mlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, reflevel = \"air\", method = \"nr\") Frequencies of alternatives:choice air train bus car 0.3738755 0.1666067 0.0035984 0.4559194 nr method 9 iterations, 0h:0m:0s g'(-H)^-1g = 0.00014 successive function values within tolerance limits Coefficients : Estimate Std. Error z-value Pr(>|z|) (Intercept):train 3.2741952 0.6244152 5.2436 1.575e-07 *** (Intercept):bus 0.6983381 1.2802466 0.5455 0.5854292 (Intercept):car 1.8441129 0.7085089 2.6028 0.0092464 ** cost -0.0333389 0.0070955 -4.6986 2.620e-06 *** freq 0.0925297 0.0050976 18.1517 < 2.2e-16 *** ovt -0.0430036 0.0032247 -13.3356 < 2.2e-16 *** income:train -0.0381466 0.0040831 -9.3426 < 2.2e-16 *** income:bus -0.0890867 0.0183471 -4.8556 1.200e-06 *** income:car -0.0279930 0.0038726 -7.2286 4.881e-13 *** ivt:air 0.0595097 0.0100727 5.9080 3.463e-09 *** ivt:train -0.0014504 0.0011875 -1.2214 0.2219430 ivt:bus -0.0067835 0.0044334 -1.5301 0.1259938 ivt:car -0.0064603 0.0018985 -3.4029 0.0006668 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Log-Likelihood: -1874.3 McFadden R^2: 0.35443 Likelihood ratio test : chisq = 2058.1 (p.value = < 2.22e-16)","title":"Tutorial for Conditional Logit Model"},{"location":"conditional_logit_model_mode_canada/#tutorial-conditional-logit-model-on-modecanada-dataset","text":"Author: Tianyu Du (tianyudu@stanford.edu) Update: May. 3, 2022 Reference: This tutorial is modified from the Random utility model and the multinomial logit model in th documentation of mlogit package in R. Please note that the dataset involved in this example is fairly small (2,779 choice records), so we don't expect the performance to be faster than the R implementation. We provide this tutorial mainly to check the correctness of our prediction. The fully potential of PyTorch is better exploited on much larger dataset. The executable Jupyter notebook for this tutorial is located at Random Utility Model (RUM) 1: Conditional Logit Model . Let's first import essential Python packages. from time import time import numpy as np import pandas as pd import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , utils from torch_choice.model import ConditionalLogitModel from torch_choice.utils.run_helper import run This tutorial will run both with and without graphic processing unit (GPU). However, our package is much faster with GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) device = 'cuda' else : print ( 'Running tutorial on CPU.' ) device = 'cpu' Running tutorial on CPU.","title":"Tutorial: Conditional Logit Model on ModeCanada Dataset"},{"location":"conditional_logit_model_mode_canada/#load-dataset","text":"We have included the ModeCanada dataset in our package, which is located at ./public_datasets/ . The ModeCanada dataset contains individuals' choice on traveling methods. The raw dataset is in a long-format, in which the case variable identifies each choice. Using the terminology mentioned in the data management tutorial, each choice is called a purchasing record (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as \\(B\\) . For example, the first four row below (with case == 109 ) corresponds to the first choice, the alt column lists all alternatives/items available. The choice column identifies which alternative/item is chosen. The second row in the data snapshot below, we have choice == 1 and alt == 'air' for case == 109 . This indicates the travelling mode chosen in case = 109 was air . Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial. We focus on cases when four alternatives were available by filtering noalt == 4 . df = pd . read_csv ( './public_datasets/ModeCanada.csv' ) df = df . query ( 'noalt == 4' ) . reset_index ( drop = True ) df . sort_values ( by = 'case' , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 304 109 train 0 377 58.25 215 74 4 45 0 4 1 305 109 air 1 377 142.80 56 85 9 45 0 4 2 306 109 bus 0 377 27.52 301 63 8 45 0 4 3 307 109 car 0 377 71.63 262 0 0 45 0 4 4 308 110 train 0 377 58.25 215 74 4 70 0 4 Since there are 4 rows corresponding to each purchasing record , the length of the long-format data is \\(4 \\times B\\) . Please refer to the data management tutorial for notations. df . shape (11116, 12)","title":"Load Dataset"},{"location":"conditional_logit_model_mode_canada/#construct-the-item_index-tensor","text":"The first thing is to construct the item_index tensor identifying which item (i.e., travel mode) was chosen in each purchasing record. We can now construct the item_index array containing which item was chosen in each purchasing record. item_index = df [ df [ 'choice' ] == 1 ] . sort_values ( by = 'case' )[ 'alt' ] . reset_index ( drop = True ) print ( item_index ) 0 air 1 air 2 air 3 air 4 air ... 2774 car 2775 car 2776 car 2777 car 2778 car Name: alt, Length: 2779, dtype: object Since we will be training our model using PyTorch , we need to encode {'air', 'bus', 'car', 'train'} into integer values. Travel Mode Name Encoded Integer Values air 0 bus 1 car 2 train 3 The generated item_index would be a tensor of shape 2,778 (i.e., number of purchasing records in this dataset) with values {0, 1, 2, 3} . item_names = [ 'air' , 'bus' , 'car' , 'train' ] num_items = 4 encoder = dict ( zip ( item_names , range ( num_items ))) print ( f \" { encoder =:} \" ) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) print ( f \" { item_index =:} \" ) encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3} item_index=tensor([0, 0, 0, ..., 2, 2, 2])","title":"Construct the item_index tensor"},{"location":"conditional_logit_model_mode_canada/#construct-observables","text":"Then let's constrct tensors for observables. As mentioned in the data management tutorial, the session is capturing the temporal dimension of our data. Since we have different values cost , freq and ovt for each purchasing record and for each item, it's natural to say each purchasing record has its own session. Consequently, these three variables are price observables since they vary by both item and session. The tensor holding these observables has shape \\((\\text{numer of purchasing records}, \\text{number of items}, 3)\\) We do the same for variable ivt , we put ivt into a separate tensor because we want to model its coefficient differently later. price_cost_freq_ovt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = [ 'cost' , 'freq' , 'ovt' ]) print ( f ' { price_cost_freq_ovt . shape =:} ' ) price_ivt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = 'ivt' ) print ( f ' { price_ivt . shape =:} ' ) price_cost_freq_ovt.shape=torch.Size([2779, 4, 3]) price_ivt.shape=torch.Size([2779, 4, 1]) In contrast, the income variable varies only by session (i.e., purchasing record), but not by item. income is therefore naturally a session variable. session_income = df . groupby ( 'case' )[ 'income' ] . first () session_income = torch . Tensor ( session_income . values ) . view ( - 1 , 1 ) print ( f ' { session_income . shape =:} ' ) session_income.shape=torch.Size([2779, 1]) To summarize, the ChoiceDataset constructed contains 2779 choice records. Since the original dataset did not reveal the identity of each decision maker, we consider all 2779 choices were made by a single user but in 2779 different sessions to handle variations. In this case, the cost , freq and ovt are observables depending on both sessions and items, we created a price_cost_freq_ovt tensor with shape (num_sessions, num_items, 3) = (2779, 4, 3) to contain these variables. In contrast, the income information depends only on session but not on items, hence we create the session_income tensor to store it. Because we wish to fit item-specific coefficients for the ivt variable, which varies by both sessions and items as well, we create another price_ivt tensor in addition to the price_cost_freq_ovt tensor. Lastly, we put all tensors we created to a single ChoiceDataset object, and move the dataset to the appropriate device. dataset = ChoiceDataset ( item_index = item_index , price_cost_freq_ovt = price_cost_freq_ovt , session_income = session_income , price_ivt = price_ivt ) . to ( device ) You can print(dataset) to check shapes of tensors contained in the ChoiceDataset . print ( dataset ) ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)","title":"Construct Observables"},{"location":"conditional_logit_model_mode_canada/#create-the-model","text":"We now construct the ConditionalLogitModel to fit the dataset we constructed above. To start with, we aim to estimate the following model formulation: \\[ U_{uit} = \\beta^0_i + \\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{price:ivt} + \\epsilon_{uit} \\] We now initialize the ConditionalLogitModel to predict choices from the dataset. Please see the documentation for a complete description of the ConditionalLogitModel class. At it's core, the ConditionalLogitModel constructor requires the following four components.","title":"Create the Model"},{"location":"conditional_logit_model_mode_canada/#define-variation-of-each-beta-using-coef_variation_dict","text":"The keyword coef_variation_dict is a dictionary with variable names (defined above while constructing the dataset) as keys and values from {constant, user, item, item-full} . For instance, since we wish to have constant coefficients for cost , freq and ovt observables, and these three observables are stored in the price_cost_freq_ovt tensor of the choice dataset, we set coef_variation_dict['price_cost_freq_ovt'] = 'constant' (corresponding to the \\(\\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it}\\) term above). The models allows for the option of zeroing coefficient for one item. The variation of \\(\\beta^3\\) above is specified as item-full which indicates 4 values of \\(\\beta^3\\) is learned (one for each item). In contrast, \\(\\beta^0, \\beta^2\\) are specified to have variation item instead of item-full . In this case, the \\(\\beta\\) correspond to the first item (i.e., the baseline item, which is encoded as 0 in the label tensor, air in our example) is force to be zero. The researcher needs to declare intercept explicitly for the model to fit an intercept as well, otherwise the model assumes zero intercept term.","title":"Define variation of each \\(\\beta\\) using coef_variation_dict"},{"location":"conditional_logit_model_mode_canada/#define-the-dimension-of-each-beta-using-num_param_dict","text":"The num_param_dict is a dictionary with keys exactly the same as the coef_variation_dict . Each of dictionary values tells the dimension of the corresponding observables, hence the dimension of the coefficient. For example, the price_cost_freq_ovt consists of three observables and we set the corresponding to three. Even the model can infer num_param_dict['intercept'] = 1 , but we recommend the research to include it for completeness.","title":"Define the dimension of each \\(\\beta\\) using num_param_dict"},{"location":"conditional_logit_model_mode_canada/#number-of-items","text":"The num_items keyword informs the model how many alternatives users are choosing from.","title":"Number of items"},{"location":"conditional_logit_model_mode_canada/#number-of-users","text":"The num_users keyword is an optional integer informing the model how many users there are in the dataset. However, in this example we implicitly assume there is only one user making all the decisions and we do not have any user_obs involved, hence num_users argument is not supplied. model = ConditionalLogitModel ( coef_variation_dict = { 'price_cost_freq_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_param_dict = { 'price_cost_freq_ovt' : 3 , 'session_income' : 1 , 'price_ivt' : 1 , 'intercept' : 1 }, num_items = 4 ) Then we move the model to the appropriate device. model = model . to ( device ) One can print the ConditionalLogitModel object to obtain a summary of the model. print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total). (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total). (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt] with 3 parameters, with constant level variation. X[session_income] with 1 parameters, with item level variation. X[price_ivt] with 1 parameters, with item-full level variation. X[intercept] with 1 parameters, with item level variation.","title":"Number of users"},{"location":"conditional_logit_model_mode_canada/#train-the-model","text":"We provide an easy-to-use helper function run() imported from torch_choice.utils.run_helper to fit the model with a particular dataset. We provide an easy-to-use model runner for both ConditionalLogitModel and NestedLogitModel (see later) instances. The run() mehtod supports mini-batch updating as well, for small datasets like the one we are dealing right now, we can use batch_size = -1 to conduct full-batch gradient update. start_time = time () run ( model , dataset , num_epochs = 50000 , learning_rate = 0.01 , batch_size =- 1 ) print ( 'Time taken:' , time () - start_time ) ==================== received model ==================== ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total). (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total). (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt] with 3 parameters, with constant level variation. X[session_income] with 1 parameters, with item level variation. X[price_ivt] with 1 parameters, with item-full level variation. X[intercept] with 1 parameters, with item level variation. ==================== received dataset ==================== ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) ==================== training the model ==================== Epoch 5000: Log-likelihood=-1875.552490234375 Epoch 10000: Log-likelihood=-1892.94775390625 Epoch 15000: Log-likelihood=-1877.9156494140625 Epoch 20000: Log-likelihood=-1881.0845947265625 Epoch 25000: Log-likelihood=-1884.7335205078125 Epoch 30000: Log-likelihood=-1874.423828125 Epoch 35000: Log-likelihood=-1875.3016357421875 Epoch 40000: Log-likelihood=-1874.3779296875 Epoch 45000: Log-likelihood=-1875.703125 Epoch 50000: Log-likelihood=-1899.8175048828125 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.01 Batch Size: 2779 out of 2779 observations in total Final Log-likelihood: -1899.8175048828125 Coefficients: | Coefficient | Estimation | Std. Err. | |:----------------------|-------------:|------------:| | price_cost_freq_ovt_0 | -0.0342194 | 0.00731707 | | price_cost_freq_ovt_1 | 0.092262 | 0.00520946 | | price_cost_freq_ovt_2 | -0.0439827 | 0.00342765 | | session_income_0 | -0.0901207 | 0.0205214 | | session_income_1 | -0.0272581 | 0.00385396 | | session_income_2 | -0.0390468 | 0.00428838 | | price_ivt_0 | 0.0592097 | 0.0102933 | | price_ivt_1 | -0.00753696 | 0.00496264 | | price_ivt_2 | -0.00604297 | 0.00193414 | | price_ivt_3 | -0.00207518 | 0.00123286 | | intercept_0 | 0.700786 | 1.39368 | | intercept_1 | 1.85016 | 0.728283 | | intercept_2 | 3.2782 | 0.648064 | Time taken: 179.84411025047302","title":"Train the Model"},{"location":"conditional_logit_model_mode_canada/#parameter-estimation-from-r","text":"The following is the R-output from the mlogit implementation, the estimation, standard error, and log-likelihood from our torch_choice implementation is the same as the result from mlogit implementation. We see that the final log-likelihood of models estimated using two packages are all around -1874 . The run() method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters. Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages. Coefficient Name in Python Estimation Std. Err. Coeffcient Name in R R Estimation R Std. Err. price_cost_freq_ovt_0 -0.0342194 0.00731707 cost -0.0333389 0.0070955 price_cost_freq_ovt_1 0.092262 0.00520946 freq 0.0925297 0.0050976 price_cost_freq_ovt_2 -0.0439827 0.00342765 ovt -0.0430036 0.0032247 session_income_0 -0.0901207 0.0205214 income:bus -0.0890867 0.0183471 session_income_1 -0.0272581 0.00385396 income:car -0.0279930 0.0038726 session_income_2 -0.0390468 0.00428838 ivt:train -0.0014504 0.0011875 price_ivt_0 0.0592097 0.0102933 ivt:air 0.0595097 0.0100727 price_ivt_1 -0.00753696 0.00496264 ivt:bus -0.0067835 0.0044334 price_ivt_2 -0.00604297 0.00193414 ivt:car -0.0064603 0.0018985 price_ivt_3 -0.00207518 0.00123286 ivt:train -0.0014504 0.0011875 intercept_0 0.700786 1.39368 (Intercept):bus 0.6983381 1.2802466 intercept_1 1.85016 0.728283 (Intercept):car 1.8441129 0.7085089 intercept_2 3.2782 0.648064 (Intercept):train 3.2741952 0.6244152","title":"Parameter Estimation from R"},{"location":"conditional_logit_model_mode_canada/#r-output","text":"install.packages ( \"mlogit\" ) library ( \"mlogit\" ) data ( \"ModeCanada\" , package = \"mlogit\" ) MC <- dfidx ( ModeCanada , subset = noalt == 4 ) ml.MC1 <- mlogit ( choice ~ cost + freq + ovt | income | ivt , MC , reflevel = 'air' ) summary ( ml.MC1 ) Call: mlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, reflevel = \"air\", method = \"nr\") Frequencies of alternatives:choice air train bus car 0.3738755 0.1666067 0.0035984 0.4559194 nr method 9 iterations, 0h:0m:0s g'(-H)^-1g = 0.00014 successive function values within tolerance limits Coefficients : Estimate Std. Error z-value Pr(>|z|) (Intercept):train 3.2741952 0.6244152 5.2436 1.575e-07 *** (Intercept):bus 0.6983381 1.2802466 0.5455 0.5854292 (Intercept):car 1.8441129 0.7085089 2.6028 0.0092464 ** cost -0.0333389 0.0070955 -4.6986 2.620e-06 *** freq 0.0925297 0.0050976 18.1517 < 2.2e-16 *** ovt -0.0430036 0.0032247 -13.3356 < 2.2e-16 *** income:train -0.0381466 0.0040831 -9.3426 < 2.2e-16 *** income:bus -0.0890867 0.0183471 -4.8556 1.200e-06 *** income:car -0.0279930 0.0038726 -7.2286 4.881e-13 *** ivt:air 0.0595097 0.0100727 5.9080 3.463e-09 *** ivt:train -0.0014504 0.0011875 -1.2214 0.2219430 ivt:bus -0.0067835 0.0044334 -1.5301 0.1259938 ivt:car -0.0064603 0.0018985 -3.4029 0.0006668 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Log-Likelihood: -1874.3 McFadden R^2: 0.35443 Likelihood ratio test : chisq = 2058.1 (p.value = < 2.22e-16)","title":"R Output"},{"location":"data_management/","text":"Tutorial: Data Management Author: Tianyu Du (tianyudu@stanford.edu) Note : please go through the introduction tutorial here before proceeding. This notebook aims to help users understand the functionality of ChoiceDataset object. The ChoiceDataset is an instance of the more general PyTorch dataset object holding information of consumer choices. The ChoiceDataset offers easy, clean and efficient data management. The Jupyter-notebook version of this tutorial can be found here . This tutorial provides in-depth explanations on how the torch-choice library manages data. We are also providing an easy-to-use data wrapper converting long-format dataset to ChoiceDataset here , you can harness the torch-choice library without going through this tutorial. Note : since this package was initially proposed for modelling consumer choices, attribute names of ChoiceDataset are borrowed from the consumer choice literature. Observables Observables are tensors with specific shapes, we classify observables into four categories based on their variations. Basic Usage Optionally, the researcher can incorporate observables of, for example, users and items. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. The researcher should supply them with as appropriate keyword arguments while constructing the ChoiceDataset object. Advanced Usage: Additional Observables In some cases, the researcher may wish to handle different parts of user_obs (or other observable tensors) differently. For example, the researcher wishes to model the utility for user \\(u\\) to purchase item \\(i\\) in session \\(s\\) as the following: \\[ U_{usi} = \\beta_{i} X^{(u)}_{user\\ income} + \\gamma X^{(u)}_{user\\ market\\ membership} \\] The coefficient for user income is item-specific so that it captures the nature of the product (i.e., a luxury or an essential good). Additionally, the utility representation admits an user market membership becomes shoppers with active memberships tend to purchase more, and the coefficient of this term is constant across all items. As we will cover later in the modelling section, we need to supply two user observable tensors in this case for the model to build coefficient with different levels of variations (i.e., item-specific coefficients versus constant coefficients). In this case, the researcher needs to supply two tensors user_income and user_market_membership as keyword arguments to the ChoiceDataset constructor. The ChoiceDataset handles multiple user/item/session/price observables internally, for example, every keyword arguments passed into ChoiceDataset with name starting with item_ (except for the reserved item_availability ) will be treated as item observable tensors. All keywords with names starting user_ , session_ and price_ (except for reserved names like user_index and session_index mentioned above) will be interpreted as user/session/price observable tensors. # import required dependencies. import numpy as np import torch from torch_choice.data import ChoiceDataset , JointDataset # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' ) Creating ChoiceDataset Object # Feel free to modify it as you want. num_users = 10 num_items = 4 num_sessions = 500 length_of_dataset = 10000 Step 1: Generate some random purchase records and observables We will be creating a randomly generated dataset with 10000 purchase records from 10 users, 4 items and 500 sessions. The first step is to randomly generate the purchase records using the following code. For simplicity, we assume all items are available in all sessions. # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. price_obs = torch . randn ( num_sessions , num_items , 12 ) We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen. item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool () Step 2: Initialize the ChoiceDataset . You can construct a choice set using the following code, which manage all information for you. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs ) What you can do with the ChoiceDataset ? print(dataset) and dataset.__str__ The command print(dataset) will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory). print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) dataset.num_{users, items, sessions} You can use the num_{users, items, sessions} attribute to obtain the number of users, items, and sessions, they are determined automatically from the {user, item, session}_obs tensors provided while initializing the dataset object. Note : the print =: operator requires Python3.8 or higher, you can remove =: if you are using an earlier copy of Python. print ( f ' { dataset . num_users =:} ' ) print ( f ' { dataset . num_items =:} ' ) print ( f ' { dataset . num_sessions =:} ' ) print ( f ' { len ( dataset ) =:} ' ) dataset.num_users=10 dataset.num_items=4 dataset.num_sessions=500 len(dataset)=10000 dataset.clone() The ChoiceDataset offers a clone method allow you to make copy of the dataset, you can modify the cloned dataset arbitrarily without changing the original dataset. # clone print ( dataset . item_index [: 10 ]) dataset_cloned = dataset . clone () dataset_cloned . item_index = 99 * torch . ones ( num_sessions ) print ( dataset_cloned . item_index [: 10 ]) print ( dataset . item_index [: 10 ]) # does not change the original dataset. tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) tensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.]) tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) dataset.to('cuda') and dataset._check_device_consistency() . One key advantage of the torch_choice and bemb is their compatibility with GPUs, you can easily move tensors in a ChoiceDataset object between host memory (i.e., cpu memory) and device memory (i.e., GPU memory) using dataset.to() method. Please note that the following code runs only if your machine has a compatible GPU and GPU-compatible version of PyTorch installed. Similarly, one can move data to host-memory using dataset.to('cpu') . The dataset also provides a dataset._check_device_consistency() method to check if all tensors are on the same device. If we only move the label to cpu without moving other tensors, this will result in an error message. # move to device print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset = dataset . to ( 'cuda' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . item_index . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset.device=cpu dataset.device=cpu dataset.user_index.device=cpu dataset.session_index.device=cpu dataset.device=cuda:0 dataset.item_index.device=cuda:0 dataset.user_index.device=cuda:0 dataset.session_index.device=cuda:0 dataset . _check_device_consistency () # # NOTE: this cell will result errors, this is intentional. dataset . item_index = dataset . item_index . to ( 'cpu' ) dataset . _check_device_consistency () --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-56-40d626c6d436> in <module> 1 # # NOTE: this cell will result errors, this is intentional. 2 dataset.item_index = dataset.item_index.to('cpu') ----> 3 dataset._check_device_consistency() ~/Development/torch-choice/torch_choice/data/choice_dataset.py in _check_device_consistency(self) 180 devices.append(val.device) 181 if len(set(devices)) > 1: --> 182 raise Exception(f'Found tensors on different devices: {set(devices)}.', 183 'Use dataset.to() method to align devices.') 184 Exception: (\"Found tensors on different devices: {device(type='cuda', index=0), device(type='cpu')}.\", 'Use dataset.to() method to align devices.') # create dictionary inputs for model.forward() # collapse to a dictionary object. print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) Subset method One can use dataset[indices] with indices as an integer-valued tensor or array to get the corresponding rows of the dataset. The example code block below queries the 6256-th, 4119-th, 453-th, 5520-th, and 1877-th row of the dataset object. The item_index , user_index , session_index of the resulted subset will be different from the original dataset, but other tensors will be the same. # __getitem__ to get batch. # pick 5 random sessions as the mini-batch. dataset = dataset . to ( 'cpu' ) indices = torch . Tensor ( np . random . choice ( len ( dataset ), size = 5 , replace = False )) . long () print ( indices ) subset = dataset [ indices ] print ( dataset ) print ( subset ) # print_dict_shape(subset.x_dict) # assert torch.all(dataset.x_dict['price_obs'][indices, :, :] == subset.x_dict['price_obs']) # assert torch.all(dataset.item_index[indices] == subset.item_index) tensor([1118, 976, 1956, 290, 8283]) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) ChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) The subset method internally creates a copy of the datasets so that any modification applied on the subset will not be reflected on the original dataset. The researcher can feel free to do in-place modification to the subset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) subset . item_index += 1 # modifying the batch does not change the original dataset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) tensor([0, 1, 0, 0, 0]) tensor([0, 1, 0, 0, 0]) tensor([1, 2, 1, 1, 1]) tensor([0, 1, 0, 0, 0]) print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) subset . item_obs += 1 print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) tensor(-1.5811) tensor(-1.5811) tensor(-0.5811) tensor(-1.5811) print ( id ( subset . item_index )) print ( id ( dataset . item_index [ indices ])) 140339656298640 140339656150528 Using Pytorch dataloader for the training loop. The ChoiceDataset object natively support batch samplers from PyTorch. For demonstration purpose, we turned off the shuffling option. from torch.utils.data.sampler import BatchSampler , SequentialSampler , RandomSampler shuffle = False # for demonstration purpose. batch_size = 32 # Create sampler. sampler = BatchSampler ( RandomSampler ( dataset ) if shuffle else SequentialSampler ( dataset ), batch_size = batch_size , drop_last = False ) dataloader = torch . utils . data . DataLoader ( dataset , sampler = sampler , num_workers = 1 , collate_fn = lambda x : x [ 0 ], pin_memory = ( dataset . device == 'cpu' )) print ( f ' { item_obs . shape =:} ' ) item_obs_all = item_obs . view ( 1 , num_items , - 1 ) . expand ( len ( dataset ), - 1 , - 1 ) item_obs_all = item_obs_all . to ( dataset . device ) item_index_all = item_index . to ( dataset . device ) print ( f ' { item_obs_all . shape =:} ' ) item_obs.shape=torch.Size([4, 64]) item_obs_all.shape=torch.Size([10000, 4, 64]) for i , batch in enumerate ( dataloader ): first , last = i * batch_size , min ( len ( dataset ), ( i + 1 ) * batch_size ) idx = torch . arange ( first , last ) assert torch . all ( item_obs_all [ idx , :, :] == batch . x_dict [ 'item_obs' ]) assert torch . all ( item_index_all [ idx ] == batch . item_index ) batch . x_dict [ 'item_obs' ] . shape torch.Size([16, 4, 64]) print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) dataset . __len__ () 10000 Chaining Multiple Datasets: JointDataset Examples dataset1 = dataset . clone () dataset2 = dataset . clone () joint_dataset = JointDataset ( the_dataset = dataset1 , another_dataset = dataset2 ) joint_dataset JointDataset with 2 sub-datasets: ( the_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) another_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) )","title":"Tutorial for Data Management"},{"location":"data_management/#tutorial-data-management","text":"Author: Tianyu Du (tianyudu@stanford.edu) Note : please go through the introduction tutorial here before proceeding. This notebook aims to help users understand the functionality of ChoiceDataset object. The ChoiceDataset is an instance of the more general PyTorch dataset object holding information of consumer choices. The ChoiceDataset offers easy, clean and efficient data management. The Jupyter-notebook version of this tutorial can be found here . This tutorial provides in-depth explanations on how the torch-choice library manages data. We are also providing an easy-to-use data wrapper converting long-format dataset to ChoiceDataset here , you can harness the torch-choice library without going through this tutorial. Note : since this package was initially proposed for modelling consumer choices, attribute names of ChoiceDataset are borrowed from the consumer choice literature.","title":"Tutorial: Data Management"},{"location":"data_management/#observables","text":"Observables are tensors with specific shapes, we classify observables into four categories based on their variations.","title":"Observables"},{"location":"data_management/#basic-usage","text":"Optionally, the researcher can incorporate observables of, for example, users and items. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. The researcher should supply them with as appropriate keyword arguments while constructing the ChoiceDataset object.","title":"Basic Usage"},{"location":"data_management/#advanced-usage-additional-observables","text":"In some cases, the researcher may wish to handle different parts of user_obs (or other observable tensors) differently. For example, the researcher wishes to model the utility for user \\(u\\) to purchase item \\(i\\) in session \\(s\\) as the following: \\[ U_{usi} = \\beta_{i} X^{(u)}_{user\\ income} + \\gamma X^{(u)}_{user\\ market\\ membership} \\] The coefficient for user income is item-specific so that it captures the nature of the product (i.e., a luxury or an essential good). Additionally, the utility representation admits an user market membership becomes shoppers with active memberships tend to purchase more, and the coefficient of this term is constant across all items. As we will cover later in the modelling section, we need to supply two user observable tensors in this case for the model to build coefficient with different levels of variations (i.e., item-specific coefficients versus constant coefficients). In this case, the researcher needs to supply two tensors user_income and user_market_membership as keyword arguments to the ChoiceDataset constructor. The ChoiceDataset handles multiple user/item/session/price observables internally, for example, every keyword arguments passed into ChoiceDataset with name starting with item_ (except for the reserved item_availability ) will be treated as item observable tensors. All keywords with names starting user_ , session_ and price_ (except for reserved names like user_index and session_index mentioned above) will be interpreted as user/session/price observable tensors. # import required dependencies. import numpy as np import torch from torch_choice.data import ChoiceDataset , JointDataset # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' )","title":"Advanced Usage: Additional Observables"},{"location":"data_management/#creating-choicedataset-object","text":"# Feel free to modify it as you want. num_users = 10 num_items = 4 num_sessions = 500 length_of_dataset = 10000","title":"Creating  ChoiceDataset Object"},{"location":"data_management/#step-1-generate-some-random-purchase-records-and-observables","text":"We will be creating a randomly generated dataset with 10000 purchase records from 10 users, 4 items and 500 sessions. The first step is to randomly generate the purchase records using the following code. For simplicity, we assume all items are available in all sessions. # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. price_obs = torch . randn ( num_sessions , num_items , 12 ) We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen. item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool ()","title":"Step 1: Generate some random purchase records and observables"},{"location":"data_management/#step-2-initialize-the-choicedataset","text":"You can construct a choice set using the following code, which manage all information for you. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs )","title":"Step 2: Initialize the ChoiceDataset."},{"location":"data_management/#what-you-can-do-with-the-choicedataset","text":"","title":"What you can do with the ChoiceDataset?"},{"location":"data_management/#printdataset-and-dataset__str__","text":"The command print(dataset) will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory). print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)","title":"print(dataset) and dataset.__str__"},{"location":"data_management/#datasetnum_users-items-sessions","text":"You can use the num_{users, items, sessions} attribute to obtain the number of users, items, and sessions, they are determined automatically from the {user, item, session}_obs tensors provided while initializing the dataset object. Note : the print =: operator requires Python3.8 or higher, you can remove =: if you are using an earlier copy of Python. print ( f ' { dataset . num_users =:} ' ) print ( f ' { dataset . num_items =:} ' ) print ( f ' { dataset . num_sessions =:} ' ) print ( f ' { len ( dataset ) =:} ' ) dataset.num_users=10 dataset.num_items=4 dataset.num_sessions=500 len(dataset)=10000","title":"dataset.num_{users, items, sessions}"},{"location":"data_management/#datasetclone","text":"The ChoiceDataset offers a clone method allow you to make copy of the dataset, you can modify the cloned dataset arbitrarily without changing the original dataset. # clone print ( dataset . item_index [: 10 ]) dataset_cloned = dataset . clone () dataset_cloned . item_index = 99 * torch . ones ( num_sessions ) print ( dataset_cloned . item_index [: 10 ]) print ( dataset . item_index [: 10 ]) # does not change the original dataset. tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) tensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.]) tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])","title":"dataset.clone()"},{"location":"data_management/#datasettocuda-and-dataset_check_device_consistency","text":"One key advantage of the torch_choice and bemb is their compatibility with GPUs, you can easily move tensors in a ChoiceDataset object between host memory (i.e., cpu memory) and device memory (i.e., GPU memory) using dataset.to() method. Please note that the following code runs only if your machine has a compatible GPU and GPU-compatible version of PyTorch installed. Similarly, one can move data to host-memory using dataset.to('cpu') . The dataset also provides a dataset._check_device_consistency() method to check if all tensors are on the same device. If we only move the label to cpu without moving other tensors, this will result in an error message. # move to device print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset = dataset . to ( 'cuda' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . item_index . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset.device=cpu dataset.device=cpu dataset.user_index.device=cpu dataset.session_index.device=cpu dataset.device=cuda:0 dataset.item_index.device=cuda:0 dataset.user_index.device=cuda:0 dataset.session_index.device=cuda:0 dataset . _check_device_consistency () # # NOTE: this cell will result errors, this is intentional. dataset . item_index = dataset . item_index . to ( 'cpu' ) dataset . _check_device_consistency () --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-56-40d626c6d436> in <module> 1 # # NOTE: this cell will result errors, this is intentional. 2 dataset.item_index = dataset.item_index.to('cpu') ----> 3 dataset._check_device_consistency() ~/Development/torch-choice/torch_choice/data/choice_dataset.py in _check_device_consistency(self) 180 devices.append(val.device) 181 if len(set(devices)) > 1: --> 182 raise Exception(f'Found tensors on different devices: {set(devices)}.', 183 'Use dataset.to() method to align devices.') 184 Exception: (\"Found tensors on different devices: {device(type='cuda', index=0), device(type='cpu')}.\", 'Use dataset.to() method to align devices.') # create dictionary inputs for model.forward() # collapse to a dictionary object. print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12])","title":"dataset.to('cuda') and dataset._check_device_consistency()."},{"location":"data_management/#subset-method","text":"One can use dataset[indices] with indices as an integer-valued tensor or array to get the corresponding rows of the dataset. The example code block below queries the 6256-th, 4119-th, 453-th, 5520-th, and 1877-th row of the dataset object. The item_index , user_index , session_index of the resulted subset will be different from the original dataset, but other tensors will be the same. # __getitem__ to get batch. # pick 5 random sessions as the mini-batch. dataset = dataset . to ( 'cpu' ) indices = torch . Tensor ( np . random . choice ( len ( dataset ), size = 5 , replace = False )) . long () print ( indices ) subset = dataset [ indices ] print ( dataset ) print ( subset ) # print_dict_shape(subset.x_dict) # assert torch.all(dataset.x_dict['price_obs'][indices, :, :] == subset.x_dict['price_obs']) # assert torch.all(dataset.item_index[indices] == subset.item_index) tensor([1118, 976, 1956, 290, 8283]) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) ChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) The subset method internally creates a copy of the datasets so that any modification applied on the subset will not be reflected on the original dataset. The researcher can feel free to do in-place modification to the subset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) subset . item_index += 1 # modifying the batch does not change the original dataset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) tensor([0, 1, 0, 0, 0]) tensor([0, 1, 0, 0, 0]) tensor([1, 2, 1, 1, 1]) tensor([0, 1, 0, 0, 0]) print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) subset . item_obs += 1 print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) tensor(-1.5811) tensor(-1.5811) tensor(-0.5811) tensor(-1.5811) print ( id ( subset . item_index )) print ( id ( dataset . item_index [ indices ])) 140339656298640 140339656150528","title":"Subset method"},{"location":"data_management/#using-pytorch-dataloader-for-the-training-loop","text":"The ChoiceDataset object natively support batch samplers from PyTorch. For demonstration purpose, we turned off the shuffling option. from torch.utils.data.sampler import BatchSampler , SequentialSampler , RandomSampler shuffle = False # for demonstration purpose. batch_size = 32 # Create sampler. sampler = BatchSampler ( RandomSampler ( dataset ) if shuffle else SequentialSampler ( dataset ), batch_size = batch_size , drop_last = False ) dataloader = torch . utils . data . DataLoader ( dataset , sampler = sampler , num_workers = 1 , collate_fn = lambda x : x [ 0 ], pin_memory = ( dataset . device == 'cpu' )) print ( f ' { item_obs . shape =:} ' ) item_obs_all = item_obs . view ( 1 , num_items , - 1 ) . expand ( len ( dataset ), - 1 , - 1 ) item_obs_all = item_obs_all . to ( dataset . device ) item_index_all = item_index . to ( dataset . device ) print ( f ' { item_obs_all . shape =:} ' ) item_obs.shape=torch.Size([4, 64]) item_obs_all.shape=torch.Size([10000, 4, 64]) for i , batch in enumerate ( dataloader ): first , last = i * batch_size , min ( len ( dataset ), ( i + 1 ) * batch_size ) idx = torch . arange ( first , last ) assert torch . all ( item_obs_all [ idx , :, :] == batch . x_dict [ 'item_obs' ]) assert torch . all ( item_index_all [ idx ] == batch . item_index ) batch . x_dict [ 'item_obs' ] . shape torch.Size([16, 4, 64]) print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) dataset . __len__ () 10000","title":"Using Pytorch dataloader for the training loop."},{"location":"data_management/#chaining-multiple-datasets-jointdataset-examples","text":"dataset1 = dataset . clone () dataset2 = dataset . clone () joint_dataset = JointDataset ( the_dataset = dataset1 , another_dataset = dataset2 ) joint_dataset JointDataset with 2 sub-datasets: ( the_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) another_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) )","title":"Chaining Multiple Datasets: JointDataset Examples"},{"location":"easy_data_management/","text":"Easy Data Wrapper Tutorial The data construction covered in the Data Management tutorial might be too complicated for users without prior experience in PyTorch. This tutorial offers a helper class to wrap the dataset, all the user needs to know is (1) loading data-frames to Python, Pandas provides one-line solution to loading various types of data files including CSV, TSV, Stata, and Excel. (2) basic usage of pandas. We aim to make this tutorial as self-contained as possible, so you don't need to be worried if you haven't went through the Data Management tutorial . But we invite you to go through that tutorial to obtain a more in-depth understanding of data management in this project. Author: Tianyu Du Date: May. 20, 2022 Update: Jul. 9, 2022 __author__ = 'Tianyu Du' Let's import a few necessary packages. import pandas as pd import torch from torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper References and Background for Stata Users This tutorial aim to show how to manage choice datasets using the torch-choice package, we will follow the Stata documentation here to offer a seamless experience for the user to transfer prior knowledge in other packages to our package. From Stata Documentation : Choice models (CM) are models for data with outcomes that are choices. The choices are selected by a decision maker, such as a person or a business (i.e., the user ), from a set of possible alternatives (i.e., the items ). For instance, we could model choices made by consumers who select a breakfast cereal from several different brands. Or we could model choices made by businesses who chose whether to buy TV, radio, Internet, or newspaper advertising. Models for choice data come in two varieties\u2014models for discrete choices and models for rank-ordered alternatives. When each individual selects a single alternative, say, he or she purchases one box of cereal, the data are discrete choice data. When each individual ranks the choices, say, he or she orders cereals from most favorite to least favorite, the data are rank-ordered data. Stata has commands for fitting both discrete choice models and rank-ordered models. Our torch-choice package handles the discrete choice models in the Stata document above. Motivations In the following parts, we demonstrate how to convert a long-format data (e.g., the one used in Stata) to the ChoiceDataset data format expected by our package. But first, Why do we want another ChoiceDataset object instead of just one long-format data-frame? In earlier versions of Stata, we can only have one single data-frame loaded in memory, this would introduce memory error especially when teh dataset is large. For example, you have a dataset of a million decisions recorded, each consists of four items, and each item has a persistent built quality that stay the same in all observations. The Stata format would make a million copy of these variables, which is very inefficient. We would need to collect a couple of data-frames as the essential pieces to build our ChoiceDataset . Don't worry, as soon as you have the data-frames ready, the EasyDataWrapper helper class would take care of the rest. We call a single statistical observation a \"purchase record\" and use this terminology throughout the tutorial. df = pd . read_stata ( 'https://www.stata-press.com/data/r17/carchoice.dta' ) We load the artificial dataset from the Stata website. Here we borrow the description of dataset reported from the describe command in Stata. Contains data from https://www.stata-press.com/data/r17/carchoice.dta Observations: 3,160 Car choice data Variables: 6 30 Jul 2020 14:58 --------------------------------------------------------------------------------------------------------------------------------------------------- Variable Storage Display Value name type format label Variable label --------------------------------------------------------------------------------------------------------------------------------------------------- consumerid int %8.0g ID of individual consumer car byte %9.0g nation Nationality of car purchase byte %10.0g Indicator of car purchased gender byte %9.0g gender Gender: 0 = Female, 1 = Male income float %9.0g Income (in $1,000) dealers byte %9.0g No. of dealerships in community --------------------------------------------------------------------------------------------------------------------------------------------------- Sorted by: consumerid car In this dataset, the first four rows with consumerid == 1 corresponds to the first purchasing record , it means the consumer with ID 1 was making the decision among four types of cars (i.e., items ) and chose American car (since the purchase == 1 in that row of American car). Even though there were four types of cars, not all of them were available all the time. For example, for the purchase record by consumer with ID 4, only American, Japanese, and European cars were available (note that there is no row in the dataset with consumerid == 4 and car == 'Korean' , this indicates unavailability of a certain item.) df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 Male 46.699997 9 1 1 Japanese 0 Male 46.699997 11 2 1 European 0 Male 46.699997 5 3 1 Korean 0 Male 46.699997 1 4 2 American 1 Male 26.100000 10 5 2 Japanese 0 Male 26.100000 7 6 2 European 0 Male 26.100000 2 7 2 Korean 0 Male 26.100000 1 8 3 American 0 Male 32.700001 8 9 3 Japanese 1 Male 32.700001 6 10 3 European 0 Male 32.700001 2 11 4 American 1 Female 49.199997 5 12 4 Japanese 0 Female 49.199997 4 13 4 European 0 Female 49.199997 3 14 5 American 0 Male 24.299999 8 15 5 Japanese 0 Male 24.299999 3 16 5 European 1 Male 24.299999 3 17 6 American 1 Female 39.000000 10 18 6 Japanese 0 Female 39.000000 6 19 6 European 0 Female 39.000000 1 20 7 American 0 Male 33.000000 10 21 7 Japanese 0 Male 33.000000 6 22 7 European 1 Male 33.000000 4 23 7 Korean 0 Male 33.000000 1 24 8 American 1 Male 20.299999 6 25 8 Japanese 0 Male 20.299999 5 26 8 European 0 Male 20.299999 3 27 9 American 0 Male 38.000000 9 28 9 Japanese 1 Male 38.000000 9 29 9 European 0 Male 38.000000 2 Components of the Consumer Choice Modelling Problem We begin with essential component of the consumer choice modelling problem. Walking through these components should help you understand what kind of data our models are working on. Purchasing Record Each row (record) of the dataset is called a purchasing record , which includes who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record (i.e., who bought what at where and when ). Items and Categories To begin with, there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) under our consideration. Further, the researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) , it is easy to verify that \\[ \\bigcup_{c \\in \\{1, 2, \\dots, C\\}} I_c = \\{1, 2, \\dots I\\} \\] If the researcher does not wish to model different categories differently, the researcher can simply put all items in one single category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so that all items belong to the same category. Note : since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24 inch LCD monitor). Raw item names can be encoded easily with sklearn.preprocessing.OrdinalEncoder . Users Each purchaing reocrd is naturally associated with an user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) ( who ) as well. Sessions Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, when the data came from a single store over the period of a year. In this case, the notion of where does not matter that much, and session \\(s\\) is simply the date of purchase. Another example is that we have the purchase record from different stores, the session \\(s\\) can be defined as a pair of (date, store) instead. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all rows of the dataset. To summarize, each purchasing record \\(b\\) in the dataset is characterized by a user-session-item tuple \\((u, s, i)\\) . When there are multiple items bought by the same user in the same session, there will be multiple rows in the dataset with the same \\((u, s)\\) corresponding to the same receipt. Format the Dataset a Little Bit The wrapper we built requires several data frames, providing the correct information is all we need to do in this tutorial, the data wrapper will handle the construction of ChoiceDataset for you. Note : The dataset in this tutorial is a bit over-simplified, we only have one purchase record for each user in each session, so the consumerid column identifies all of the user, the session, and the purchase record (because we have different dealers for the same type of car, we define each purchase record of it's session instead of assigning all purchase records to the same session). That is, we have a single user makes a single choice in each single session. The main dataset should contain the following columns: purchase_record_column : a column identifies purchase record (also called case in Stata syntax). this tutorial, the consumerid column is the identifier. For example, the first 4 rows of the dataset (see above) has consumerid == 1 , this means we should look at the first 4 rows together and they constitute the first purchase record. item_name_column : a column identifies names of items , which is car in the dataset above. This column provides information above the availability as well. As mentioned above, there is no column with car == Korean in the fourth purchasing record ( consumerid == 4 ), so we know that Korean car was not available that time. choice_column : a column identifies the choice made by the consumer in each purchase record, which is the purchase column in our example. Exactly one row per purchase record (i.e., rows with the same values in purchase_record_column ) should have 1, while the values are zeros for all other rows. user_index_column : a optional column identifies the user making the choice, which is also consumerid in our case. session_index_column : a optional column identifies the session of the choice, which is also consumerid in our case. As you might have noticed, the consumerid column in the data-frame identifies multiple pieces of information: purchase_record , user_index , and session_index . This is not a mistake, you can use the same column in df to supply multiple pieces of information. df . gender . value_counts ( dropna = False ) Male 2283 Female 854 NaN 23 Name: gender, dtype: int64 The only modification required is to convert gender (with values of Male , Female or NaN ) to integers because PyTorch does not handle strings. For simplicity, we will assume all NaN gender to be Female (you should not do this in a real application!) and re-define the gender variable as \\(\\mathbb{I}\\{\\texttt{gender} == \\texttt{Male}\\}\\) . # we change gender to binary 0/1 because pytorch doesn't handle strings. df [ 'gender' ] = ( df [ 'gender' ] == 'Male' ) . astype ( int ) Now the gender column contains only binary integers. df . gender . value_counts ( dropna = False ) 1 2283 0 877 Name: gender, dtype: int64 The data-frame looks like the following right now: df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 Adding the Observables The next step is to identify observables going into the model. Specifically, we would want to add: 1. gender and income as user-specific observables 2. and dealers as (session, item)-specific observable. Such observables are called price observables in our setting, why? because price is the most typical (session, item)-specific observable. Method 1: Adding Observables by Extracting Columns of the Dataset As you can see, gender , income and dealers are already encompassed in df , the first way to add observables is simply mentioning these columns while initializing the EasyDatasetWrapper object. You can supply a list of names of columns to each of {user, item, session, price}_observable_columns keyword argument. For example, we use user_observable_columns=['gender', 'income'] to inform the EasyDatasetWrapper that we wish to derive user-specific observables from the gender and income columns of df . Also, we inform the EasyDatasetWrapper that we want to derive (session, item)-specific (i.e., price observable) by specifying price_observable_columns=['dealers'] . Since our package leverages GPU-acceleration, it is necessary to supply the device on which the dataset should reside. The EasyDatasetWrapper also takes a device keyword, which can be either 'cpu' or an appropriate CUDA device. if torch . cuda . is_available (): device = 'cuda' # use GPU if available else : device = 'cpu' # use CPU otherwise data_1 = EasyDatasetWrapper ( main_data = df , # TODO: better naming convention? Need to discuss. # after discussion, we add it to the default value # in the data wrapper class. # these are just names. purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # it can be derived from columns of the dataframe or supplied as user_observable_columns = [ 'gender' , 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. The dataset has a summary() method, which can be used to print out the summary of the dataset. data_1 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) You can access the ChoiceDataset object constructed by calling the data.choice_dataset object. data_1 . choice_dataset ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Method 2: Adding Observables as Data Frames We can also construct data frames and use data frames to supply different observables. This is useful when you have a large dataset, for example, if there are many purchase records for the same user (to be concrete, say \\(U\\) users and \\(N\\) purchase records for each user, resulting \\(U \\times N\\) total purchase records). Using a single data-frame requires a lot of memory: you need to store \\(U \\times N\\) entires of user genders in total. However, user genders should be persistent across all purchasing records, if we use a separate data-frame mapping user index to gender of the user, we only need to store \\(U\\) entries (i.e., one for each user) of gender information. Similarly, the long-format data requires storing each piece of item-specific information for number of purchase records times, which leads to inefficient usage of disk/memory space. How Do Observable Data-frame Look Like? Our package natively support the following four types of observables: User Observables : user-specific observables (e.g., gender and income) should (1) have length equal to the number of unique users in the dataset (885 here); (2) contains a column named as user_index_column ( user_index_column is a variable, the actual column name should be the value of variable user_index_column ! E.g., here the user observable data-frame should have a column named 'consumerid' ); (3) the user observable can have any number of other columns beside the user_index_column column, each of them corresponding to a user-specific observable. For example, a data-frame containing \\(X\\) user-specific observables has shape (num_users, X + 1) . Item Observables item-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique items in the dataset (4 here); (2) contain a column named as item_index_column ( item_index_column is a variable, the actual column name should be the value of variable item_index_column ! E.g., here the item observable data-frame should have a column named 'car' ); (3) the item observable can have any number of other columns beside the item_index_column column, each of them corresponding to a item-specific observable. Session Observable session-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique sessions in the dataset; (2) contain a column named as session_index_column ( session_index_column is a variable, the actual column name should be the value of variable session_index_column ! E.g., here the session observable data-frame should have a column named 'consumerid' ); (3) the session observable can have any number of other columns beside the session_index_column column, each of them corresponding to a session-specific observable. Price Observables (session, item)-specific observables (e.g., dealers) should be (1) contains a column named as session_index_column (e.g., consumerid in our example) and a column named as item_name_column (e.g., car in our example), (2) the price observable can have any number of other columns beside the session_index_column and item_name_column columns, each of them corresponding to a (session, item)-specific observable. For example, a data-frame containing \\(X\\) (session, item)-specific observables has shape (num_sessions, num_items, X + 2) . We encourage the reader to review the Data Management Tutorial for more details on types of observables. Suggested Procedure of Storing and Loading Data Suppose SESSION_INDEX column in df_main is the index of the session, ALTERNATIVES column is the index of the car. For user-specific observables, you should have a CSV on disk with columns { consumerid , var_1 , var_2 , ...}. You load the user-specific dataset as user_obs = pd.read_csv(..., index='consumerid') . Let's first construct the data frame for user genders first. gender = df . groupby ( 'consumerid' )[ 'gender' ] . first () . reset_index () The user-observable data-frame contains a column of user IDs (the consumerid column), this column should have exactly the same name as the column containing user indices. Otherwise, the wrapper won't know which column corresponds to user IDs and which column corresponds to variables. gender . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender 0 1 1 1 2 1 2 3 1 3 4 0 4 5 1 Then, let's build the data-frame for user-specific income variables. income = df . groupby ( 'consumerid' )[ 'income' ] . first () . reset_index () income . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid income 0 1 46.699997 1 2 26.100000 2 3 32.700001 3 4 49.199997 4 5 24.299999 Please note that we can have multiple observables contained in the same data-frame as well. gender_and_income = df . groupby ( 'consumerid' )[[ 'gender' , 'income' ]] . first () . reset_index () gender_and_income .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender income 0 1 1 46.699997 1 2 1 26.100000 2 3 1 32.700001 3 4 0 49.199997 4 5 1 24.299999 ... ... ... ... 880 881 1 45.700001 881 882 1 69.800003 882 883 0 45.599998 883 884 1 20.900000 884 885 1 30.600000 885 rows \u00d7 3 columns The price observable data-frame contains two columns identifying session (i.e., the consumerid column) and item (i.e., the car column). The session index column should have exactly the same name as the session index column in df and the column indexing columns should have exactly the same name as the item-name-column in df . dealers = df [[ 'consumerid' , 'car' , 'dealers' ]] dealers . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car dealers 0 1 American 9 1 1 Japanese 11 2 1 European 5 3 1 Korean 1 4 2 American 10 Build Datasets using EasyDatasetWrapper with Observables as Data-Frames We can observables as data-frames using {user, item, session, price}_observable_data keyword arguments. data_2 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender , 'income' : income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. # Use summary to see what's inside the data wrapper. data_2 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Alternatively, we can supply user income and gender as a single dataframe, instead of user_gender and user_income tensors, now the constructed ChoiceDataset contains a single user_gender_and_income tensor with shape (885, 2) encompassing both income and gender of users. data_3 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender_and_income' : gender_and_income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_3 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender_and_income=[885, 2], price_dealers=[885, 4, 1], device=cuda:0) Method 3: Mixing Method 1 and Method 2 The EasyDataWrapper also support supplying observables as a mixture of above methods. The following example supplies gender user observable as a data-frame but income and dealers as column names. data_4 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender }, user_observable_columns = [ 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_4 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Sanity Checks Lastly, let's check choice datasets constructed via different methods are actually the same. The == method of choice datasets will compare the non-NAN entries of all tensors in datasets. print ( data_1 . choice_dataset == data_2 . choice_dataset ) print ( data_1 . choice_dataset == data_4 . choice_dataset ) True True For data_3 , we have income and gender combined: data_3 . choice_dataset . user_gender_and_income == torch . cat ([ data_1 . choice_dataset . user_gender , data_1 . choice_dataset . user_income ], dim = 1 ) tensor([[True, True], [True, True], [True, True], ..., [True, True], [True, True], [True, True]], device='cuda:0') Now let's compare what's inside the data structure and our raw data. bought_raw = df [ df [ 'purchase' ] == 1 ][ 'car' ] . values bought_data = list () encoder = { 0 : 'American' , 1 : 'European' , 2 : 'Japanese' , 3 : 'Korean' } for b in data_1 . choice_dataset . item_index : bought_data . append ( encoder [ float ( b )]) all ( bought_raw == bought_data ) True Then, let's compare the income and gender variable contained in the dataset. X = df . groupby ( 'consumerid' )[ 'income' ] . first () . values Y = data_1 . choice_dataset . user_income . cpu () . numpy () . squeeze () all ( X == Y ) True X = df . groupby ( 'consumerid' )[ 'gender' ] . first () . values Y = data_1 . choice_dataset . user_gender . cpu () . numpy () . squeeze () all ( X == Y ) True Lastly, let's compare the price_dealer variable. Since there are NAN-values in it for unavailable cars, we can't not use all(X == Y) to compare them. We will first fill NANs values with -1 and then compare resulted data-frames. # rearrange columns to align it with the internal encoding scheme of the data wrapper. X = df . pivot ( 'consumerid' , 'car' , 'dealers' )[[ 'American' , 'European' , 'Japanese' , 'Korean' ]] X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } car American European Japanese Korean consumerid 1 9.0 5.0 11.0 1.0 2 10.0 2.0 7.0 1.0 3 8.0 2.0 6.0 NaN 4 5.0 3.0 4.0 NaN 5 8.0 3.0 3.0 NaN ... ... ... ... ... 881 8.0 2.0 10.0 NaN 882 8.0 6.0 8.0 1.0 883 9.0 5.0 8.0 1.0 884 12.0 4.0 10.0 NaN 885 10.0 4.0 5.0 NaN 885 rows \u00d7 4 columns Y = data_1 . choice_dataset . price_dealers . squeeze ( dim =- 1 ) Y tensor([[ 9., 5., 11., 1.], [10., 2., 7., 1.], [ 8., 2., 6., nan], ..., [ 9., 5., 8., 1.], [12., 4., 10., nan], [10., 4., 5., nan]], device='cuda:0') print ( X . fillna ( - 1 ) . values == torch . nan_to_num ( Y , - 1 ) . cpu () . numpy ()) [[ True True True True] [ True True True True] [ True True True True] ... [ True True True True] [ True True True True] [ True True True True]] This concludes our tutorial on building the dataset, if you wish more in-depth understanding of the data structure, please refer to the Data Management Tutorial .","title":"Tutorial for Easy Data Management and Stata Users"},{"location":"easy_data_management/#easy-data-wrapper-tutorial","text":"The data construction covered in the Data Management tutorial might be too complicated for users without prior experience in PyTorch. This tutorial offers a helper class to wrap the dataset, all the user needs to know is (1) loading data-frames to Python, Pandas provides one-line solution to loading various types of data files including CSV, TSV, Stata, and Excel. (2) basic usage of pandas. We aim to make this tutorial as self-contained as possible, so you don't need to be worried if you haven't went through the Data Management tutorial . But we invite you to go through that tutorial to obtain a more in-depth understanding of data management in this project. Author: Tianyu Du Date: May. 20, 2022 Update: Jul. 9, 2022 __author__ = 'Tianyu Du' Let's import a few necessary packages. import pandas as pd import torch from torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper","title":"Easy Data Wrapper Tutorial"},{"location":"easy_data_management/#references-and-background-for-stata-users","text":"This tutorial aim to show how to manage choice datasets using the torch-choice package, we will follow the Stata documentation here to offer a seamless experience for the user to transfer prior knowledge in other packages to our package. From Stata Documentation : Choice models (CM) are models for data with outcomes that are choices. The choices are selected by a decision maker, such as a person or a business (i.e., the user ), from a set of possible alternatives (i.e., the items ). For instance, we could model choices made by consumers who select a breakfast cereal from several different brands. Or we could model choices made by businesses who chose whether to buy TV, radio, Internet, or newspaper advertising. Models for choice data come in two varieties\u2014models for discrete choices and models for rank-ordered alternatives. When each individual selects a single alternative, say, he or she purchases one box of cereal, the data are discrete choice data. When each individual ranks the choices, say, he or she orders cereals from most favorite to least favorite, the data are rank-ordered data. Stata has commands for fitting both discrete choice models and rank-ordered models. Our torch-choice package handles the discrete choice models in the Stata document above.","title":"References and Background for Stata Users"},{"location":"easy_data_management/#motivations","text":"In the following parts, we demonstrate how to convert a long-format data (e.g., the one used in Stata) to the ChoiceDataset data format expected by our package. But first, Why do we want another ChoiceDataset object instead of just one long-format data-frame? In earlier versions of Stata, we can only have one single data-frame loaded in memory, this would introduce memory error especially when teh dataset is large. For example, you have a dataset of a million decisions recorded, each consists of four items, and each item has a persistent built quality that stay the same in all observations. The Stata format would make a million copy of these variables, which is very inefficient. We would need to collect a couple of data-frames as the essential pieces to build our ChoiceDataset . Don't worry, as soon as you have the data-frames ready, the EasyDataWrapper helper class would take care of the rest. We call a single statistical observation a \"purchase record\" and use this terminology throughout the tutorial. df = pd . read_stata ( 'https://www.stata-press.com/data/r17/carchoice.dta' ) We load the artificial dataset from the Stata website. Here we borrow the description of dataset reported from the describe command in Stata. Contains data from https://www.stata-press.com/data/r17/carchoice.dta Observations: 3,160 Car choice data Variables: 6 30 Jul 2020 14:58 --------------------------------------------------------------------------------------------------------------------------------------------------- Variable Storage Display Value name type format label Variable label --------------------------------------------------------------------------------------------------------------------------------------------------- consumerid int %8.0g ID of individual consumer car byte %9.0g nation Nationality of car purchase byte %10.0g Indicator of car purchased gender byte %9.0g gender Gender: 0 = Female, 1 = Male income float %9.0g Income (in $1,000) dealers byte %9.0g No. of dealerships in community --------------------------------------------------------------------------------------------------------------------------------------------------- Sorted by: consumerid car In this dataset, the first four rows with consumerid == 1 corresponds to the first purchasing record , it means the consumer with ID 1 was making the decision among four types of cars (i.e., items ) and chose American car (since the purchase == 1 in that row of American car). Even though there were four types of cars, not all of them were available all the time. For example, for the purchase record by consumer with ID 4, only American, Japanese, and European cars were available (note that there is no row in the dataset with consumerid == 4 and car == 'Korean' , this indicates unavailability of a certain item.) df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 Male 46.699997 9 1 1 Japanese 0 Male 46.699997 11 2 1 European 0 Male 46.699997 5 3 1 Korean 0 Male 46.699997 1 4 2 American 1 Male 26.100000 10 5 2 Japanese 0 Male 26.100000 7 6 2 European 0 Male 26.100000 2 7 2 Korean 0 Male 26.100000 1 8 3 American 0 Male 32.700001 8 9 3 Japanese 1 Male 32.700001 6 10 3 European 0 Male 32.700001 2 11 4 American 1 Female 49.199997 5 12 4 Japanese 0 Female 49.199997 4 13 4 European 0 Female 49.199997 3 14 5 American 0 Male 24.299999 8 15 5 Japanese 0 Male 24.299999 3 16 5 European 1 Male 24.299999 3 17 6 American 1 Female 39.000000 10 18 6 Japanese 0 Female 39.000000 6 19 6 European 0 Female 39.000000 1 20 7 American 0 Male 33.000000 10 21 7 Japanese 0 Male 33.000000 6 22 7 European 1 Male 33.000000 4 23 7 Korean 0 Male 33.000000 1 24 8 American 1 Male 20.299999 6 25 8 Japanese 0 Male 20.299999 5 26 8 European 0 Male 20.299999 3 27 9 American 0 Male 38.000000 9 28 9 Japanese 1 Male 38.000000 9 29 9 European 0 Male 38.000000 2","title":"Motivations"},{"location":"easy_data_management/#components-of-the-consumer-choice-modelling-problem","text":"We begin with essential component of the consumer choice modelling problem. Walking through these components should help you understand what kind of data our models are working on.","title":"Components of the Consumer Choice Modelling Problem"},{"location":"easy_data_management/#purchasing-record","text":"Each row (record) of the dataset is called a purchasing record , which includes who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record (i.e., who bought what at where and when ).","title":"Purchasing Record"},{"location":"easy_data_management/#items-and-categories","text":"To begin with, there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) under our consideration. Further, the researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) , it is easy to verify that \\[ \\bigcup_{c \\in \\{1, 2, \\dots, C\\}} I_c = \\{1, 2, \\dots I\\} \\] If the researcher does not wish to model different categories differently, the researcher can simply put all items in one single category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so that all items belong to the same category. Note : since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24 inch LCD monitor). Raw item names can be encoded easily with sklearn.preprocessing.OrdinalEncoder .","title":"Items and Categories"},{"location":"easy_data_management/#users","text":"Each purchaing reocrd is naturally associated with an user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) ( who ) as well.","title":"Users"},{"location":"easy_data_management/#sessions","text":"Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, when the data came from a single store over the period of a year. In this case, the notion of where does not matter that much, and session \\(s\\) is simply the date of purchase. Another example is that we have the purchase record from different stores, the session \\(s\\) can be defined as a pair of (date, store) instead. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all rows of the dataset. To summarize, each purchasing record \\(b\\) in the dataset is characterized by a user-session-item tuple \\((u, s, i)\\) . When there are multiple items bought by the same user in the same session, there will be multiple rows in the dataset with the same \\((u, s)\\) corresponding to the same receipt.","title":"Sessions"},{"location":"easy_data_management/#format-the-dataset-a-little-bit","text":"The wrapper we built requires several data frames, providing the correct information is all we need to do in this tutorial, the data wrapper will handle the construction of ChoiceDataset for you. Note : The dataset in this tutorial is a bit over-simplified, we only have one purchase record for each user in each session, so the consumerid column identifies all of the user, the session, and the purchase record (because we have different dealers for the same type of car, we define each purchase record of it's session instead of assigning all purchase records to the same session). That is, we have a single user makes a single choice in each single session. The main dataset should contain the following columns: purchase_record_column : a column identifies purchase record (also called case in Stata syntax). this tutorial, the consumerid column is the identifier. For example, the first 4 rows of the dataset (see above) has consumerid == 1 , this means we should look at the first 4 rows together and they constitute the first purchase record. item_name_column : a column identifies names of items , which is car in the dataset above. This column provides information above the availability as well. As mentioned above, there is no column with car == Korean in the fourth purchasing record ( consumerid == 4 ), so we know that Korean car was not available that time. choice_column : a column identifies the choice made by the consumer in each purchase record, which is the purchase column in our example. Exactly one row per purchase record (i.e., rows with the same values in purchase_record_column ) should have 1, while the values are zeros for all other rows. user_index_column : a optional column identifies the user making the choice, which is also consumerid in our case. session_index_column : a optional column identifies the session of the choice, which is also consumerid in our case. As you might have noticed, the consumerid column in the data-frame identifies multiple pieces of information: purchase_record , user_index , and session_index . This is not a mistake, you can use the same column in df to supply multiple pieces of information. df . gender . value_counts ( dropna = False ) Male 2283 Female 854 NaN 23 Name: gender, dtype: int64 The only modification required is to convert gender (with values of Male , Female or NaN ) to integers because PyTorch does not handle strings. For simplicity, we will assume all NaN gender to be Female (you should not do this in a real application!) and re-define the gender variable as \\(\\mathbb{I}\\{\\texttt{gender} == \\texttt{Male}\\}\\) . # we change gender to binary 0/1 because pytorch doesn't handle strings. df [ 'gender' ] = ( df [ 'gender' ] == 'Male' ) . astype ( int ) Now the gender column contains only binary integers. df . gender . value_counts ( dropna = False ) 1 2283 0 877 Name: gender, dtype: int64 The data-frame looks like the following right now: df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10","title":"Format the Dataset a Little Bit"},{"location":"easy_data_management/#adding-the-observables","text":"The next step is to identify observables going into the model. Specifically, we would want to add: 1. gender and income as user-specific observables 2. and dealers as (session, item)-specific observable. Such observables are called price observables in our setting, why? because price is the most typical (session, item)-specific observable.","title":"Adding the Observables"},{"location":"easy_data_management/#method-1-adding-observables-by-extracting-columns-of-the-dataset","text":"As you can see, gender , income and dealers are already encompassed in df , the first way to add observables is simply mentioning these columns while initializing the EasyDatasetWrapper object. You can supply a list of names of columns to each of {user, item, session, price}_observable_columns keyword argument. For example, we use user_observable_columns=['gender', 'income'] to inform the EasyDatasetWrapper that we wish to derive user-specific observables from the gender and income columns of df . Also, we inform the EasyDatasetWrapper that we want to derive (session, item)-specific (i.e., price observable) by specifying price_observable_columns=['dealers'] . Since our package leverages GPU-acceleration, it is necessary to supply the device on which the dataset should reside. The EasyDatasetWrapper also takes a device keyword, which can be either 'cpu' or an appropriate CUDA device. if torch . cuda . is_available (): device = 'cuda' # use GPU if available else : device = 'cpu' # use CPU otherwise data_1 = EasyDatasetWrapper ( main_data = df , # TODO: better naming convention? Need to discuss. # after discussion, we add it to the default value # in the data wrapper class. # these are just names. purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # it can be derived from columns of the dataframe or supplied as user_observable_columns = [ 'gender' , 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. The dataset has a summary() method, which can be used to print out the summary of the dataset. data_1 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) You can access the ChoiceDataset object constructed by calling the data.choice_dataset object. data_1 . choice_dataset ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0)","title":"Method 1: Adding Observables by Extracting Columns of the Dataset"},{"location":"easy_data_management/#method-2-adding-observables-as-data-frames","text":"We can also construct data frames and use data frames to supply different observables. This is useful when you have a large dataset, for example, if there are many purchase records for the same user (to be concrete, say \\(U\\) users and \\(N\\) purchase records for each user, resulting \\(U \\times N\\) total purchase records). Using a single data-frame requires a lot of memory: you need to store \\(U \\times N\\) entires of user genders in total. However, user genders should be persistent across all purchasing records, if we use a separate data-frame mapping user index to gender of the user, we only need to store \\(U\\) entries (i.e., one for each user) of gender information. Similarly, the long-format data requires storing each piece of item-specific information for number of purchase records times, which leads to inefficient usage of disk/memory space.","title":"Method 2: Adding Observables as Data Frames"},{"location":"easy_data_management/#how-do-observable-data-frame-look-like","text":"Our package natively support the following four types of observables: User Observables : user-specific observables (e.g., gender and income) should (1) have length equal to the number of unique users in the dataset (885 here); (2) contains a column named as user_index_column ( user_index_column is a variable, the actual column name should be the value of variable user_index_column ! E.g., here the user observable data-frame should have a column named 'consumerid' ); (3) the user observable can have any number of other columns beside the user_index_column column, each of them corresponding to a user-specific observable. For example, a data-frame containing \\(X\\) user-specific observables has shape (num_users, X + 1) . Item Observables item-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique items in the dataset (4 here); (2) contain a column named as item_index_column ( item_index_column is a variable, the actual column name should be the value of variable item_index_column ! E.g., here the item observable data-frame should have a column named 'car' ); (3) the item observable can have any number of other columns beside the item_index_column column, each of them corresponding to a item-specific observable. Session Observable session-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique sessions in the dataset; (2) contain a column named as session_index_column ( session_index_column is a variable, the actual column name should be the value of variable session_index_column ! E.g., here the session observable data-frame should have a column named 'consumerid' ); (3) the session observable can have any number of other columns beside the session_index_column column, each of them corresponding to a session-specific observable. Price Observables (session, item)-specific observables (e.g., dealers) should be (1) contains a column named as session_index_column (e.g., consumerid in our example) and a column named as item_name_column (e.g., car in our example), (2) the price observable can have any number of other columns beside the session_index_column and item_name_column columns, each of them corresponding to a (session, item)-specific observable. For example, a data-frame containing \\(X\\) (session, item)-specific observables has shape (num_sessions, num_items, X + 2) . We encourage the reader to review the Data Management Tutorial for more details on types of observables.","title":"How Do Observable Data-frame Look Like?"},{"location":"easy_data_management/#suggested-procedure-of-storing-and-loading-data","text":"Suppose SESSION_INDEX column in df_main is the index of the session, ALTERNATIVES column is the index of the car. For user-specific observables, you should have a CSV on disk with columns { consumerid , var_1 , var_2 , ...}. You load the user-specific dataset as user_obs = pd.read_csv(..., index='consumerid') . Let's first construct the data frame for user genders first. gender = df . groupby ( 'consumerid' )[ 'gender' ] . first () . reset_index () The user-observable data-frame contains a column of user IDs (the consumerid column), this column should have exactly the same name as the column containing user indices. Otherwise, the wrapper won't know which column corresponds to user IDs and which column corresponds to variables. gender . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender 0 1 1 1 2 1 2 3 1 3 4 0 4 5 1 Then, let's build the data-frame for user-specific income variables. income = df . groupby ( 'consumerid' )[ 'income' ] . first () . reset_index () income . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid income 0 1 46.699997 1 2 26.100000 2 3 32.700001 3 4 49.199997 4 5 24.299999 Please note that we can have multiple observables contained in the same data-frame as well. gender_and_income = df . groupby ( 'consumerid' )[[ 'gender' , 'income' ]] . first () . reset_index () gender_and_income .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender income 0 1 1 46.699997 1 2 1 26.100000 2 3 1 32.700001 3 4 0 49.199997 4 5 1 24.299999 ... ... ... ... 880 881 1 45.700001 881 882 1 69.800003 882 883 0 45.599998 883 884 1 20.900000 884 885 1 30.600000 885 rows \u00d7 3 columns The price observable data-frame contains two columns identifying session (i.e., the consumerid column) and item (i.e., the car column). The session index column should have exactly the same name as the session index column in df and the column indexing columns should have exactly the same name as the item-name-column in df . dealers = df [[ 'consumerid' , 'car' , 'dealers' ]] dealers . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car dealers 0 1 American 9 1 1 Japanese 11 2 1 European 5 3 1 Korean 1 4 2 American 10","title":"Suggested Procedure of Storing and Loading Data"},{"location":"easy_data_management/#build-datasets-using-easydatasetwrapper-with-observables-as-data-frames","text":"We can observables as data-frames using {user, item, session, price}_observable_data keyword arguments. data_2 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender , 'income' : income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. # Use summary to see what's inside the data wrapper. data_2 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Alternatively, we can supply user income and gender as a single dataframe, instead of user_gender and user_income tensors, now the constructed ChoiceDataset contains a single user_gender_and_income tensor with shape (885, 2) encompassing both income and gender of users. data_3 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender_and_income' : gender_and_income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_3 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender_and_income=[885, 2], price_dealers=[885, 4, 1], device=cuda:0)","title":"Build Datasets using EasyDatasetWrapper with Observables as Data-Frames"},{"location":"easy_data_management/#method-3-mixing-method-1-and-method-2","text":"The EasyDataWrapper also support supplying observables as a mixture of above methods. The following example supplies gender user observable as a data-frame but income and dealers as column names. data_4 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender }, user_observable_columns = [ 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_4 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0)","title":"Method 3: Mixing Method 1 and Method 2"},{"location":"easy_data_management/#sanity-checks","text":"Lastly, let's check choice datasets constructed via different methods are actually the same. The == method of choice datasets will compare the non-NAN entries of all tensors in datasets. print ( data_1 . choice_dataset == data_2 . choice_dataset ) print ( data_1 . choice_dataset == data_4 . choice_dataset ) True True For data_3 , we have income and gender combined: data_3 . choice_dataset . user_gender_and_income == torch . cat ([ data_1 . choice_dataset . user_gender , data_1 . choice_dataset . user_income ], dim = 1 ) tensor([[True, True], [True, True], [True, True], ..., [True, True], [True, True], [True, True]], device='cuda:0') Now let's compare what's inside the data structure and our raw data. bought_raw = df [ df [ 'purchase' ] == 1 ][ 'car' ] . values bought_data = list () encoder = { 0 : 'American' , 1 : 'European' , 2 : 'Japanese' , 3 : 'Korean' } for b in data_1 . choice_dataset . item_index : bought_data . append ( encoder [ float ( b )]) all ( bought_raw == bought_data ) True Then, let's compare the income and gender variable contained in the dataset. X = df . groupby ( 'consumerid' )[ 'income' ] . first () . values Y = data_1 . choice_dataset . user_income . cpu () . numpy () . squeeze () all ( X == Y ) True X = df . groupby ( 'consumerid' )[ 'gender' ] . first () . values Y = data_1 . choice_dataset . user_gender . cpu () . numpy () . squeeze () all ( X == Y ) True Lastly, let's compare the price_dealer variable. Since there are NAN-values in it for unavailable cars, we can't not use all(X == Y) to compare them. We will first fill NANs values with -1 and then compare resulted data-frames. # rearrange columns to align it with the internal encoding scheme of the data wrapper. X = df . pivot ( 'consumerid' , 'car' , 'dealers' )[[ 'American' , 'European' , 'Japanese' , 'Korean' ]] X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } car American European Japanese Korean consumerid 1 9.0 5.0 11.0 1.0 2 10.0 2.0 7.0 1.0 3 8.0 2.0 6.0 NaN 4 5.0 3.0 4.0 NaN 5 8.0 3.0 3.0 NaN ... ... ... ... ... 881 8.0 2.0 10.0 NaN 882 8.0 6.0 8.0 1.0 883 9.0 5.0 8.0 1.0 884 12.0 4.0 10.0 NaN 885 10.0 4.0 5.0 NaN 885 rows \u00d7 4 columns Y = data_1 . choice_dataset . price_dealers . squeeze ( dim =- 1 ) Y tensor([[ 9., 5., 11., 1.], [10., 2., 7., 1.], [ 8., 2., 6., nan], ..., [ 9., 5., 8., 1.], [12., 4., 10., nan], [10., 4., 5., nan]], device='cuda:0') print ( X . fillna ( - 1 ) . values == torch . nan_to_num ( Y , - 1 ) . cpu () . numpy ()) [[ True True True True] [ True True True True] [ True True True True] ... [ True True True True] [ True True True True] [ True True True True]] This concludes our tutorial on building the dataset, if you wish more in-depth understanding of the data structure, please refer to the Data Management Tutorial .","title":"Sanity Checks"},{"location":"install/","text":"Installation This page will guide you through the installation procedure of torch-choice and bemb . There are two parts of this project: the torch_choice library consisting of data management modules, logit and nested-logit models for consumer choice modelling. For researchers wish to use the Bayesian Embedding (BEMB) model, they need to install an additional bemb package, which was built on the top of torch_choice . Note Since this project is still on its pre-release stage and subject to changes, we have not uploaded our packages to PIP or CONDA. Researchers need to install these packages from Github source code. Option 1: Install using Source Code from Github Repository To install torch_choice and bemb from source, 1. Clone the repositories of both torch_choice and bemb to your local machine. 2. Install required dependencies (e.g., PyTorch and PyTorch-Lightning). 3. For each of repositories, run python3 ./setup.py develop to add the package to your Python environment. 4. Check installation by running python3 -c \"import torch_choice; print(torch_choice.__version__)\" . 5. Check installation by running python3 -c \"import bemb; print(bemb.__version__)\" . Option 2: Install using Pip The torch-choice is available on PIP now here ! You can use pip install torch-choice to install it. Note : We are working on publishing BEMB to PIP.","title":"Installation"},{"location":"install/#installation","text":"This page will guide you through the installation procedure of torch-choice and bemb . There are two parts of this project: the torch_choice library consisting of data management modules, logit and nested-logit models for consumer choice modelling. For researchers wish to use the Bayesian Embedding (BEMB) model, they need to install an additional bemb package, which was built on the top of torch_choice . Note Since this project is still on its pre-release stage and subject to changes, we have not uploaded our packages to PIP or CONDA. Researchers need to install these packages from Github source code.","title":"Installation"},{"location":"install/#option-1-install-using-source-code-from-github-repository","text":"To install torch_choice and bemb from source, 1. Clone the repositories of both torch_choice and bemb to your local machine. 2. Install required dependencies (e.g., PyTorch and PyTorch-Lightning). 3. For each of repositories, run python3 ./setup.py develop to add the package to your Python environment. 4. Check installation by running python3 -c \"import torch_choice; print(torch_choice.__version__)\" . 5. Check installation by running python3 -c \"import bemb; print(bemb.__version__)\" .","title":"Option 1: Install using Source Code from Github Repository"},{"location":"install/#option-2-install-using-pip","text":"The torch-choice is available on PIP now here ! You can use pip install torch-choice to install it. Note : We are working on publishing BEMB to PIP.","title":"Option 2: Install using Pip"},{"location":"intro/","text":"Introduction This document briefly introduces the choice model we aim to solve. In short, all models (the conditional logit model, the nested logit model, and the Bayesian embedding model) in the package aim to predict which item a user will purchase while facing the shelves in a supermarket. Specifically, for each user \\(u\\) and item \\(i\\) , models compute a value \\(U_{ui}\\) predicting the utility user \\(u\\) will get from purchasing item \\(i\\) . User \\(u\\) is predicted to purchase the item \\(i\\) , generating the maximum utility. However, the usage of our models is not limited to this supermarket context; researchers can adjust the definition of user and item to fit any choice modeling context. The related project page overviews some extensions of our models to other contexts. Notes on Encodings Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). Components of the Choice Modeling Problem We aim to predict users' choices while facing multiple items available, e.g., which brand of milk the user will purchase in the supermarket. We begin with essential components of the choice modeling problem. Walking through these components should help you understand what kind of data our models are working on. Purchasing Record A purchasing record is a record describing who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows/observation of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record. What : Items and Categories To begin with, suppose there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) . The researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) . It's easy to see that the union of all \\(I_c\\) is the entire set of items \\(\\{1, 2, \\dots I\\}\\) . Suppose the researcher does not wish to model different categories differently. In that case, the researcher can put all items in one category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so all items belong to the same category. For each purchasing record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(i_b \\in \\{1,2,\\dots,I\\}\\) saying which item was chosen in this record. Important Note on Encoding Since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). Here is an example of encoding generic item names to integers using sklearn.preprocessing.LabelEncoder : from sklearn.preprocessing import LabelEncoder enc = LabelEncoder () raw_items = [ 'Macbook Laptop' , 'Dell 24-inch Monitor' , 'Orange' , 'Apple (Fruit)' ] encoded_items = enc . fit_transform ( raw_items ) print ( encoded_items ) # output: [2 1 3 0] # for each 0 <= i <= 3, enc.classes_[i] reveals the raw name of item encoded to i. print ( enc . classes_ ) # output: ['Apple (Fruit)' 'Dell 24-inch Monitor' 'Macbook Laptop' 'Orange'] # For example, the first entry of enc.classes_ is 'Apple (Fruit)', this means 'Apple (Fruit)' was encoded to 0 in this process. # The last item in the `raw_item` list was 'Apple (Fruit)', and the last item in the `encoded_item` list was 0 as we expected. Who : Users The counter-part of items in our setting are user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) as well. For each purchasing record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(u_b \\in \\{1,2,\\dots,I\\}\\) describing which user was making the decision. When and Where : Sessions Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, we had the purchase record from five different stores for every day in 2021, then a session \\(s\\) is defined as a pair of (date, storeID) , and there are \\(5 \\times 365\\) sessions in total. In another example, suppose the data came from a single store for over a year. In this case, the notion of where is immaterial, and session \\(s\\) is simply the date of purchase. The notion of sessions can be more flexible than just date and location. For example, if we want to distinguish between online ordering and in-store purchasing, we can define the session as (date, storeID, IsOnlineOrdering). The session variable serves as a tool for the researcher to split the dataset; the usefulness of the session will be more evident after introducing observables (features) later. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all dataset rows. Putting Everything Together To summarize, each purchasing record \\(b \\in \\{1, 2, \\dots, B\\}\\) in the dataset is characterized by a user-session-item tuple \\((u_b, s_b, i_b)\\) . The totality of \\(B\\) purchasing records consists of the dataset we are modeling. When the same user buys multiple items in the same session, the dataset will have multiple purchasing records with the same \\((u, s)\\) corresponding to the same receipt. Item Availability It is not necessarily that all items are available in every session; items can get out of stock in particular sessions. To handle these cases, the researcher can optionally provide a boolean tensor \\(A \\in \\{\\texttt{True}, \\texttt{False}\\}^{S\\times I}\\) to indicate which items are available for purchasing in each session. \\(A_{s, i} = \\texttt{True}\\) if and only if item \\(i\\) was available in session \\(s\\) . While predicting the purchase probabilities, the model sets the probability for these unavailable items to zero and normalizes probabilities among available items. If the item availability is not provided, the model assumes all items are available in all sessions. Observables Next, let's talk about observables (yes, it's the same as feature in machine learning literature, it's the \\(X\\) variable). The researcher can incorporate observables of, for example, users and items into the model. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. Please note that we consider these four types as definitions of observable types. For example, whenever a variable is user-specific, then we call it an user_obs . This package defines observables in the above way so that the package can easily track the variation of variables and handle these observable tensors correctly. Note on the Price Observable The price_obs term might look confusing at the first glance. As mentioned above, price-observables are defined to be these observables depending on both session and item. If an observable depends on both session and item, it is called a price observable no matter if it is related to the actual price or not. For example, in the context of online shopping, the shipping cost depends on both the item (i.e., the item purchased) and the session (i.e., when and where you purchase). In this case, the shipping cost observable is a price-observable but it's not an actual price. Conversely, the actual price of an item might not change across sessions. For example, a 10-dollar Amazon gift card costs 10 dollars regardless of the session; in this case the price variable is in fact an item observable as it only depends on the item. A Toy Example Suppose we have a dataset of purchase history from two stores (Store A and B) on two dates (Sep 16 and 17), both stores sell {apple, banana, orange} ( num_items=3 ) and there are three people came to those stores between Sep 16 and 17. user_index session_index item_index Amy Sep-17-2021-Store-A banana Ben Sep-17-2021-Store-B apple Ben Sep-16-2021-Store-A orange Charlie Sep-16-2021-Store-B apple Charlie Sep-16-2021-Store-B orange NOTE : For demonstration purposes, the example dataset has user_index , session_index and item_index as strings, they should be consecutive integers in actual production. One can easily convert them to integers using sklearn.preprocessing.LabelEncoder . In the example above, - user_index=[0,1,1,2,2] (with encoding 0=Amy, 1=Ben, 2=Charlie ), - session_index=[0,1,2,3,3] (with encoding 0=Sep-17-2021-Store-A, 1=Sep-17-2021-Store-B, 2=Sep-16-2021-Store-A, 3=Sep-16-2021-Store-B ), - item_index=[0,1,2,1,2] (with encoding 0=banana, 1=apple, 2=orange ). Suppose we believe people's purchasing decision depends on the nutrition levels of these fruits; suppose apple has the highest nutrition level and banana has the lowest one, we can add item_obs=[[1.5], [12.0], [3.3]] \\(\\in \\mathbb{R}^{3\\times 1}\\) . The shape of this tensor is number-of-items by number-of-observable. NOTE : If someone went to one store and bought multiple items (e.g., Charlie bought both apple and orange at Store B on Sep-16), we include them as separate rows in the dataset and model them independently. Models The torch-choice library provides two models, the conditional logit model and the nested logit model, for modeling the dataset. Each model takes in \\((u_b, s_b)\\) altogether with observables and outputs a probability of purchasing each \\(\\tilde{i} \\in \\{1, 2, \\dots, I\\}\\) , denoted as \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) . In cases when not all items are available, the model sets the probability of unavailable items to zero and normalizes probabilities among available items. \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) is the predicted probability of purchasing item \\(\\tilde{i}\\) in session \\(s_b\\) by user \\(u_b\\) given all information we know. Model parameters are trained using gradient descent algorithm and the loss function is the negative log-likelihood of the model \\(-\\sum_{b=1}^B \\log(\\hat{p}_{u_b, s_b, i_b})\\) . The major difference among models lies in the way they compute predicted probabilities.","title":"Introduction"},{"location":"intro/#introduction","text":"This document briefly introduces the choice model we aim to solve. In short, all models (the conditional logit model, the nested logit model, and the Bayesian embedding model) in the package aim to predict which item a user will purchase while facing the shelves in a supermarket. Specifically, for each user \\(u\\) and item \\(i\\) , models compute a value \\(U_{ui}\\) predicting the utility user \\(u\\) will get from purchasing item \\(i\\) . User \\(u\\) is predicted to purchase the item \\(i\\) , generating the maximum utility. However, the usage of our models is not limited to this supermarket context; researchers can adjust the definition of user and item to fit any choice modeling context. The related project page overviews some extensions of our models to other contexts.","title":"Introduction"},{"location":"intro/#notes-on-encodings","text":"Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well).","title":"Notes on Encodings"},{"location":"intro/#components-of-the-choice-modeling-problem","text":"We aim to predict users' choices while facing multiple items available, e.g., which brand of milk the user will purchase in the supermarket. We begin with essential components of the choice modeling problem. Walking through these components should help you understand what kind of data our models are working on.","title":"Components of the Choice Modeling Problem"},{"location":"intro/#purchasing-record","text":"A purchasing record is a record describing who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows/observation of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record.","title":"Purchasing Record"},{"location":"intro/#what-items-and-categories","text":"To begin with, suppose there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) . The researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) . It's easy to see that the union of all \\(I_c\\) is the entire set of items \\(\\{1, 2, \\dots I\\}\\) . Suppose the researcher does not wish to model different categories differently. In that case, the researcher can put all items in one category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so all items belong to the same category. For each purchasing record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(i_b \\in \\{1,2,\\dots,I\\}\\) saying which item was chosen in this record.","title":"What: Items and Categories"},{"location":"intro/#important-note-on-encoding","text":"Since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). Here is an example of encoding generic item names to integers using sklearn.preprocessing.LabelEncoder : from sklearn.preprocessing import LabelEncoder enc = LabelEncoder () raw_items = [ 'Macbook Laptop' , 'Dell 24-inch Monitor' , 'Orange' , 'Apple (Fruit)' ] encoded_items = enc . fit_transform ( raw_items ) print ( encoded_items ) # output: [2 1 3 0] # for each 0 <= i <= 3, enc.classes_[i] reveals the raw name of item encoded to i. print ( enc . classes_ ) # output: ['Apple (Fruit)' 'Dell 24-inch Monitor' 'Macbook Laptop' 'Orange'] # For example, the first entry of enc.classes_ is 'Apple (Fruit)', this means 'Apple (Fruit)' was encoded to 0 in this process. # The last item in the `raw_item` list was 'Apple (Fruit)', and the last item in the `encoded_item` list was 0 as we expected.","title":"Important Note on Encoding"},{"location":"intro/#who-users","text":"The counter-part of items in our setting are user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) as well. For each purchasing record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(u_b \\in \\{1,2,\\dots,I\\}\\) describing which user was making the decision.","title":"Who: Users"},{"location":"intro/#when-and-where-sessions","text":"Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, we had the purchase record from five different stores for every day in 2021, then a session \\(s\\) is defined as a pair of (date, storeID) , and there are \\(5 \\times 365\\) sessions in total. In another example, suppose the data came from a single store for over a year. In this case, the notion of where is immaterial, and session \\(s\\) is simply the date of purchase. The notion of sessions can be more flexible than just date and location. For example, if we want to distinguish between online ordering and in-store purchasing, we can define the session as (date, storeID, IsOnlineOrdering). The session variable serves as a tool for the researcher to split the dataset; the usefulness of the session will be more evident after introducing observables (features) later. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all dataset rows.","title":"When and Where: Sessions"},{"location":"intro/#putting-everything-together","text":"To summarize, each purchasing record \\(b \\in \\{1, 2, \\dots, B\\}\\) in the dataset is characterized by a user-session-item tuple \\((u_b, s_b, i_b)\\) . The totality of \\(B\\) purchasing records consists of the dataset we are modeling. When the same user buys multiple items in the same session, the dataset will have multiple purchasing records with the same \\((u, s)\\) corresponding to the same receipt.","title":"Putting Everything Together"},{"location":"intro/#item-availability","text":"It is not necessarily that all items are available in every session; items can get out of stock in particular sessions. To handle these cases, the researcher can optionally provide a boolean tensor \\(A \\in \\{\\texttt{True}, \\texttt{False}\\}^{S\\times I}\\) to indicate which items are available for purchasing in each session. \\(A_{s, i} = \\texttt{True}\\) if and only if item \\(i\\) was available in session \\(s\\) . While predicting the purchase probabilities, the model sets the probability for these unavailable items to zero and normalizes probabilities among available items. If the item availability is not provided, the model assumes all items are available in all sessions.","title":"Item Availability"},{"location":"intro/#observables","text":"Next, let's talk about observables (yes, it's the same as feature in machine learning literature, it's the \\(X\\) variable). The researcher can incorporate observables of, for example, users and items into the model. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. Please note that we consider these four types as definitions of observable types. For example, whenever a variable is user-specific, then we call it an user_obs . This package defines observables in the above way so that the package can easily track the variation of variables and handle these observable tensors correctly.","title":"Observables"},{"location":"intro/#note-on-the-price-observable","text":"The price_obs term might look confusing at the first glance. As mentioned above, price-observables are defined to be these observables depending on both session and item. If an observable depends on both session and item, it is called a price observable no matter if it is related to the actual price or not. For example, in the context of online shopping, the shipping cost depends on both the item (i.e., the item purchased) and the session (i.e., when and where you purchase). In this case, the shipping cost observable is a price-observable but it's not an actual price. Conversely, the actual price of an item might not change across sessions. For example, a 10-dollar Amazon gift card costs 10 dollars regardless of the session; in this case the price variable is in fact an item observable as it only depends on the item.","title":"Note on the Price Observable"},{"location":"intro/#a-toy-example","text":"Suppose we have a dataset of purchase history from two stores (Store A and B) on two dates (Sep 16 and 17), both stores sell {apple, banana, orange} ( num_items=3 ) and there are three people came to those stores between Sep 16 and 17. user_index session_index item_index Amy Sep-17-2021-Store-A banana Ben Sep-17-2021-Store-B apple Ben Sep-16-2021-Store-A orange Charlie Sep-16-2021-Store-B apple Charlie Sep-16-2021-Store-B orange NOTE : For demonstration purposes, the example dataset has user_index , session_index and item_index as strings, they should be consecutive integers in actual production. One can easily convert them to integers using sklearn.preprocessing.LabelEncoder . In the example above, - user_index=[0,1,1,2,2] (with encoding 0=Amy, 1=Ben, 2=Charlie ), - session_index=[0,1,2,3,3] (with encoding 0=Sep-17-2021-Store-A, 1=Sep-17-2021-Store-B, 2=Sep-16-2021-Store-A, 3=Sep-16-2021-Store-B ), - item_index=[0,1,2,1,2] (with encoding 0=banana, 1=apple, 2=orange ). Suppose we believe people's purchasing decision depends on the nutrition levels of these fruits; suppose apple has the highest nutrition level and banana has the lowest one, we can add item_obs=[[1.5], [12.0], [3.3]] \\(\\in \\mathbb{R}^{3\\times 1}\\) . The shape of this tensor is number-of-items by number-of-observable. NOTE : If someone went to one store and bought multiple items (e.g., Charlie bought both apple and orange at Store B on Sep-16), we include them as separate rows in the dataset and model them independently.","title":"A Toy Example"},{"location":"intro/#models","text":"The torch-choice library provides two models, the conditional logit model and the nested logit model, for modeling the dataset. Each model takes in \\((u_b, s_b)\\) altogether with observables and outputs a probability of purchasing each \\(\\tilde{i} \\in \\{1, 2, \\dots, I\\}\\) , denoted as \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) . In cases when not all items are available, the model sets the probability of unavailable items to zero and normalizes probabilities among available items. \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) is the predicted probability of purchasing item \\(\\tilde{i}\\) in session \\(s_b\\) by user \\(u_b\\) given all information we know. Model parameters are trained using gradient descent algorithm and the loss function is the negative log-likelihood of the model \\(-\\sum_{b=1}^B \\log(\\hat{p}_{u_b, s_b, i_b})\\) . The major difference among models lies in the way they compute predicted probabilities.","title":"Models"},{"location":"nested_logit_model_house_cooling/","text":"Random Utility Model (RUM) Part II: Nested Logit Model Author: Tianyu Du The package implements the nested logit model as well, which allows researchers to model choices as a two-stage process: the user first picks a category of purchase and then picks the item from the chosen category that generates the most utility. Examples here are modified from Exercise 2: Nested logit model by Kenneth Train and Yves Croissant . The House Cooling (HC) dataset from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California. The dataset is small and serve as a demonstration of the nested logit model. The alternatives are: Gas central heat with cooling gcc , Electric central resistence heat with cooling ecc , Electric room resistence heat with cooling erc , Electric heat pump, which provides cooling also hpc , Gas central heat without cooling gc , Electric central resistence heat without cooling ec , Electric room resistence heat without cooling er . Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative. The variables are: depvar gives the name of the chosen alternative, ich.alt are the installation cost for the heating portion of the system, icca is the installation cost for cooling och.alt are the operating cost for the heating portion of the system occa is the operating cost for cooling income is the annual income of the household Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling. Nested Logit Model: Background The following code block provides an example initialization of the NestedLogitModel (please refer to examples below for details). model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) The nested logit model decompose the utility of choosing item \\(i\\) into the (1) item-specific values and (2) category specify values. For simplicity, suppose item \\(i\\) belongs to category \\(k \\in \\{1, \\dots, K\\}\\) : \\(i \\in B_k\\) . \\[ U_{uit} = W_{ukt} + Y_{uit} \\] Where both \\(W\\) and \\(Y\\) are estimated using linear models from as in the conditional logit model. The log-likelihood for user \\(u\\) to choose item \\(i\\) at time/session \\(t\\) decomposes into the item-level likelihood and category-level likelihood. \\[ \\log P(i \\mid u, t) = \\log P(i \\mid u, t, B_k) + \\log P(k \\mid u, t) \\\\ = \\log \\left(\\frac{\\exp(Y_{uit}/\\lambda_k)}{\\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)}\\right) + \\log \\left( \\frac{\\exp(W_{ukt} + \\lambda_k I_{ukt})}{\\sum_{\\ell=1}^K \\exp(W_{u\\ell t} + \\lambda_\\ell I_{u\\ell t})}\\right) \\] The inclusive value of category \\(k\\) , \\(I_{ukt}\\) is defined as \\(\\log \\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)\\) , which is the expected utility from choosing the best alternative from category \\(k\\) . The category_to_item keyword defines a dictionary of the mapping \\(k \\mapsto B_k\\) , where keys of category_to_item are integer \\(k\\) 's and category_to_item[k] is a list consisting of IDs of items in \\(B_k\\) . The {category, item}_coef_variation_dict provides specification to \\(W_{ukt}\\) and \\(Y_{uit}\\) respectively, torch_choice allows for empty category level models by providing an empty dictionary (in this case, \\(W_{ukt} = \\epsilon_{ukt}\\) ) since the inclusive value term \\(\\lambda_k I_{ukt}\\) will be used to model the choice over categories. However, by specifying an empty second stage model ( \\(Y_{uit} = \\epsilon_{uit}\\) ), the nested logit model reduces to a conditional logit model of choices over categories. Hence, one should never use the NestedLogitModel class with an empty item-level model. Similar to the conditional logit model, {category, item}_num_param_dict specify the dimension (number of observables to be multiplied with the coefficient) of coefficients. The above code initializes a simple model built upon item-time-specific observables \\(X_{it} \\in \\mathbb{R}^7\\) , \\[ Y_{uit} = \\beta^\\top X_{it} + \\epsilon_{uit} \\\\ W_{ukt} = \\epsilon_{ukt} \\] The research may wish to enfoce the elasiticity \\(\\lambda_k\\) to be constant across categories, setting shared_lambda=True enforces \\(\\lambda_k = \\lambda\\ \\forall k \\in [K]\\) . Load Essential Packages We firstly read essential packages for this tutorial. import argparse import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model.nested_logit_model import NestedLogitModel from torch_choice.utils.run_helper import run We then select the appropriate device to run the model on, our package supports both CPU and GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) DEVICE = 'cuda' else : print ( 'Running tutorial on CPU' ) DEVICE = 'cpu' CUDA device used: NVIDIA GeForce RTX 3090 Load Datasets We firstly read the dataset for this tutorial, the csv file can be found at ./public_datasets/HC.csv . df = pd . read_csv ( './public_datasets/HC.csv' , index_col = 0 ) df = df . reset_index ( drop = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } depvar icca occa income ich och idx.id1 idx.id2 inc.room inc.cooling int.cooling cooling.modes room.modes 0 False 0.00 0.00 20 24.50 4.09 1 ec 0 0 0 False False 1 False 27.28 2.95 20 7.86 4.09 1 ecc 0 20 1 True False 2 False 0.00 0.00 20 7.37 3.85 1 er 20 0 0 False True 3 True 27.28 2.95 20 8.79 3.85 1 erc 20 20 1 True True 4 False 0.00 0.00 20 24.08 2.26 1 gc 0 0 0 False False The raw dataset is in a long-format (i.e., each row contains information of one item). df [ 'idx.id2' ] . value_counts () ec 250 ecc 250 er 250 erc 250 gc 250 gcc 250 hpc 250 Name: idx.id2, dtype: int64 # what was actually chosen. item_index = df [ df [ 'depvar' ] == True ] . sort_values ( by = 'idx.id1' )[ 'idx.id2' ] . reset_index ( drop = True ) item_names = [ 'ec' , 'ecc' , 'er' , 'erc' , 'gc' , 'gcc' , 'hpc' ] num_items = df [ 'idx.id2' ] . nunique () # cardinal encoder. encoder = dict ( zip ( item_names , range ( num_items ))) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) Because we will be training our model with PyTorch , we need to encode item names to integers (from 0 to 6). We do this manually in this exercise given the small amount of items, for more items, one can use sklearn.preprocessing.OrdinalEncoder to encode. Raw item names will be encoded as the following. encoder {'ec': 0, 'ecc': 1, 'er': 2, 'erc': 3, 'gc': 4, 'gcc': 5, 'hpc': 6} Category Level Dataset We firstly construct the category-level dataset, however, there is no observable that is constant within the same category, so we don't need to include any observable tensor to the category_dataset . All we need to do is adding the item_index (i.e., which item is chosen) to the dataset, so that category_dataset knows the total number of choices made. # category feature: no category feature, all features are item-level. category_dataset = ChoiceDataset ( item_index = item_index . clone ()) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Item Level Dataset For simplicity, we treat each purchasing record as its own session. Moreover, we treat all observables as price observables (i.e., varying by both session and item). Since there are 7 observables in total, the resulted price_obs has shape (250, 7, 7) corresponding to number_of_sessions by number_of_items by number_of_observables . # item feature. item_feat_cols = [ 'ich' , 'och' , 'icca' , 'occa' , 'inc.room' , 'inc.cooling' , 'int.cooling' ] price_obs = utils . pivot3d ( df , dim0 = 'idx.id1' , dim1 = 'idx.id2' , values = item_feat_cols ) price_obs . shape torch.Size([250, 7, 7]) Then, we construct the item level dataset by providing both item_index and price_obs . We move item_dataset to the appropriate device as well. This is only necessary if we are using GPU to accelerate the model. item_dataset = ChoiceDataset ( item_index = item_index , price_obs = price_obs ) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Finally, we chain the category-level and item-level dataset into a single JointDataset . dataset = JointDataset ( category = category_dataset , item = item_dataset ) One can print the joint dataset to see its contents, and tensors contained in each of these sub-datasets. print ( dataset ) JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) Examples There are multiple ways to group 7 items into categories, different classification will result in different utility functions and estimations (see the background of nested logit models). We will demonstrate the usage of our package by presenting three different categorization schemes and corresponding model estimations. Example 1 In the first example, the model is specified to have the cooling alternatives {gcc, ecc, erc, hpc} in one category and the non-cooling alternatives {gc, ec, er} in another category. We create a category_to_item dictionary to inform the model our categorization scheme. The dictionary should have keys ranging from 0 to number_of_categories - 1 , each integer corresponds to a category. The value of each key is a list of item IDs in the category, the encoding of item names should be exactly the same as in the construction of item_index . category_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' , 'hpc' ], 1 : [ 'gc' , 'ec' , 'er' ]} # encode items to integers. for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) In this example, we have item [1, 3, 5, 6] in the first category (category 0 ) and the rest of items in the second category (category 1 ). print ( category_to_item ) {0: [1, 3, 5, 6], 1: [0, 2, 4]} Next, let's create the NestedLogitModel class! The first thing to put in is the category_to_item dictionary we just built. For category_coef_variation_dict , category_num_param_dict , since we don't have any category-specific observables, we can simply put an empty dictionary there. Coefficients for all observables are constant across items, and there are 7 observables in total. As for shared_lambda=True , please refer to the background recap for nested logit model. model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) You can print the model to get summary information of the NestedLogitModel class. print ( model ) NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) NOTE : We are computing the standard errors using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with respect to model parameters. This leads to slight different results compared with R implementation. run ( model , dataset , num_epochs = 10000 ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 1000: Log-likelihood=-187.43597412109375 Epoch 2000: Log-likelihood=-179.69964599609375 Epoch 3000: Log-likelihood=-178.70831298828125 Epoch 4000: Log-likelihood=-178.28799438476562 Epoch 5000: Log-likelihood=-178.17779541015625 Epoch 6000: Log-likelihood=-178.13650512695312 Epoch 7000: Log-likelihood=-178.12576293945312 Epoch 8000: Log-likelihood=-178.14144897460938 Epoch 9000: Log-likelihood=-178.12478637695312 Epoch 10000: Log-likelihood=-178.13674926757812 ==================== model results ==================== Training Epochs: 10000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -178.13674926757812 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 0.585981 | 0.167168 | | item_price_obs_0 | -0.555577 | 0.145414 | | item_price_obs_1 | -0.85812 | 0.238405 | | item_price_obs_2 | -0.224599 | 0.111092 | | item_price_obs_3 | -1.08912 | 1.04131 | | item_price_obs_4 | -0.379067 | 0.101126 | | item_price_obs_5 | 0.250203 | 0.0522721 | | item_price_obs_6 | -5.99917 | 4.85404 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) R Output Here we provide the output from mlogit model in R for estimation reference. Coefficient names reported are slightly different in Python and R , please use the following table for comparison. Please note that the lambda_weight_0 in Python (at the top) corresponds to the iv (inclusive value) in R (at the bottom). Orderings of coefficients for observables should be the same in both languages. Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", ## \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 11 iterations, 0h:0m:0s ## g'(-H)^-1g = 7.26E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.554878 0.144205 -3.8478 0.0001192 *** ## och -0.857886 0.255313 -3.3601 0.0007791 *** ## icca -0.225079 0.144423 -1.5585 0.1191212 ## occa -1.089458 1.219821 -0.8931 0.3717882 ## inc.room -0.378971 0.099631 -3.8038 0.0001425 *** ## inc.cooling 0.249575 0.059213 4.2149 2.499e-05 *** ## int.cooling -6.000415 5.562423 -1.0787 0.2807030 ## iv 0.585922 0.179708 3.2604 0.0011125 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -178.12 Example 2 The second example is similar to the first one, but we change the way we group items into different categories. Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.) category_to_item = { 0 : [ 'ec' , 'ecc' , 'gc' , 'gcc' , 'hpc' ], 1 : [ 'er' , 'erc' ]} for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset , num_epochs = 5000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 500: Log-likelihood=-193.73406982421875 Epoch 1000: Log-likelihood=-185.25933837890625 Epoch 1500: Log-likelihood=-183.55142211914062 Epoch 2000: Log-likelihood=-181.8164825439453 Epoch 2500: Log-likelihood=-180.4320526123047 Epoch 3000: Log-likelihood=-180.04095458984375 Epoch 3500: Log-likelihood=-180.7447509765625 Epoch 4000: Log-likelihood=-180.39688110351562 Epoch 4500: Log-likelihood=-180.27947998046875 Epoch 5000: Log-likelihood=-181.1483612060547 ==================== model results ==================== Training Epochs: 5000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -181.1483612060547 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 1.61072 | 0.787735 | | item_price_obs_0 | -1.34719 | 0.631206 | | item_price_obs_1 | -2.16109 | 1.0451 | | item_price_obs_2 | -0.393868 | 0.255138 | | item_price_obs_3 | -2.53253 | 2.2719 | | item_price_obs_4 | -0.884873 | 0.379626 | | item_price_obs_5 | 0.496491 | 0.248118 | | item_price_obs_6 | -15.6477 | 9.88054 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) R Output You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", ## \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 10 iterations, 0h:0m:0s ## g'(-H)^-1g = 5.87E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -1.13818 0.54216 -2.0993 0.03579 * ## och -1.82532 0.93228 -1.9579 0.05024 . ## icca -0.33746 0.26934 -1.2529 0.21024 ## occa -2.06328 1.89726 -1.0875 0.27681 ## inc.room -0.75722 0.34292 -2.2081 0.02723 * ## inc.cooling 0.41689 0.20742 2.0099 0.04444 * ## int.cooling -13.82487 7.94031 -1.7411 0.08167 . ## iv 1.36201 0.65393 2.0828 0.03727 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.02 Example 3 For the third example, we now group items into three categories. Specifically, we have items gcc , ecc and erc in the first category (category 0 in the category_to_item dictionary), hpc in a category (category 1 ) alone, and items gc , ec and er in the last category (category 2 ). category_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' ], 1 : [ 'hpc' ], 2 : [ 'gc' , 'ec' , 'er' ]} for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 500: Log-likelihood=-187.12100219726562 Epoch 1000: Log-likelihood=-182.98468017578125 Epoch 1500: Log-likelihood=-181.72171020507812 Epoch 2000: Log-likelihood=-181.3906707763672 Epoch 2500: Log-likelihood=-181.2037353515625 Epoch 3000: Log-likelihood=-181.0186767578125 Epoch 3500: Log-likelihood=-180.83331298828125 Epoch 4000: Log-likelihood=-180.6610107421875 Epoch 4500: Log-likelihood=-180.51480102539062 Epoch 5000: Log-likelihood=-180.40383911132812 ==================== model results ==================== Training Epochs: 5000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -180.40383911132812 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 0.939528 | 0.193704 | | item_price_obs_0 | -0.823672 | 0.0973065 | | item_price_obs_1 | -1.31387 | 0.182701 | | item_price_obs_2 | -0.305365 | 0.12726 | | item_price_obs_3 | -1.89104 | 1.14781 | | item_price_obs_4 | -0.559503 | 0.0734163 | | item_price_obs_5 | 0.310081 | 0.0551569 | | item_price_obs_6 | -7.68508 | 5.09592 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) R Output You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", ## \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), ## un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 8 iterations, 0h:0m:0s ## g'(-H)^-1g = 3.71E-08 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.838394 0.100546 -8.3384 < 2.2e-16 *** ## och -1.331598 0.252069 -5.2827 1.273e-07 *** ## icca -0.256131 0.145564 -1.7596 0.07848 . ## occa -1.405656 1.207281 -1.1643 0.24430 ## inc.room -0.571352 0.077950 -7.3297 2.307e-13 *** ## inc.cooling 0.311355 0.056357 5.5247 3.301e-08 *** ## int.cooling -10.413384 5.612445 -1.8554 0.06354 . ## iv 0.956544 0.180722 5.2929 1.204e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.26","title":"Tutorial for Nested Logit Model"},{"location":"nested_logit_model_house_cooling/#random-utility-model-rum-part-ii-nested-logit-model","text":"Author: Tianyu Du The package implements the nested logit model as well, which allows researchers to model choices as a two-stage process: the user first picks a category of purchase and then picks the item from the chosen category that generates the most utility. Examples here are modified from Exercise 2: Nested logit model by Kenneth Train and Yves Croissant . The House Cooling (HC) dataset from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California. The dataset is small and serve as a demonstration of the nested logit model. The alternatives are: Gas central heat with cooling gcc , Electric central resistence heat with cooling ecc , Electric room resistence heat with cooling erc , Electric heat pump, which provides cooling also hpc , Gas central heat without cooling gc , Electric central resistence heat without cooling ec , Electric room resistence heat without cooling er . Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative. The variables are: depvar gives the name of the chosen alternative, ich.alt are the installation cost for the heating portion of the system, icca is the installation cost for cooling och.alt are the operating cost for the heating portion of the system occa is the operating cost for cooling income is the annual income of the household Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling.","title":"Random Utility Model (RUM) Part II: Nested Logit Model"},{"location":"nested_logit_model_house_cooling/#nested-logit-model-background","text":"The following code block provides an example initialization of the NestedLogitModel (please refer to examples below for details). model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) The nested logit model decompose the utility of choosing item \\(i\\) into the (1) item-specific values and (2) category specify values. For simplicity, suppose item \\(i\\) belongs to category \\(k \\in \\{1, \\dots, K\\}\\) : \\(i \\in B_k\\) . \\[ U_{uit} = W_{ukt} + Y_{uit} \\] Where both \\(W\\) and \\(Y\\) are estimated using linear models from as in the conditional logit model. The log-likelihood for user \\(u\\) to choose item \\(i\\) at time/session \\(t\\) decomposes into the item-level likelihood and category-level likelihood. \\[ \\log P(i \\mid u, t) = \\log P(i \\mid u, t, B_k) + \\log P(k \\mid u, t) \\\\ = \\log \\left(\\frac{\\exp(Y_{uit}/\\lambda_k)}{\\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)}\\right) + \\log \\left( \\frac{\\exp(W_{ukt} + \\lambda_k I_{ukt})}{\\sum_{\\ell=1}^K \\exp(W_{u\\ell t} + \\lambda_\\ell I_{u\\ell t})}\\right) \\] The inclusive value of category \\(k\\) , \\(I_{ukt}\\) is defined as \\(\\log \\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)\\) , which is the expected utility from choosing the best alternative from category \\(k\\) . The category_to_item keyword defines a dictionary of the mapping \\(k \\mapsto B_k\\) , where keys of category_to_item are integer \\(k\\) 's and category_to_item[k] is a list consisting of IDs of items in \\(B_k\\) . The {category, item}_coef_variation_dict provides specification to \\(W_{ukt}\\) and \\(Y_{uit}\\) respectively, torch_choice allows for empty category level models by providing an empty dictionary (in this case, \\(W_{ukt} = \\epsilon_{ukt}\\) ) since the inclusive value term \\(\\lambda_k I_{ukt}\\) will be used to model the choice over categories. However, by specifying an empty second stage model ( \\(Y_{uit} = \\epsilon_{uit}\\) ), the nested logit model reduces to a conditional logit model of choices over categories. Hence, one should never use the NestedLogitModel class with an empty item-level model. Similar to the conditional logit model, {category, item}_num_param_dict specify the dimension (number of observables to be multiplied with the coefficient) of coefficients. The above code initializes a simple model built upon item-time-specific observables \\(X_{it} \\in \\mathbb{R}^7\\) , \\[ Y_{uit} = \\beta^\\top X_{it} + \\epsilon_{uit} \\\\ W_{ukt} = \\epsilon_{ukt} \\] The research may wish to enfoce the elasiticity \\(\\lambda_k\\) to be constant across categories, setting shared_lambda=True enforces \\(\\lambda_k = \\lambda\\ \\forall k \\in [K]\\) .","title":"Nested Logit Model: Background"},{"location":"nested_logit_model_house_cooling/#load-essential-packages","text":"We firstly read essential packages for this tutorial. import argparse import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model.nested_logit_model import NestedLogitModel from torch_choice.utils.run_helper import run We then select the appropriate device to run the model on, our package supports both CPU and GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) DEVICE = 'cuda' else : print ( 'Running tutorial on CPU' ) DEVICE = 'cpu' CUDA device used: NVIDIA GeForce RTX 3090","title":"Load Essential Packages"},{"location":"nested_logit_model_house_cooling/#load-datasets","text":"We firstly read the dataset for this tutorial, the csv file can be found at ./public_datasets/HC.csv . df = pd . read_csv ( './public_datasets/HC.csv' , index_col = 0 ) df = df . reset_index ( drop = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } depvar icca occa income ich och idx.id1 idx.id2 inc.room inc.cooling int.cooling cooling.modes room.modes 0 False 0.00 0.00 20 24.50 4.09 1 ec 0 0 0 False False 1 False 27.28 2.95 20 7.86 4.09 1 ecc 0 20 1 True False 2 False 0.00 0.00 20 7.37 3.85 1 er 20 0 0 False True 3 True 27.28 2.95 20 8.79 3.85 1 erc 20 20 1 True True 4 False 0.00 0.00 20 24.08 2.26 1 gc 0 0 0 False False The raw dataset is in a long-format (i.e., each row contains information of one item). df [ 'idx.id2' ] . value_counts () ec 250 ecc 250 er 250 erc 250 gc 250 gcc 250 hpc 250 Name: idx.id2, dtype: int64 # what was actually chosen. item_index = df [ df [ 'depvar' ] == True ] . sort_values ( by = 'idx.id1' )[ 'idx.id2' ] . reset_index ( drop = True ) item_names = [ 'ec' , 'ecc' , 'er' , 'erc' , 'gc' , 'gcc' , 'hpc' ] num_items = df [ 'idx.id2' ] . nunique () # cardinal encoder. encoder = dict ( zip ( item_names , range ( num_items ))) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) Because we will be training our model with PyTorch , we need to encode item names to integers (from 0 to 6). We do this manually in this exercise given the small amount of items, for more items, one can use sklearn.preprocessing.OrdinalEncoder to encode. Raw item names will be encoded as the following. encoder {'ec': 0, 'ecc': 1, 'er': 2, 'erc': 3, 'gc': 4, 'gcc': 5, 'hpc': 6}","title":"Load Datasets"},{"location":"nested_logit_model_house_cooling/#category-level-dataset","text":"We firstly construct the category-level dataset, however, there is no observable that is constant within the same category, so we don't need to include any observable tensor to the category_dataset . All we need to do is adding the item_index (i.e., which item is chosen) to the dataset, so that category_dataset knows the total number of choices made. # category feature: no category feature, all features are item-level. category_dataset = ChoiceDataset ( item_index = item_index . clone ()) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session.","title":"Category Level Dataset"},{"location":"nested_logit_model_house_cooling/#item-level-dataset","text":"For simplicity, we treat each purchasing record as its own session. Moreover, we treat all observables as price observables (i.e., varying by both session and item). Since there are 7 observables in total, the resulted price_obs has shape (250, 7, 7) corresponding to number_of_sessions by number_of_items by number_of_observables . # item feature. item_feat_cols = [ 'ich' , 'och' , 'icca' , 'occa' , 'inc.room' , 'inc.cooling' , 'int.cooling' ] price_obs = utils . pivot3d ( df , dim0 = 'idx.id1' , dim1 = 'idx.id2' , values = item_feat_cols ) price_obs . shape torch.Size([250, 7, 7]) Then, we construct the item level dataset by providing both item_index and price_obs . We move item_dataset to the appropriate device as well. This is only necessary if we are using GPU to accelerate the model. item_dataset = ChoiceDataset ( item_index = item_index , price_obs = price_obs ) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Finally, we chain the category-level and item-level dataset into a single JointDataset . dataset = JointDataset ( category = category_dataset , item = item_dataset ) One can print the joint dataset to see its contents, and tensors contained in each of these sub-datasets. print ( dataset ) JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) )","title":"Item Level Dataset"},{"location":"nested_logit_model_house_cooling/#examples","text":"There are multiple ways to group 7 items into categories, different classification will result in different utility functions and estimations (see the background of nested logit models). We will demonstrate the usage of our package by presenting three different categorization schemes and corresponding model estimations.","title":"Examples"},{"location":"nested_logit_model_house_cooling/#example-1","text":"In the first example, the model is specified to have the cooling alternatives {gcc, ecc, erc, hpc} in one category and the non-cooling alternatives {gc, ec, er} in another category. We create a category_to_item dictionary to inform the model our categorization scheme. The dictionary should have keys ranging from 0 to number_of_categories - 1 , each integer corresponds to a category. The value of each key is a list of item IDs in the category, the encoding of item names should be exactly the same as in the construction of item_index . category_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' , 'hpc' ], 1 : [ 'gc' , 'ec' , 'er' ]} # encode items to integers. for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) In this example, we have item [1, 3, 5, 6] in the first category (category 0 ) and the rest of items in the second category (category 1 ). print ( category_to_item ) {0: [1, 3, 5, 6], 1: [0, 2, 4]} Next, let's create the NestedLogitModel class! The first thing to put in is the category_to_item dictionary we just built. For category_coef_variation_dict , category_num_param_dict , since we don't have any category-specific observables, we can simply put an empty dictionary there. Coefficients for all observables are constant across items, and there are 7 observables in total. As for shared_lambda=True , please refer to the background recap for nested logit model. model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) You can print the model to get summary information of the NestedLogitModel class. print ( model ) NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) NOTE : We are computing the standard errors using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with respect to model parameters. This leads to slight different results compared with R implementation. run ( model , dataset , num_epochs = 10000 ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 1000: Log-likelihood=-187.43597412109375 Epoch 2000: Log-likelihood=-179.69964599609375 Epoch 3000: Log-likelihood=-178.70831298828125 Epoch 4000: Log-likelihood=-178.28799438476562 Epoch 5000: Log-likelihood=-178.17779541015625 Epoch 6000: Log-likelihood=-178.13650512695312 Epoch 7000: Log-likelihood=-178.12576293945312 Epoch 8000: Log-likelihood=-178.14144897460938 Epoch 9000: Log-likelihood=-178.12478637695312 Epoch 10000: Log-likelihood=-178.13674926757812 ==================== model results ==================== Training Epochs: 10000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -178.13674926757812 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 0.585981 | 0.167168 | | item_price_obs_0 | -0.555577 | 0.145414 | | item_price_obs_1 | -0.85812 | 0.238405 | | item_price_obs_2 | -0.224599 | 0.111092 | | item_price_obs_3 | -1.08912 | 1.04131 | | item_price_obs_4 | -0.379067 | 0.101126 | | item_price_obs_5 | 0.250203 | 0.0522721 | | item_price_obs_6 | -5.99917 | 4.85404 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) )","title":"Example 1"},{"location":"nested_logit_model_house_cooling/#r-output","text":"Here we provide the output from mlogit model in R for estimation reference. Coefficient names reported are slightly different in Python and R , please use the following table for comparison. Please note that the lambda_weight_0 in Python (at the top) corresponds to the iv (inclusive value) in R (at the bottom). Orderings of coefficients for observables should be the same in both languages. Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", ## \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 11 iterations, 0h:0m:0s ## g'(-H)^-1g = 7.26E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.554878 0.144205 -3.8478 0.0001192 *** ## och -0.857886 0.255313 -3.3601 0.0007791 *** ## icca -0.225079 0.144423 -1.5585 0.1191212 ## occa -1.089458 1.219821 -0.8931 0.3717882 ## inc.room -0.378971 0.099631 -3.8038 0.0001425 *** ## inc.cooling 0.249575 0.059213 4.2149 2.499e-05 *** ## int.cooling -6.000415 5.562423 -1.0787 0.2807030 ## iv 0.585922 0.179708 3.2604 0.0011125 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -178.12","title":"R Output"},{"location":"nested_logit_model_house_cooling/#example-2","text":"The second example is similar to the first one, but we change the way we group items into different categories. Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.) category_to_item = { 0 : [ 'ec' , 'ecc' , 'gc' , 'gcc' , 'hpc' ], 1 : [ 'er' , 'erc' ]} for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset , num_epochs = 5000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 500: Log-likelihood=-193.73406982421875 Epoch 1000: Log-likelihood=-185.25933837890625 Epoch 1500: Log-likelihood=-183.55142211914062 Epoch 2000: Log-likelihood=-181.8164825439453 Epoch 2500: Log-likelihood=-180.4320526123047 Epoch 3000: Log-likelihood=-180.04095458984375 Epoch 3500: Log-likelihood=-180.7447509765625 Epoch 4000: Log-likelihood=-180.39688110351562 Epoch 4500: Log-likelihood=-180.27947998046875 Epoch 5000: Log-likelihood=-181.1483612060547 ==================== model results ==================== Training Epochs: 5000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -181.1483612060547 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 1.61072 | 0.787735 | | item_price_obs_0 | -1.34719 | 0.631206 | | item_price_obs_1 | -2.16109 | 1.0451 | | item_price_obs_2 | -0.393868 | 0.255138 | | item_price_obs_3 | -2.53253 | 2.2719 | | item_price_obs_4 | -0.884873 | 0.379626 | | item_price_obs_5 | 0.496491 | 0.248118 | | item_price_obs_6 | -15.6477 | 9.88054 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) )","title":"Example 2"},{"location":"nested_logit_model_house_cooling/#r-output_1","text":"You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", ## \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 10 iterations, 0h:0m:0s ## g'(-H)^-1g = 5.87E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -1.13818 0.54216 -2.0993 0.03579 * ## och -1.82532 0.93228 -1.9579 0.05024 . ## icca -0.33746 0.26934 -1.2529 0.21024 ## occa -2.06328 1.89726 -1.0875 0.27681 ## inc.room -0.75722 0.34292 -2.2081 0.02723 * ## inc.cooling 0.41689 0.20742 2.0099 0.04444 * ## int.cooling -13.82487 7.94031 -1.7411 0.08167 . ## iv 1.36201 0.65393 2.0828 0.03727 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.02","title":"R Output"},{"location":"nested_logit_model_house_cooling/#example-3","text":"For the third example, we now group items into three categories. Specifically, we have items gcc , ecc and erc in the first category (category 0 in the category_to_item dictionary), hpc in a category (category 1 ) alone, and items gc , ec and er in the last category (category 2 ). category_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' ], 1 : [ 'hpc' ], 2 : [ 'gc' , 'ec' , 'er' ]} for k , v in category_to_item . items (): v = [ encoder [ item ] for item in v ] category_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( category_to_item = category_to_item , category_coef_variation_dict = {}, category_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset ) ==================== received model ==================== NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0) ) ==================== training the model ==================== Epoch 500: Log-likelihood=-187.12100219726562 Epoch 1000: Log-likelihood=-182.98468017578125 Epoch 1500: Log-likelihood=-181.72171020507812 Epoch 2000: Log-likelihood=-181.3906707763672 Epoch 2500: Log-likelihood=-181.2037353515625 Epoch 3000: Log-likelihood=-181.0186767578125 Epoch 3500: Log-likelihood=-180.83331298828125 Epoch 4000: Log-likelihood=-180.6610107421875 Epoch 4500: Log-likelihood=-180.51480102539062 Epoch 5000: Log-likelihood=-180.40383911132812 ==================== model results ==================== Training Epochs: 5000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -180.40383911132812 Coefficients: | Coefficient | Estimation | Std. Err. | |:-----------------|-------------:|------------:| | lambda_weight_0 | 0.939528 | 0.193704 | | item_price_obs_0 | -0.823672 | 0.0973065 | | item_price_obs_1 | -1.31387 | 0.182701 | | item_price_obs_2 | -0.305365 | 0.12726 | | item_price_obs_3 | -1.89104 | 1.14781 | | item_price_obs_4 | -0.559503 | 0.0734163 | | item_price_obs_5 | 0.310081 | 0.0551569 | | item_price_obs_6 | -7.68508 | 5.09592 | NestedLogitModel( (category_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total). ) )","title":"Example 3"},{"location":"nested_logit_model_house_cooling/#r-output_2","text":"You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", ## \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), ## un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 8 iterations, 0h:0m:0s ## g'(-H)^-1g = 3.71E-08 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.838394 0.100546 -8.3384 < 2.2e-16 *** ## och -1.331598 0.252069 -5.2827 1.273e-07 *** ## icca -0.256131 0.145564 -1.7596 0.07848 . ## occa -1.405656 1.207281 -1.1643 0.24430 ## inc.room -0.571352 0.077950 -7.3297 2.307e-13 *** ## inc.cooling 0.311355 0.056357 5.5247 3.301e-08 *** ## int.cooling -10.413384 5.612445 -1.8554 0.06354 . ## iv 0.956544 0.180722 5.2929 1.204e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.26","title":"R Output"},{"location":"projects/","text":"Research Projects using this Package Question-Answering Data for Educational Applications Tutorial on Educational Question-Answering","title":"Related Projects"},{"location":"projects/#research-projects-using-this-package","text":"","title":"Research Projects using this Package"},{"location":"projects/#question-answering-data-for-educational-applications","text":"Tutorial on Educational Question-Answering","title":"Question-Answering Data for Educational Applications"},{"location":"test/","text":"Compatibility Check List We have tested the tutorials using the following environments, please let us know if there is any issue with our packages on other systems. Tutorial Platform Versions CPU GPU Device Tested Data Management MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Data Management Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Conditional Logit Model MacOS 12.2 Python 3.9 PyTorch 1.10.0 M1 Max N/A cpu Conditional Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Nested Logit Model MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Nested Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda","title":"Compatibility Tests"},{"location":"test/#compatibility-check-list","text":"We have tested the tutorials using the following environments, please let us know if there is any issue with our packages on other systems. Tutorial Platform Versions CPU GPU Device Tested Data Management MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Data Management Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Conditional Logit Model MacOS 12.2 Python 3.9 PyTorch 1.10.0 M1 Max N/A cpu Conditional Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Nested Logit Model MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Nested Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda","title":"Compatibility Check List"}]}