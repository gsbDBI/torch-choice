{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torch-choice Authors: Tianyu Du, Ayush Kanodia and Susan Athey; Contact: tianyudu@stanford.edu Acknowledgements: We would like to thank Erik Sverdrup, Charles Pebereau and Keshav Agrawal for their feedback. torch-choice is a library for flexible, fast choice modeling with PyTorch: it has logit and nested logit models, designed for both estimation and prediction. See the complete documentation for more details. Unique features: 1. GPU support via torch for speed 2. Specify customized models 3. Specify availability sets 4. Maximum Likelihood Estimation (MLE) (optionally, reporting standard errors or MAP inference with Bayesian Priors on coefficients) 5. Estimation via minimization of Cross Entropy Loss (optionally with L1/L2 regularization) Introduction Logistic Regression and Choice Models Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] where, \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] here \\(X\\) , \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\) , \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\) . When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\) , as \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] where \\(\\epsilon\\) is an unobserved random error term. If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\) , this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\) , as shown by McFadden , and as often studied in Econometrics. Package We implement a fully flexible setup, where we allow 1. coefficients ( \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) , \\(\\dots\\) ) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\) ), item-specific (i.e., \\(\\alpha=\\alpha_i\\) ), session-specific (i.e., \\(\\alpha=\\alpha_t\\) ), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\) ). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables ( \\(X\\) , \\(Y\\) , \\(\\dots\\) ) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\) This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\) , which enables modelling rich structures. As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial) \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3) MNIST classification (Upcoming Detailed Tutorial) \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1) This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form. Installation Clone the repository to your local machine or server. Install required dependencies using: pip3 install -r requirements.txt . Run pip3 install torch-choice . Check installation by running python3 -c 'import torch_choice; print(torch_choice.__version__)' . The installation page provides more details on installation. Example Usage - Transportation Choice Dataset In this demonstration, we setup a minimal example of fitting a conditional logit model using our package. We provide equivalent R code as well for reference, to aid replicating from R to this package. We are modelling people's choices on transportation modes using the publicly available ModeCanada dataset. More information about the ModeCanada: Mode Choice for the Montreal-Toronto Corridor . In this example, we are estimating the utility for user \\(u\\) to choose transport method \\(i\\) in session \\(s\\) as $$ U_{uis} = \\alpha_i + \\beta_i \\text{income}_s + \\gamma \\text{cost} + \\delta \\text{freq} + \\eta \\text{ovt} + \\iota_i \\text{ivt} + \\varepsilon $$ this is equivalent to the functional form described in the previous section Mode Canada with Torch-Choice # load packages. import pandas as pd import torch_choice # load data. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA' ) . query ( 'noalt == 4' ) . reset_index ( drop = True ) # format data. data = torch_choice . utils . easy_data_wrapper . EasyDatasetWrapper ( main_data = df , purchase_record_column = 'case' , choice_column = 'choice' , item_name_column = 'alt' , user_index_column = 'case' , session_index_column = 'case' , session_observable_columns = [ 'income' ], price_observable_columns = [ 'cost' , 'freq' , 'ovt' , 'ivt' ]) # define the conditional logit model. model = torch_choice . model . ConditionalLogitModel ( coef_variation_dict = { 'price_cost' : 'constant' , 'price_freq' : 'constant' , 'price_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_items = 4 ) # fit the conditional logit model. torch_choice . utils . run_helper . run ( model , data . choice_dataset , num_epochs = 5000 , learning_rate = 0.01 , batch_size =- 1 ) Mode Canada with R We include the R code for the ModeCanada example as well. # load packages. library(\"mlogit\") # load data. ModeCanada <- read.csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA') ModeCanada <- select(ModeCanada, -X) ModeCanada$alt <- as.factor(ModeCanada$alt) # format data. MC <- dfidx(ModeCanada, subset = noalt == 4) # fit the data. ml.MC1 <- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air') summary(ml.MC1) What's in the package? Overall, the torch-choice package offers the following features: The package includes a data management module called ChoiceDataset , which is built upon PyTorch's dataset module. Our dataset implementation allows users to easily move data between CPU and GPU. Unlike traditional long or wide formats, the ChoiceDataset offers a memory-efficient way to manage observables. The package provides a (1) conditional logit model and (2) a nested logit model for consumer choice modeling. The package leverage GPU acceleration using PyTorch and easily scale to large dataset of millions of choice records. All models are trained using state-of-the-art optimizers by in PyTorch. These optimization algorithms are tested to be scalable by modern machine learning practitioners. However, you can rest assure that the package runs flawlessly when no GPU is used as well. Setting up the PyTorch training pipelines can be frustrating. We provide easy-to-use PyTorch lightning wrapper of models to free researchers from the hassle from setting up PyTorch optimizers and training loops.","title":"Home"},{"location":"#torch-choice","text":"Authors: Tianyu Du, Ayush Kanodia and Susan Athey; Contact: tianyudu@stanford.edu Acknowledgements: We would like to thank Erik Sverdrup, Charles Pebereau and Keshav Agrawal for their feedback. torch-choice is a library for flexible, fast choice modeling with PyTorch: it has logit and nested logit models, designed for both estimation and prediction. See the complete documentation for more details. Unique features: 1. GPU support via torch for speed 2. Specify customized models 3. Specify availability sets 4. Maximum Likelihood Estimation (MLE) (optionally, reporting standard errors or MAP inference with Bayesian Priors on coefficients) 5. Estimation via minimization of Cross Entropy Loss (optionally with L1/L2 regularization)","title":"torch-choice"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#logistic-regression-and-choice-models","text":"Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] where, \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] here \\(X\\) , \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\) , \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\) . When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\) , as \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] where \\(\\epsilon\\) is an unobserved random error term. If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\) , this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\) , as shown by McFadden , and as often studied in Econometrics.","title":"Logistic Regression and Choice Models"},{"location":"#package","text":"We implement a fully flexible setup, where we allow 1. coefficients ( \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) , \\(\\dots\\) ) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\) ), item-specific (i.e., \\(\\alpha=\\alpha_i\\) ), session-specific (i.e., \\(\\alpha=\\alpha_t\\) ), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\) ). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables ( \\(X\\) , \\(Y\\) , \\(\\dots\\) ) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\) This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\) , which enables modelling rich structures. As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial) \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3) MNIST classification (Upcoming Detailed Tutorial) \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1) This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form.","title":"Package"},{"location":"#installation","text":"Clone the repository to your local machine or server. Install required dependencies using: pip3 install -r requirements.txt . Run pip3 install torch-choice . Check installation by running python3 -c 'import torch_choice; print(torch_choice.__version__)' . The installation page provides more details on installation.","title":"Installation"},{"location":"#example-usage-transportation-choice-dataset","text":"In this demonstration, we setup a minimal example of fitting a conditional logit model using our package. We provide equivalent R code as well for reference, to aid replicating from R to this package. We are modelling people's choices on transportation modes using the publicly available ModeCanada dataset. More information about the ModeCanada: Mode Choice for the Montreal-Toronto Corridor . In this example, we are estimating the utility for user \\(u\\) to choose transport method \\(i\\) in session \\(s\\) as $$ U_{uis} = \\alpha_i + \\beta_i \\text{income}_s + \\gamma \\text{cost} + \\delta \\text{freq} + \\eta \\text{ovt} + \\iota_i \\text{ivt} + \\varepsilon $$ this is equivalent to the functional form described in the previous section","title":"Example Usage - Transportation Choice Dataset"},{"location":"#mode-canada-with-torch-choice","text":"# load packages. import pandas as pd import torch_choice # load data. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA' ) . query ( 'noalt == 4' ) . reset_index ( drop = True ) # format data. data = torch_choice . utils . easy_data_wrapper . EasyDatasetWrapper ( main_data = df , purchase_record_column = 'case' , choice_column = 'choice' , item_name_column = 'alt' , user_index_column = 'case' , session_index_column = 'case' , session_observable_columns = [ 'income' ], price_observable_columns = [ 'cost' , 'freq' , 'ovt' , 'ivt' ]) # define the conditional logit model. model = torch_choice . model . ConditionalLogitModel ( coef_variation_dict = { 'price_cost' : 'constant' , 'price_freq' : 'constant' , 'price_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_items = 4 ) # fit the conditional logit model. torch_choice . utils . run_helper . run ( model , data . choice_dataset , num_epochs = 5000 , learning_rate = 0.01 , batch_size =- 1 )","title":"Mode Canada with Torch-Choice"},{"location":"#mode-canada-with-r","text":"We include the R code for the ModeCanada example as well. # load packages. library(\"mlogit\") # load data. ModeCanada <- read.csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA') ModeCanada <- select(ModeCanada, -X) ModeCanada$alt <- as.factor(ModeCanada$alt) # format data. MC <- dfidx(ModeCanada, subset = noalt == 4) # fit the data. ml.MC1 <- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air') summary(ml.MC1)","title":"Mode Canada with R"},{"location":"#whats-in-the-package","text":"Overall, the torch-choice package offers the following features: The package includes a data management module called ChoiceDataset , which is built upon PyTorch's dataset module. Our dataset implementation allows users to easily move data between CPU and GPU. Unlike traditional long or wide formats, the ChoiceDataset offers a memory-efficient way to manage observables. The package provides a (1) conditional logit model and (2) a nested logit model for consumer choice modeling. The package leverage GPU acceleration using PyTorch and easily scale to large dataset of millions of choice records. All models are trained using state-of-the-art optimizers by in PyTorch. These optimization algorithms are tested to be scalable by modern machine learning practitioners. However, you can rest assure that the package runs flawlessly when no GPU is used as well. Setting up the PyTorch training pipelines can be frustrating. We provide easy-to-use PyTorch lightning wrapper of models to free researchers from the hassle from setting up PyTorch optimizers and training loops.","title":"What's in the package?"},{"location":"api_torch_choice/","text":"API Reference: Torch Choice Bases: torch . utils . data . Dataset Source code in torch_choice/data/choice_dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 class ChoiceDataset ( torch . utils . data . Dataset ): def __init__ ( self , item_index : torch . LongTensor , num_items : int = None , num_users : int = None , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`. num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) 6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price observable. \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . _num_items = num_items self . _num_users = num_users self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): if self . _is_attribute ( key ): # all observable should be float. item = item . float () setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset. def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) def __contains__ ( self , key : str ) -> bool : return key in self . keys def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag @property def device ( self ) -> str : \"\"\"Returns the device of the dataset. Returns: str: the device of the dataset. \"\"\" for attr in self . __dict__ . values (): if torch . is_tensor ( attr ): return attr . device @property def num_users ( self ) -> int : \"\"\"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: int: the number of users involved in this dataset. \"\"\" if self . _num_users is not None : return self . _num_users elif self . user_index is not None : # infer from the number of unique items in user_index. return len ( torch . unique ( self . user_index )) else : return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_user_attribute(key) or self._is_taste_attribute(key): # return val.shape[0] # return 1 @property def num_items ( self ) -> int : \"\"\"Returns the number of items involved in this dataset. Returns: int: the number of items involved in this dataset. \"\"\" if self . _num_items is not None : # return the _num_items provided in the constructor. return self . _num_items else : # infer the number of items from item_index. return len ( torch . unique ( self . item_index )) # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_item_attribute(key): # return val.shape[0] # elif self._is_taste_attribute(key) or self._is_price_attribute(key): # return val.shape[1] # return 1 @property def num_sessions ( self ) -> int : \"\"\"Returns the number of sessions involved in this dataset. Returns: int: the number of sessions involved in this dataset. \"\"\" return len ( torch . unique ( self . session_index )) # if self.session_index is None: # return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_session_attribute(key) or self._is_price_attribute(key): # return val.shape[0] # return 1 @property def x_dict ( self ) -> Dict [ object , torch . Tensor ]: \"\"\"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. \"\"\" out = dict () for key , val in self . __dict__ . items (): if self . _is_attribute ( key ): # only include attributes. out [ key ] = self . _expand_tensor ( key , val ) # reshape to (num_sessions, num_items, num_params). return out @classmethod def _from_dict ( cls , dictionary : Dict [ str , torch . tensor ]) -> \"ChoiceDataset\" : \"\"\"Creates an instance of ChoiceDataset from a dictionary of arguments. Args: dictionary (Dict[str, torch.tensor]): a dictionary with keys as argument names and values as arguments. Returns: ChoiceDataset: the created copy of dataset. \"\"\" dataset = cls ( ** dictionary ) for key , item in dictionary . items (): setattr ( dataset , key , item ) return dataset def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) def _check_device_consistency ( self ) -> None : \"\"\"Checks if all tensors in this dataset are on the same device. Raises: Exception: an exception is raised if not all tensors are on the same device. \"\"\" # assert all tensors are on the same device. devices = list () for val in self . __dict__ . values (): if torch . is_tensor ( val ): devices . append ( val . device ) if len ( set ( devices )) > 1 : raise Exception ( f 'Found tensors on different devices: { set ( devices ) } .' , 'Use dataset.to() method to align devices.' ) def _size_repr ( self , value : object ) -> List [ int ]: \"\"\"A helper method to get the string-representation of object sizes, this is helpful while constructing the string representation of the dataset. Args: value (object): an object to examine its size. Returns: List[int]: list of integers representing the size of the object, length of the list is equal to dimension of `value`. \"\"\" if torch . is_tensor ( value ): return list ( value . size ()) elif isinstance ( value , int ) or isinstance ( value , float ): return [ 1 ] elif isinstance ( value , list ) or isinstance ( value , tuple ): return [ len ( value )] else : return [] def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" # don't print shapes of internal attributes like _num_users and _num_items. info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items () if not key . startswith ( '_' )] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" # ================================================================================================================== # methods for checking attribute categories. # ================================================================================================================== @staticmethod def _is_item_attribute ( key : str ) -> bool : return key . startswith ( 'item_' ) and ( key != 'item_availability' ) and ( key != 'item_index' ) @staticmethod def _is_user_attribute ( key : str ) -> bool : return key . startswith ( 'user_' ) and ( key != 'user_index' ) @staticmethod def _is_session_attribute ( key : str ) -> bool : return key . startswith ( 'session_' ) and ( key != 'session_index' ) @staticmethod def _is_taste_attribute ( key : str ) -> bool : return key . startswith ( 'taste_' ) @staticmethod def _is_price_attribute ( key : str ) -> bool : return key . startswith ( 'price_' ) or key . startswith ( 'itemsession_' ) def _is_attribute ( self , key : str ) -> bool : return self . _is_item_attribute ( key ) \\ or self . _is_user_attribute ( key ) \\ or self . _is_session_attribute ( key ) \\ or self . _is_taste_attribute ( key ) \\ or self . _is_price_attribute ( key ) def _expand_tensor ( self , key : str , val : torch . Tensor ) -> torch . Tensor : \"\"\"Expands attribute tensor to (num_sessions, num_items, num_params) shape for prediction tasks, this method won't reshape the tensor at all if the `key` (i.e., name of the tensor) suggests its not an attribute of any kind. Args: key (str): name of the attribute used to determine the raw shape of the tensor. For example, 'item_obs' means the raw tensor is in shape (num_items, num_params). val (torch.Tensor): the attribute tensor to be reshaped. Returns: torch.Tensor: the reshaped tensor with shape (num_sessions, num_items, num_params). \"\"\" if not self . _is_attribute ( key ): print ( f 'Warning: the input key { key } is not an attribute of the dataset, will NOT modify the provided tensor.' ) # don't expand non-attribute tensors, if any. return val num_params = val . shape [ - 1 ] if self . _is_user_attribute ( key ): # user_attribute (num_users, *) out = val [ self . user_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_item_attribute ( key ): # item_attribute (num_items, *) out = val . view ( 1 , self . num_items , num_params ) . expand ( len ( self ), - 1 , - 1 ) elif self . _is_session_attribute ( key ): # session_attribute (num_sessions, *) out = val [ self . session_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_taste_attribute ( key ): # taste_attribute (num_users, num_items, *) out = val [ self . user_index , :, :] elif self . _is_price_attribute ( key ): # price_attribute (num_sessions, num_items, *) out = val [ self . session_index , :, :] assert out . shape == ( len ( self ), self . num_items , num_params ) return out @staticmethod def unique ( tensor : torch . Tensor ) -> Tuple [ np . ndarray ]: arr = tensor . cpu () . numpy () unique , counts = np . unique ( arr , return_counts = True ) count_sort_ind = np . argsort ( - counts ) unique = unique [ count_sort_ind ] counts = counts [ count_sort_ind ] return unique , counts def summary ( self ) -> None : \"\"\"A method to summarize the dataset. Returns: str: the string representation of the dataset. \"\"\" summary = [ 'ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .' . format ( self . num_sessions , self . num_items , self . num_users if self . user_index is not None else 'single' , len ( self ))] # summarize users. if self . user_index is not None : unique , counts = self . unique ( self . user_index ) summary . append ( f \"The most frequent user is { unique [ 0 ] } with { counts [ 0 ] } observations; the least frequent user is { unique [ - 1 ] } with { counts [ - 1 ] } observations; on average, there are { counts . astype ( float ) . mean () : .2f } observations per user.\" ) N = len ( unique ) K = min ( 5 , N ) string = f ' { K } most frequent users are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent users are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) # summarize items. unique , counts = self . unique ( self . item_index ) N = len ( unique ) K = min ( 5 , N ) summary . append ( f \"The most frequent item is { unique [ 0 ] } , it was chosen { counts [ 0 ] } times; the least frequent item is { unique [ - 1 ] } it was { counts [ - 1 ] } times; on average, each item was purchased { counts . astype ( float ) . mean () : .2f } times.\" ) string = f ' { K } most frequent items are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent items are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) summary . append ( 'Attribute Summaries:' ) for key , item in self . __dict__ . items (): if self . _is_attribute ( key ) and torch . is_tensor ( item ): summary . append ( \"Observable Tensor ' {} ' with shape {} \" . format ( key , item . shape )) # price attributes are 3-dimensional tensors, ignore for cleanness here. if not self . _is_price_attribute ( key ): summary . append ( str ( pd . DataFrame ( item . to ( 'cpu' ) . float () . numpy ()) . describe ())) print ( ' \\n ' . join ( summary ) + f \" \\n device= { self . device } \" ) return None device : str property Returns the device of the dataset. Returns: Name Type Description str str the device of the dataset. num_items : int property Returns the number of items involved in this dataset. Returns: Name Type Description int int the number of items involved in this dataset. num_sessions : int property Returns the number of sessions involved in this dataset. Returns: Name Type Description int int the number of sessions involved in this dataset. num_users : int property Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: Name Type Description int int the number of users involved in this dataset. x_dict : Dict [ object , torch . Tensor ] property Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Type Description Dict [ object , torch . Tensor ] Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. __eq__ ( other ) Returns whether all tensor attributes of both ChoiceDatasets are equal. Source code in torch_choice/data/choice_dataset.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag __getitem__ ( indices ) Retrieves samples corresponding to the provided index or list of indices. Parameters: Name Type Description Default indices Union [ int , torch . LongTensor ] a single integer index or a tensor of indices. required Returns: Name Type Description ChoiceDataset ChoiceDataset a subset of the dataset. Source code in torch_choice/data/choice_dataset.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) __init__ ( item_index , num_items = None , num_users = None , label = None , user_index = None , session_index = None , item_availability = None , ** kwargs ) Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called batch_size in the documentation. The batch_size corresponds to the file length in wide-format dataset, and often denoted using N . We call it batch_size to follow the convention in machine learning literature. A choice instance is a row of the dataset, so there are batch_size choice instances in each ChoiceDataset . The dataset consists of: (1) a collection of batch_size tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of observables associated with item, user, session, etc. Parameters: Name Type Description Default item_index torch . LongTensor a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the label tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. required num_items Optional [ int ] the number of items in the dataset. If None is provided (default), the number of items will be inferred from the number of unique numbers in item_index . None num_users Optional [ int ] the number of users in the dataset. If None is provided (default), the number of users will be inferred from the number of unique numbers in user_index . None label Optional [ torch . LongTensor ] a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the label argument as None in the initialization method, and the model will use item_index as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. None user_index Optional [ torch . LongTensor ] a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If None user index is provided, it's assumed that the choice instances are from the same user. user_index is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. None session_index Optional [ torch . LongTensor ] a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as None . In this case, the ChoiceDataset object will assume each choice instance to be in its own session. Defaults to None. None item_availability Optional [ torch . BoolTensor ] A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. None Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, ) 2. item observables must start with 'item_' and have shape (num_items, ) 3. session observables must start with 'session_' and have shape (num_sessions, ) 4. taste observables (those vary by user and item) must start with taste_ and have shape (num_users, num_items, ). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with price_ and have shape (num_sessions, num_items, *) 6. itemsession observables starting with itemsession_ , this is a more intuitive alias to the price observable. Source code in torch_choice/data/choice_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , item_index : torch . LongTensor , num_items : int = None , num_users : int = None , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`. num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) 6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price observable. \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . _num_items = num_items self . _num_users = num_users self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): if self . _is_attribute ( key ): # all observable should be float. item = item . float () setattr ( self , key , item ) __len__ () Returns number of samples in this dataset. Returns: Name Type Description int int length of the dataset. Source code in torch_choice/data/choice_dataset.py 153 154 155 156 157 158 159 def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) __repr__ () A method to get a string representation of the dataset. Returns: Name Type Description str str the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py 358 359 360 361 362 363 364 365 366 def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" # don't print shapes of internal attributes like _num_users and _num_items. info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items () if not key . startswith ( '_' )] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" apply_tensor ( func ) This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Parameters: Name Type Description Default func callable a callable function to be applied on tensors and tensor-values of dictionaries. required Returns: Name Type Description ChoiceDataset ChoiceDataset the modified dataset. Source code in torch_choice/data/choice_dataset.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self clone () Creates a copy of self. Returns: Name Type Description ChoiceDataset ChoiceDataset a copy of self. Source code in torch_choice/data/choice_dataset.py 310 311 312 313 314 315 316 317 318 319 320 321 322 def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) summary () A method to summarize the dataset. Returns: Name Type Description str None the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def summary ( self ) -> None : \"\"\"A method to summarize the dataset. Returns: str: the string representation of the dataset. \"\"\" summary = [ 'ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .' . format ( self . num_sessions , self . num_items , self . num_users if self . user_index is not None else 'single' , len ( self ))] # summarize users. if self . user_index is not None : unique , counts = self . unique ( self . user_index ) summary . append ( f \"The most frequent user is { unique [ 0 ] } with { counts [ 0 ] } observations; the least frequent user is { unique [ - 1 ] } with { counts [ - 1 ] } observations; on average, there are { counts . astype ( float ) . mean () : .2f } observations per user.\" ) N = len ( unique ) K = min ( 5 , N ) string = f ' { K } most frequent users are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent users are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) # summarize items. unique , counts = self . unique ( self . item_index ) N = len ( unique ) K = min ( 5 , N ) summary . append ( f \"The most frequent item is { unique [ 0 ] } , it was chosen { counts [ 0 ] } times; the least frequent item is { unique [ - 1 ] } it was { counts [ - 1 ] } times; on average, each item was purchased { counts . astype ( float ) . mean () : .2f } times.\" ) string = f ' { K } most frequent items are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent items are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) summary . append ( 'Attribute Summaries:' ) for key , item in self . __dict__ . items (): if self . _is_attribute ( key ) and torch . is_tensor ( item ): summary . append ( \"Observable Tensor ' {} ' with shape {} \" . format ( key , item . shape )) # price attributes are 3-dimensional tensors, ignore for cleanness here. if not self . _is_price_attribute ( key ): summary . append ( str ( pd . DataFrame ( item . to ( 'cpu' ) . float () . numpy ()) . describe ())) print ( ' \\n ' . join ( summary ) + f \" \\n device= { self . device } \" ) return None to ( device ) Moves all tensors in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union [ str , torch . device ] the destination device. required Returns: Name Type Description ChoiceDataset ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/choice_dataset.py 299 300 301 302 303 304 305 306 307 308 def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) Bases: torch . utils . data . Dataset A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Source code in torch_choice/data/joint_dataset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class JointDataset ( torch . utils . data . Dataset ): \"\"\"A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. \"\"\" def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) @property def device ( self ) -> str : \"\"\"Returns the device of datasets contained in the joint dataset. Returns: str: the device of the dataset. \"\"\" for d in self . datasets . values (): return d . device def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self device : str property Returns the device of datasets contained in the joint dataset. Returns: Name Type Description str str the device of the dataset. __getitem__ ( indices ) Queries samples from the dataset by index. Parameters: Name Type Description Default indices Union [ int , torch . LongTensor ] an integer or a 1D tensor of multiple indices. required Returns: Type Description Dict [ str , ChoiceDataset ] Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the datasets argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. Source code in torch_choice/data/joint_dataset.py 55 56 57 58 59 60 61 62 63 64 65 66 def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) __init__ ( ** datasets ) The initialize methods. Source code in torch_choice/data/joint_dataset.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 __len__ () Get the number of samples in the joint dataset. Returns: Name Type Description int int the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. Source code in torch_choice/data/joint_dataset.py 46 47 48 49 50 51 52 53 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) __repr__ () A method to get a string representation of the dataset. Returns: Name Type Description str str the string representation of the dataset. Source code in torch_choice/data/joint_dataset.py 68 69 70 71 72 73 74 75 76 77 78 def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) to ( device ) Moves all datasets in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union [ str , torch . device ] the destination device. required Returns: Name Type Description ChoiceDataset JointDataset the modified dataset on the new device. Source code in torch_choice/data/joint_dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self Bases: nn . Module The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: unless the -full flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. constant: constant over all users and items, user: user-specific parameters but constant across all items, item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. item-full: item-specific parameters but constant across all users, explicitly model for all items. user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full: parameters that are specific to both user and item, explicitly model for all items. Source code in torch_choice/model/conditional_logit_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 class ConditionalLogitModel ( nn . Module ): \"\"\"The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: NOTE: unless the `-full` flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. - constant: constant over all users and items, - user: user-specific parameters but constant across all items, - item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. - item-full: item-specific parameters but constant across all users, explicitly model for all items. - user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - user-item-full: parameters that are specific to both user and item, explicitly model for all items. \"\"\" def __init__ ( self , formula : Optional [ str ] = None , dataset : Optional [ ChoiceDataset ] = None , coef_variation_dict : Optional [ Dict [ str , str ]] = None , num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\" Args: formula (str): a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # ============================================================================================================== # Check that the model received a valid combination of inputs so that it can be initialized. # ============================================================================================================== if coef_variation_dict is None and formula is None : raise ValueError ( \"Either coef_variation_dict or formula should be provided to specify the model.\" ) if ( coef_variation_dict is not None ) and ( formula is not None ): raise ValueError ( \"Only one of coef_variation_dict or formula should be provided to specify the model.\" ) if ( formula is not None ) and ( dataset is None ): raise ValueError ( \"If formula is provided, data should be provided to specify the model.\" ) # ============================================================================================================== # Build necessary dictionaries for model initialization. # ============================================================================================================== if formula is None : # Use dictionaries to initialize the model. if num_param_dict is None : warnings . warn ( \"`num_param_dict` is not provided, all variables will be treated as having one parameter.\" ) num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () # variable `var` with variation `spec` to variable `var[spec]`. rename = dict () # old variable name --> new variable name. for variable , specificity in coef_variation_dict . items (): rename [ variable ] = f \" { variable } [ { specificity } ]\" for old_name , new_name in rename . items (): coef_variation_dict [ new_name ] = coef_variation_dict . pop ( old_name ) num_param_dict [ new_name ] = num_param_dict . pop ( old_name ) else : # Use the formula to infer model. coef_variation_dict , num_param_dict = parse_formula ( formula , dataset ) # ============================================================================================================== # Model Initialization. # ============================================================================================================== super ( ConditionalLogitModel , self ) . __init__ () self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ) and variable not in self . num_param_dict . keys (): warnings . warn ( f \"` { variable } ` key found in coef_variation_dict but not in num_param_dict, num_param_dict[' { variable } '] has been set to 1.\" ) self . num_param_dict [ variable ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) + ' \\n ' + f 'device= { self . device } ' @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ): # intercept term has no input tensor from the ChoiceDataset data structure. # the tensor for intercept has only 1 feature, every entry is 1. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) break # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name. corresponding_observable = var_type . split ( \"[\" )[ 0 ] total_utility += coef ( x_dict [ corresponding_observable ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavailable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for param in self . parameters (): nll += self . regularization_weight * torch . norm ( param , p = L ) return nll @property def device ( self ) -> torch . device : \"\"\"Returns the device of the coefficient. Returns: torch.device: the device of the model. \"\"\" return next ( iter ( self . coef_dict . values ())) . device @staticmethod def is_intercept_term ( variable : str ): # check if the given variable is an intercept (fixed effect) term. # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc. return ( variable . startswith ( 'intercept[' ) and variable . endswith ( ']' )) def get_coefficient ( self , variable : str ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" return self . state_dict ()[ f \"coef_dict. { variable } .coef\" ] . detach () . clone () device : torch . device property Returns the device of the coefficient. Returns: Type Description torch . device torch.device: the device of the model. num_params : int property Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Name Type Description int int the total number of learnable parameters. __init__ ( formula = None , dataset = None , coef_variation_dict = None , num_param_dict = None , num_items = None , num_users = None , regularization = None , regularization_weight = None ) Parameters: Name Type Description Default formula str a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. None data ChoiceDataset a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. required coef_variation_dict Dict [ str , str ] variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with itemsession_ , price_ , user_ , etc), or intercept if the researcher requires an intercept term. For each variable name X_var (e.g., user_income ) or intercept , the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. constant : the coefficient constant over all users and items: \\(X \beta\\) . user : user-specific parameters but constant across all items: \\(X \beta_{u}\\) . item : item-specific parameters but constant across all users, \\(X \beta_{i}\\) . Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. item-full : the same configuration as item , but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - user-item : parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full : parameters that are specific to both user and item, explicitly model for all items. None num_param_dict Optional [ Dict [ str , int ]] variable type to number of parameters dictionary with keys exactly the same as the coef_variation_dict . Values of num_param_dict records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the coef_variation_dict dictionary and values of all ones. Default to be None. None num_items int number of items in the dataset. None num_users int number of users in the dataset. None regularization Optional [ str ] this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. None regularization_weight Optional [ float ] the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. None Source code in torch_choice/model/conditional_logit_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , formula : Optional [ str ] = None , dataset : Optional [ ChoiceDataset ] = None , coef_variation_dict : Optional [ Dict [ str , str ]] = None , num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\" Args: formula (str): a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # ============================================================================================================== # Check that the model received a valid combination of inputs so that it can be initialized. # ============================================================================================================== if coef_variation_dict is None and formula is None : raise ValueError ( \"Either coef_variation_dict or formula should be provided to specify the model.\" ) if ( coef_variation_dict is not None ) and ( formula is not None ): raise ValueError ( \"Only one of coef_variation_dict or formula should be provided to specify the model.\" ) if ( formula is not None ) and ( dataset is None ): raise ValueError ( \"If formula is provided, data should be provided to specify the model.\" ) # ============================================================================================================== # Build necessary dictionaries for model initialization. # ============================================================================================================== if formula is None : # Use dictionaries to initialize the model. if num_param_dict is None : warnings . warn ( \"`num_param_dict` is not provided, all variables will be treated as having one parameter.\" ) num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () # variable `var` with variation `spec` to variable `var[spec]`. rename = dict () # old variable name --> new variable name. for variable , specificity in coef_variation_dict . items (): rename [ variable ] = f \" { variable } [ { specificity } ]\" for old_name , new_name in rename . items (): coef_variation_dict [ new_name ] = coef_variation_dict . pop ( old_name ) num_param_dict [ new_name ] = num_param_dict . pop ( old_name ) else : # Use the formula to infer model. coef_variation_dict , num_param_dict = parse_formula ( formula , dataset ) # ============================================================================================================== # Model Initialization. # ============================================================================================================== super ( ConditionalLogitModel , self ) . __init__ () self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ) and variable not in self . num_param_dict . keys (): warnings . warn ( f \"` { variable } ` key found in coef_variation_dict but not in num_param_dict, num_param_dict[' { variable } '] has been set to 1.\" ) self . num_param_dict [ variable ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) __repr__ () Return a string representation of the model. Returns: Name Type Description str str the string representation of the model. Source code in torch_choice/model/conditional_logit_model.py 180 181 182 183 184 185 186 187 188 189 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) + ' \\n ' + f 'device= { self . device } ' forward ( batch , manual_coef_value_dict = None ) Forward pass of the model. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object. required manual_coef_value_dict Optional [ Dict [ str , torch . Tensor ]] a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. None Returns: Type Description torch . Tensor torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. Source code in torch_choice/model/conditional_logit_model.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ): # intercept term has no input tensor from the ChoiceDataset data structure. # the tensor for intercept has only 1 feature, every entry is 1. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) break # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name. corresponding_observable = var_type . split ( \"[\" )[ 0 ] total_utility += coef ( x_dict [ corresponding_observable ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavailable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility get_coefficient ( variable ) Retrieve the coefficient tensor for the given variable. Parameters: Name Type Description Default variable str the variable name. required Returns: Type Description torch . Tensor torch.Tensor: the corresponding coefficient tensor of the requested variable. Source code in torch_choice/model/conditional_logit_model.py 307 308 309 310 311 312 313 314 315 316 def get_coefficient ( self , variable : str ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" return self . state_dict ()[ f \"coef_dict. { variable } .coef\" ] . detach () . clone () loss ( * args , ** kwargs ) The loss function to be optimized. This is a wrapper of negative_log_likelihood + regularization loss if required. Source code in torch_choice/model/conditional_logit_model.py 283 284 285 286 287 288 289 290 def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for param in self . parameters (): nll += self . regularization_weight * torch . norm ( param , p = L ) return nll negative_log_likelihood ( batch , y , is_train = True ) Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data. required y torch . Tensor the label. required is_train bool whether to trace the gradient. Defaults to True. True Returns: Type Description torch . Tensor torch.Tensor: the negative log-likelihood. Source code in torch_choice/model/conditional_logit_model.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll summary () Print out the current model parameter. Source code in torch_choice/model/conditional_logit_model.py 202 203 204 205 206 207 def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) Bases: nn . Module Source code in torch_choice/model/nested_logit_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class NestedLogitModel ( nn . Module ): def __init__ ( self , nest_to_item : Dict [ object , List [ int ]], # method 1: specify variation and num param. dictionary. nest_coef_variation_dict : Optional [ Dict [ str , str ]] = None , nest_num_param_dict : Optional [ Dict [ str , int ]] = None , item_coef_variation_dict : Optional [ Dict [ str , str ]] = None , item_num_param_dict : Optional [ Dict [ str , int ]] = None , # method 2: specify formula and dataset. item_formula : Optional [ str ] = None , nest_formula : Optional [ str ] = None , dataset : Optional [ JointDataset ] = None , num_users : Optional [ int ] = None , shared_lambda : bool = False , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\"Initialization method of the nested logit model. Args: nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list of items IDs of the queried nest. nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item features. {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # handle nest level model. using_formula_to_initiate = ( item_formula is not None ) and ( nest_formula is not None ) if using_formula_to_initiate : # make sure that the research does not specify duplicated information, which might cause conflict. if ( nest_coef_variation_dict is not None ) or ( item_coef_variation_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.' ) if ( nest_num_param_dict is not None ) or ( item_num_param_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.' ) if dataset is None : raise ValueError ( 'Dataset is required if {item, nest}_formula is specified to initiate the model.' ) nest_coef_variation_dict , nest_num_param_dict = parse_formula ( nest_formula , dataset . datasets [ 'nest' ]) item_coef_variation_dict , item_num_param_dict = parse_formula ( item_formula , dataset . datasets [ 'item' ]) else : # check for conflicting information. if ( nest_formula is not None ) or ( item_formula is not None ): raise ValueError ( 'You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.' ) # make sure that the research specifies all the required information. if ( nest_coef_variation_dict is None ) or ( item_coef_variation_dict is None ): raise ValueError ( 'You should specify the {item, nest}_coef_variation_dict to initiate the model.' ) if ( nest_num_param_dict is None ) or ( item_num_param_dict is None ): raise ValueError ( 'You should specify the {item, nest}_num_param_dict to initiate the model.' ) super ( NestedLogitModel , self ) . __init__ () self . nest_to_item = nest_to_item self . nest_coef_variation_dict = nest_coef_variation_dict self . nest_num_param_dict = nest_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . nests = list ( nest_to_item . keys ()) self . num_nests = len ( self . nests ) self . num_items = sum ( len ( items ) for items in nest_to_item . values ()) # nest coefficients. self . nest_coef_dict = self . _build_coef_dict ( self . nest_coef_variation_dict , self . nest_num_param_dict , self . num_nests ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_nests ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def _build_coef_dict ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Dict [ str , int ], num_items : int ) -> nn . ModuleDict : \"\"\"Builds a coefficient dictionary containing all trainable components of the model, mapping coefficient names to the corresponding Coefficient Module. num_items could be the actual number of items or the number of nests depends on the use case. NOTE: torch-choice users don't directly interact with this method. Args: coef_variation_dict (Dict[str, str]): a dictionary mapping coefficient names (e.g., theta_user) to the level of variation (e.g., 'user'). num_param_dict (Dict[str, int]): a dictionary mapping coefficient names to the number of parameters in this coefficient. Be aware that, for example, if there is one K-dimensional coefficient for every user, then the `num_param` should be K instead of K x number of users. num_items (int): the total number of items in the prediction problem. `num_items` should be the number of nests if _build_coef_dict() is used for nest-level prediction. Returns: nn.ModuleDict: a PyTorch ModuleDict object mapping from coefficient names to training Coefficient. \"\"\" coef_dict = dict () for var_type , variation in coef_variation_dict . items (): num_params = num_param_dict [ var_type ] coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = num_items , num_users = self . num_users , num_params = num_params ) return nn . ModuleDict ( coef_dict ) def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'nest' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) def _forward ( self , nest_x_dict : Dict [ str , torch . Tensor ], item_x_dict : Dict [ str , torch . Tensor ], user_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None ) -> torch . Tensor : \"\"\"\"Computes log P[t, i] = the log probability for the user involved in trip t to choose item i. Let n denote the ID of the user involved in trip t, then P[t, i] = P_{ni} on page 86 of the book \"discrete choice methods with simulation\" by Train. The `_forward` method is an internal API, users should refer to the `forward` method. Args: nest_x_dict (torch.Tensor): a dictionary mapping from nest-level feature names to the corresponding feature tensor. item_x_dict (torch.Tensor): a dictionary mapping from item-level feature names to the corresponding feature tensor. More details on the shape of the tensors can be found in the docstring of the `x_dict` method of `ChoiceDataset`. user_index (torch.LongTensor): a tensor of shape (num_trips,) indicating which user is making decision in each trip. Setting user_index = None assumes the same user is making decisions in all trips. item_availability (torch.BoolTensor): a boolean tensor with shape (num_trips, num_items) indicating the aviliability of items in each trip. If item_availability[t, i] = False, the utility of choosing item i in trip t, V[t, i], will be set to -inf. Given the decomposition V[t, i] = W[t, k(i)] + Y[t, i] + eps, V[t, i] is set to -inf by setting Y[t, i] = -inf for unavilable items. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" if self . shared_lambda : self . lambdas = self . lambda_weight . expand ( self . num_nests ) else : self . lambdas = self . lambda_weight # if not self._clamp_called_flag: # warnings.warn('Did you forget to call clamp_lambdas() after optimizer.step()?') # The overall utility of item can be decomposed into V[item] = W[nest] + Y[item] + eps. T = list ( item_x_dict . values ())[ 0 ] . shape [ 0 ] device = list ( item_x_dict . values ())[ 0 ] . device # compute nest-specific utility with shape (T, num_nests). W = torch . zeros ( T , self . num_nests ) . to ( device ) for variable in self . nest_coef_variation_dict . keys (): if self . is_intercept_term ( variable ): nest_x_dict [ 'intercept' ] = torch . ones (( T , self . num_nests , 1 )) . to ( device ) break for variable in self . item_coef_variation_dict . keys (): if self . is_intercept_term ( variable ): item_x_dict [ 'intercept' ] = torch . ones (( T , self . num_items , 1 )) . to ( device ) break for var_type , coef in self . nest_coef_dict . items (): corresponding_observable = var_type . split ( \"[\" )[ 0 ] W += coef ( nest_x_dict [ corresponding_observable ], user_index ) # compute item-specific utility (T, num_items). Y = torch . zeros ( T , self . num_items ) . to ( device ) for var_type , coef in self . item_coef_dict . items (): corresponding_observable = var_type . split ( \"[\" )[ 0 ] Y += coef ( item_x_dict [ corresponding_observable ], user_index ) if item_availability is not None : Y [ ~ item_availability ] = torch . finfo ( Y . dtype ) . min / 2 # ============================================================================= # compute the inclusive value of each nest. inclusive_value = dict () for k , Bk in self . nest_to_item . items (): # for nest k, divide the Y of all items in Bk by lambda_k. Y [:, Bk ] /= self . lambdas [ k ] # compute inclusive value for nest k. # mask out unavilable items. inclusive_value [ k ] = torch . logsumexp ( Y [:, Bk ], dim = 1 , keepdim = False ) # (T,) # boardcast inclusive value from (T, num_nests) to (T, num_items). # for trip t, I[t, i] is the inclusive value of the nest item i belongs to. I = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . nest_to_item . items (): I [:, Bk ] = inclusive_value [ k ] . view ( - 1 , 1 ) # (T, |Bk|) # logP_item[t, i] = log P(ni|Bk), where Bk is the nest item i is in, n is the user in trip t. logP_item = Y - I # (T, num_items) # ============================================================================= # logP_nest[t, i] = log P(Bk), for item i in trip t, the probability of choosing the nest/bucket # item i belongs to. logP_nest has shape (T, num_items) # logit[t, i] = W[n, k] + lambda[k] I[n, k], where n is the user involved in trip t, k is # the nest item i belongs to. logit = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . nest_to_item . items (): logit [:, Bk ] = ( W [:, k ] + self . lambdas [ k ] * inclusive_value [ k ]) . view ( - 1 , 1 ) # (T, |Bk|) # only count each nest once in the logsumexp within the nest level model. cols = [ x [ 0 ] for x in self . nest_to_item . values ()] logP_nest = logit - torch . logsumexp ( logit [:, cols ], dim = 1 , keepdim = True ) # ============================================================================= # compute the joint log P_{ni} as in the textbook. logP = logP_item + logP_nest self . _clamp_called_flag = False return logP def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for name , param in self . named_parameters (): if name == 'lambda_weight' : # we don't regularize the lambda term, we only regularize coefficients. continue nll += self . regularization_weight * torch . norm ( param , p = L ) return nll @property def device ( self ) -> torch . device : \"\"\"Returns the device of the coefficient. Returns: torch.device: the device of the model. \"\"\" return next ( iter ( self . item_coef_dict . values ())) . device @staticmethod def is_intercept_term ( variable : str ): # check if the given variable is an intercept (fixed effect) term. # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc. return ( variable . startswith ( 'intercept[' ) and variable . endswith ( ']' )) def get_coefficient ( self , variable : str , level : Optional [ str ] = None ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" if variable == 'lambda' : return self . lambda_weight . detach () . clone () if level not in [ 'item' , 'nest' ]: raise ValueError ( f \"Level should be either 'item' or 'nest', got { level } .\" ) return self . state_dict ()[ f ' { level } _coef_dict. { variable } .coef' ] . detach () . clone () device : torch . device property Returns the device of the coefficient. Returns: Type Description torch . device torch.device: the device of the model. num_params : int property Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Name Type Description int int the total number of learnable parameters. __init__ ( nest_to_item , nest_coef_variation_dict = None , nest_num_param_dict = None , item_coef_variation_dict = None , item_num_param_dict = None , item_formula = None , nest_formula = None , dataset = None , num_users = None , shared_lambda = False , regularization = None , regularization_weight = None ) Initialization method of the nested logit model. Parameters: Name Type Description Default nest_to_item Dict [ object , List [ int ]] a dictionary maps a nest ID to a list of items IDs of the queried nest. required nest_coef_variation_dict Dict [ str , str ] a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. None nest_num_param_dict Dict [ str , int ] a dictionary maps a variable type name to the number of parameters in this variable group. None item_coef_variation_dict Dict [ str , str ] the same as nest_coef_variation_dict but for item features. None item_num_param_dict Dict [ str , int ] the same as nest_num_param_dict but for item features. None {nest, item}_formula (str a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. required dataset JointDataset a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. None num_users Optional [ int ] number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. None shared_lambda bool a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. False regularization Optional [ str ] this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. None regularization_weight Optional [ float ] the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. None Source code in torch_choice/model/nested_logit_model.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , nest_to_item : Dict [ object , List [ int ]], # method 1: specify variation and num param. dictionary. nest_coef_variation_dict : Optional [ Dict [ str , str ]] = None , nest_num_param_dict : Optional [ Dict [ str , int ]] = None , item_coef_variation_dict : Optional [ Dict [ str , str ]] = None , item_num_param_dict : Optional [ Dict [ str , int ]] = None , # method 2: specify formula and dataset. item_formula : Optional [ str ] = None , nest_formula : Optional [ str ] = None , dataset : Optional [ JointDataset ] = None , num_users : Optional [ int ] = None , shared_lambda : bool = False , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\"Initialization method of the nested logit model. Args: nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list of items IDs of the queried nest. nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item features. {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # handle nest level model. using_formula_to_initiate = ( item_formula is not None ) and ( nest_formula is not None ) if using_formula_to_initiate : # make sure that the research does not specify duplicated information, which might cause conflict. if ( nest_coef_variation_dict is not None ) or ( item_coef_variation_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.' ) if ( nest_num_param_dict is not None ) or ( item_num_param_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.' ) if dataset is None : raise ValueError ( 'Dataset is required if {item, nest}_formula is specified to initiate the model.' ) nest_coef_variation_dict , nest_num_param_dict = parse_formula ( nest_formula , dataset . datasets [ 'nest' ]) item_coef_variation_dict , item_num_param_dict = parse_formula ( item_formula , dataset . datasets [ 'item' ]) else : # check for conflicting information. if ( nest_formula is not None ) or ( item_formula is not None ): raise ValueError ( 'You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.' ) # make sure that the research specifies all the required information. if ( nest_coef_variation_dict is None ) or ( item_coef_variation_dict is None ): raise ValueError ( 'You should specify the {item, nest}_coef_variation_dict to initiate the model.' ) if ( nest_num_param_dict is None ) or ( item_num_param_dict is None ): raise ValueError ( 'You should specify the {item, nest}_num_param_dict to initiate the model.' ) super ( NestedLogitModel , self ) . __init__ () self . nest_to_item = nest_to_item self . nest_coef_variation_dict = nest_coef_variation_dict self . nest_num_param_dict = nest_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . nests = list ( nest_to_item . keys ()) self . num_nests = len ( self . nests ) self . num_items = sum ( len ( items ) for items in nest_to_item . values ()) # nest coefficients. self . nest_coef_dict = self . _build_coef_dict ( self . nest_coef_variation_dict , self . nest_num_param_dict , self . num_nests ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_nests ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) forward ( batch ) An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data batch. required Returns: Type Description torch . Tensor torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability torch . Tensor of choosing item i in trip t. Source code in torch_choice/model/nested_logit_model.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'nest' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) get_coefficient ( variable , level = None ) Retrieve the coefficient tensor for the given variable. Parameters: Name Type Description Default variable str the variable name. required level str from which level of model to extract the coefficient, can be 'item' or 'nest'. The level argument will be discarded if variable is lambda . None Returns: Type Description torch . Tensor torch.Tensor: the corresponding coefficient tensor of the requested variable. Source code in torch_choice/model/nested_logit_model.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_coefficient ( self , variable : str , level : Optional [ str ] = None ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" if variable == 'lambda' : return self . lambda_weight . detach () . clone () if level not in [ 'item' , 'nest' ]: raise ValueError ( f \"Level should be either 'item' or 'nest', got { level } .\" ) return self . state_dict ()[ f ' { level } _coef_dict. { variable } .coef' ] . detach () . clone () log_likelihood ( * args ) Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: Name Type Description _type_ the log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py 319 320 321 322 323 324 325 def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) loss ( * args , ** kwargs ) The loss function to be optimized. This is a wrapper of negative_log_likelihood + regularization loss if required. Source code in torch_choice/model/nested_logit_model.py 355 356 357 358 359 360 361 362 363 364 365 def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for name , param in self . named_parameters (): if name == 'lambda_weight' : # we don't regularize the lambda term, we only regularize coefficients. continue nll += self . regularization_weight * torch . norm ( param , p = L ) return nll negative_log_likelihood ( batch , y , is_train = True ) Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Parameters: Name Type Description Default batch ChoiceDataset the ChoiceDataset object containing the data. required y torch . LongTensor the label. required is_train bool which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, is_train should be set to True. If we merely need a performance metric, then is_train can be set to False for better performance. Defaults to True. True Returns: Type Description torch . scalar_tensor torch.scalar_tensor: the negative log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll","title":"API Reference Torch-Choice"},{"location":"api_torch_choice/#api-reference-torch-choice","text":"Bases: torch . utils . data . Dataset Source code in torch_choice/data/choice_dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 class ChoiceDataset ( torch . utils . data . Dataset ): def __init__ ( self , item_index : torch . LongTensor , num_items : int = None , num_users : int = None , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`. num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) 6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price observable. \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . _num_items = num_items self . _num_users = num_users self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): if self . _is_attribute ( key ): # all observable should be float. item = item . float () setattr ( self , key , item ) # TODO: add a validation procedure to check the consistency of the dataset. def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict ) def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index ) def __contains__ ( self , key : str ) -> bool : return key in self . keys def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag @property def device ( self ) -> str : \"\"\"Returns the device of the dataset. Returns: str: the device of the dataset. \"\"\" for attr in self . __dict__ . values (): if torch . is_tensor ( attr ): return attr . device @property def num_users ( self ) -> int : \"\"\"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: int: the number of users involved in this dataset. \"\"\" if self . _num_users is not None : return self . _num_users elif self . user_index is not None : # infer from the number of unique items in user_index. return len ( torch . unique ( self . user_index )) else : return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_user_attribute(key) or self._is_taste_attribute(key): # return val.shape[0] # return 1 @property def num_items ( self ) -> int : \"\"\"Returns the number of items involved in this dataset. Returns: int: the number of items involved in this dataset. \"\"\" if self . _num_items is not None : # return the _num_items provided in the constructor. return self . _num_items else : # infer the number of items from item_index. return len ( torch . unique ( self . item_index )) # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_item_attribute(key): # return val.shape[0] # elif self._is_taste_attribute(key) or self._is_price_attribute(key): # return val.shape[1] # return 1 @property def num_sessions ( self ) -> int : \"\"\"Returns the number of sessions involved in this dataset. Returns: int: the number of sessions involved in this dataset. \"\"\" return len ( torch . unique ( self . session_index )) # if self.session_index is None: # return 1 # for key, val in self.__dict__.items(): # if torch.is_tensor(val): # if self._is_session_attribute(key) or self._is_price_attribute(key): # return val.shape[0] # return 1 @property def x_dict ( self ) -> Dict [ object , torch . Tensor ]: \"\"\"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values. \"\"\" out = dict () for key , val in self . __dict__ . items (): if self . _is_attribute ( key ): # only include attributes. out [ key ] = self . _expand_tensor ( key , val ) # reshape to (num_sessions, num_items, num_params). return out @classmethod def _from_dict ( cls , dictionary : Dict [ str , torch . tensor ]) -> \"ChoiceDataset\" : \"\"\"Creates an instance of ChoiceDataset from a dictionary of arguments. Args: dictionary (Dict[str, torch.tensor]): a dictionary with keys as argument names and values as arguments. Returns: ChoiceDataset: the created copy of dataset. \"\"\" dataset = cls ( ** dictionary ) for key , item in dictionary . items (): setattr ( dataset , key , item ) return dataset def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary ) def _check_device_consistency ( self ) -> None : \"\"\"Checks if all tensors in this dataset are on the same device. Raises: Exception: an exception is raised if not all tensors are on the same device. \"\"\" # assert all tensors are on the same device. devices = list () for val in self . __dict__ . values (): if torch . is_tensor ( val ): devices . append ( val . device ) if len ( set ( devices )) > 1 : raise Exception ( f 'Found tensors on different devices: { set ( devices ) } .' , 'Use dataset.to() method to align devices.' ) def _size_repr ( self , value : object ) -> List [ int ]: \"\"\"A helper method to get the string-representation of object sizes, this is helpful while constructing the string representation of the dataset. Args: value (object): an object to examine its size. Returns: List[int]: list of integers representing the size of the object, length of the list is equal to dimension of `value`. \"\"\" if torch . is_tensor ( value ): return list ( value . size ()) elif isinstance ( value , int ) or isinstance ( value , float ): return [ 1 ] elif isinstance ( value , list ) or isinstance ( value , tuple ): return [ len ( value )] else : return [] def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" # don't print shapes of internal attributes like _num_users and _num_items. info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items () if not key . startswith ( '_' )] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\" # ================================================================================================================== # methods for checking attribute categories. # ================================================================================================================== @staticmethod def _is_item_attribute ( key : str ) -> bool : return key . startswith ( 'item_' ) and ( key != 'item_availability' ) and ( key != 'item_index' ) @staticmethod def _is_user_attribute ( key : str ) -> bool : return key . startswith ( 'user_' ) and ( key != 'user_index' ) @staticmethod def _is_session_attribute ( key : str ) -> bool : return key . startswith ( 'session_' ) and ( key != 'session_index' ) @staticmethod def _is_taste_attribute ( key : str ) -> bool : return key . startswith ( 'taste_' ) @staticmethod def _is_price_attribute ( key : str ) -> bool : return key . startswith ( 'price_' ) or key . startswith ( 'itemsession_' ) def _is_attribute ( self , key : str ) -> bool : return self . _is_item_attribute ( key ) \\ or self . _is_user_attribute ( key ) \\ or self . _is_session_attribute ( key ) \\ or self . _is_taste_attribute ( key ) \\ or self . _is_price_attribute ( key ) def _expand_tensor ( self , key : str , val : torch . Tensor ) -> torch . Tensor : \"\"\"Expands attribute tensor to (num_sessions, num_items, num_params) shape for prediction tasks, this method won't reshape the tensor at all if the `key` (i.e., name of the tensor) suggests its not an attribute of any kind. Args: key (str): name of the attribute used to determine the raw shape of the tensor. For example, 'item_obs' means the raw tensor is in shape (num_items, num_params). val (torch.Tensor): the attribute tensor to be reshaped. Returns: torch.Tensor: the reshaped tensor with shape (num_sessions, num_items, num_params). \"\"\" if not self . _is_attribute ( key ): print ( f 'Warning: the input key { key } is not an attribute of the dataset, will NOT modify the provided tensor.' ) # don't expand non-attribute tensors, if any. return val num_params = val . shape [ - 1 ] if self . _is_user_attribute ( key ): # user_attribute (num_users, *) out = val [ self . user_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_item_attribute ( key ): # item_attribute (num_items, *) out = val . view ( 1 , self . num_items , num_params ) . expand ( len ( self ), - 1 , - 1 ) elif self . _is_session_attribute ( key ): # session_attribute (num_sessions, *) out = val [ self . session_index , :] . view ( len ( self ), 1 , num_params ) . expand ( - 1 , self . num_items , - 1 ) elif self . _is_taste_attribute ( key ): # taste_attribute (num_users, num_items, *) out = val [ self . user_index , :, :] elif self . _is_price_attribute ( key ): # price_attribute (num_sessions, num_items, *) out = val [ self . session_index , :, :] assert out . shape == ( len ( self ), self . num_items , num_params ) return out @staticmethod def unique ( tensor : torch . Tensor ) -> Tuple [ np . ndarray ]: arr = tensor . cpu () . numpy () unique , counts = np . unique ( arr , return_counts = True ) count_sort_ind = np . argsort ( - counts ) unique = unique [ count_sort_ind ] counts = counts [ count_sort_ind ] return unique , counts def summary ( self ) -> None : \"\"\"A method to summarize the dataset. Returns: str: the string representation of the dataset. \"\"\" summary = [ 'ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .' . format ( self . num_sessions , self . num_items , self . num_users if self . user_index is not None else 'single' , len ( self ))] # summarize users. if self . user_index is not None : unique , counts = self . unique ( self . user_index ) summary . append ( f \"The most frequent user is { unique [ 0 ] } with { counts [ 0 ] } observations; the least frequent user is { unique [ - 1 ] } with { counts [ - 1 ] } observations; on average, there are { counts . astype ( float ) . mean () : .2f } observations per user.\" ) N = len ( unique ) K = min ( 5 , N ) string = f ' { K } most frequent users are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent users are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) # summarize items. unique , counts = self . unique ( self . item_index ) N = len ( unique ) K = min ( 5 , N ) summary . append ( f \"The most frequent item is { unique [ 0 ] } , it was chosen { counts [ 0 ] } times; the least frequent item is { unique [ - 1 ] } it was { counts [ - 1 ] } times; on average, each item was purchased { counts . astype ( float ) . mean () : .2f } times.\" ) string = f ' { K } most frequent items are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent items are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) summary . append ( 'Attribute Summaries:' ) for key , item in self . __dict__ . items (): if self . _is_attribute ( key ) and torch . is_tensor ( item ): summary . append ( \"Observable Tensor ' {} ' with shape {} \" . format ( key , item . shape )) # price attributes are 3-dimensional tensors, ignore for cleanness here. if not self . _is_price_attribute ( key ): summary . append ( str ( pd . DataFrame ( item . to ( 'cpu' ) . float () . numpy ()) . describe ())) print ( ' \\n ' . join ( summary ) + f \" \\n device= { self . device } \" ) return None","title":"API Reference: Torch Choice"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.device","text":"Returns the device of the dataset. Returns: Name Type Description str str the device of the dataset.","title":"device"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_items","text":"Returns the number of items involved in this dataset. Returns: Name Type Description int int the number of items involved in this dataset.","title":"num_items"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_sessions","text":"Returns the number of sessions involved in this dataset. Returns: Name Type Description int int the number of sessions involved in this dataset.","title":"num_sessions"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_users","text":"Returns number of users involved in this dataset, returns 1 if there is no user identity. Returns: Name Type Description int int the number of users involved in this dataset.","title":"num_users"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.x_dict","text":"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format. Returns: Type Description Dict [ object , torch . Tensor ] Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute tensors as values.","title":"x_dict"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__eq__","text":"Returns whether all tensor attributes of both ChoiceDatasets are equal. Source code in torch_choice/data/choice_dataset.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def __eq__ ( self , other : \"ChoiceDataset\" ) -> bool : \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\" if not isinstance ( other , ChoiceDataset ): raise TypeError ( 'You can only compare with ChoiceDataset objects.' ) else : flag = True for key , val in self . __dict__ . items (): if torch . is_tensor ( val ): # ignore NaNs while comparing. if not torch . equal ( torch . nan_to_num ( val ), torch . nan_to_num ( other . __dict__ [ key ])): print ( 'Attribute {} is not equal.' . format ( key )) flag = False return flag","title":"__eq__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__getitem__","text":"Retrieves samples corresponding to the provided index or list of indices. Parameters: Name Type Description Default indices Union [ int , torch . LongTensor ] a single integer index or a tensor of indices. required Returns: Name Type Description ChoiceDataset ChoiceDataset a subset of the dataset. Source code in torch_choice/data/choice_dataset.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> \"ChoiceDataset\" : \"\"\"Retrieves samples corresponding to the provided index or list of indices. Args: indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices. Returns: ChoiceDataset: a subset of the dataset. \"\"\" if isinstance ( indices , int ): # convert single integer index to an array of indices. indices = torch . LongTensor ([ indices ]) new_dict = dict () new_dict [ 'item_index' ] = self . item_index [ indices ] . clone () # copy optional attributes. new_dict [ 'label' ] = self . label [ indices ] . clone () if self . label is not None else None new_dict [ 'user_index' ] = self . user_index [ indices ] . clone () if self . user_index is not None else None new_dict [ 'session_index' ] = self . session_index [ indices ] . clone () if self . session_index is not None else None # item_availability has shape (num_sessions, num_items), no need to re-index it. new_dict [ 'item_availability' ] = self . item_availability # copy other attributes. for key , val in self . __dict__ . items (): if key not in new_dict . keys (): if torch . is_tensor ( val ): new_dict [ key ] = val . clone () else : new_dict [ key ] = copy . deepcopy ( val ) return self . _from_dict ( new_dict )","title":"__getitem__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__init__","text":"Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called batch_size in the documentation. The batch_size corresponds to the file length in wide-format dataset, and often denoted using N . We call it batch_size to follow the convention in machine learning literature. A choice instance is a row of the dataset, so there are batch_size choice instances in each ChoiceDataset . The dataset consists of: (1) a collection of batch_size tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of observables associated with item, user, session, etc. Parameters: Name Type Description Default item_index torch . LongTensor a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the label tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. required num_items Optional [ int ] the number of items in the dataset. If None is provided (default), the number of items will be inferred from the number of unique numbers in item_index . None num_users Optional [ int ] the number of users in the dataset. If None is provided (default), the number of users will be inferred from the number of unique numbers in user_index . None label Optional [ torch . LongTensor ] a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the label argument as None in the initialization method, and the model will use item_index as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. None user_index Optional [ torch . LongTensor ] a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If None user index is provided, it's assumed that the choice instances are from the same user. user_index is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. None session_index Optional [ torch . LongTensor ] a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as None . In this case, the ChoiceDataset object will assume each choice instance to be in its own session. Defaults to None. None item_availability Optional [ torch . BoolTensor ] A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. None Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, ) 2. item observables must start with 'item_' and have shape (num_items, ) 3. session observables must start with 'session_' and have shape (num_sessions, ) 4. taste observables (those vary by user and item) must start with taste_ and have shape (num_users, num_items, ). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with price_ and have shape (num_sessions, num_items, *) 6. itemsession observables starting with itemsession_ , this is a more intuitive alias to the price observable. Source code in torch_choice/data/choice_dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , item_index : torch . LongTensor , num_items : int = None , num_users : int = None , label : Optional [ torch . LongTensor ] = None , user_index : Optional [ torch . LongTensor ] = None , session_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None , ** kwargs ) -> None : \"\"\" Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method. The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention in machine learning literature. A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`. The dataset consists of: (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of `observables` associated with item, user, session, etc. Args: item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label. num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`. num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`. label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the `label` argument as `None` in the initialization method, and the model will use `item_index` as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None. user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed that the choice instances are from the same user. `user_index` is required if and only if there are multiple users in the dataset, for example: (1) user-observables is involved in the utility form, (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None. session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset` object will assume each choice instance to be in its own session. Defaults to None. item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None. Other Kwargs (Observables): One can specify the following types of observables, where * in shape denotes any positive integer. Typically * represents the number of observables. Please refer to the documentation for a detailed guide to use observables. 1. user observables must start with 'user_' and have shape (num_users, *) 2. item observables must start with 'item_' and have shape (num_items, *) 3. session observables must start with 'session_' and have shape (num_sessions, *) 4. taste observables (those vary by user and item) must start with `taste_` and have shape (num_users, num_items, *). NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large. 5. price observables (those vary by session and item) must start with `price_` and have shape (num_sessions, num_items, *) 6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price observable. \"\"\" # ENHANCEMENT(Tianyu): add item_names for summary. super ( ChoiceDataset , self ) . __init__ () self . label = label self . item_index = item_index self . _num_items = num_items self . _num_users = num_users self . user_index = user_index self . session_index = session_index if self . session_index is None : # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]): # if any session sensitive observable is provided, but session index is not, # infer each row in the dataset to be a session. # TODO: (design choice) should we assign unique session index to each choice instance or the same session index. print ( 'No `session_index` is provided, assume each choice instance is in its own session.' ) self . session_index = torch . arange ( len ( self . item_index )) . long () self . item_availability = item_availability for key , item in kwargs . items (): if self . _is_attribute ( key ): # all observable should be float. item = item . float () setattr ( self , key , item )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__len__","text":"Returns number of samples in this dataset. Returns: Name Type Description int int length of the dataset. Source code in torch_choice/data/choice_dataset.py 153 154 155 156 157 158 159 def __len__ ( self ) -> int : \"\"\"Returns number of samples in this dataset. Returns: int: length of the dataset. \"\"\" return len ( self . item_index )","title":"__len__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__repr__","text":"A method to get a string representation of the dataset. Returns: Name Type Description str str the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py 358 359 360 361 362 363 364 365 366 def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" # don't print shapes of internal attributes like _num_users and _num_items. info = [ f ' { key } = { self . _size_repr ( item ) } ' for key , item in self . __dict__ . items () if not key . startswith ( '_' )] return f \" { self . __class__ . __name__ } ( { ', ' . join ( info ) } , device= { self . device } )\"","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.apply_tensor","text":"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Parameters: Name Type Description Default func callable a callable function to be applied on tensors and tensor-values of dictionaries. required Returns: Name Type Description ChoiceDataset ChoiceDataset the modified dataset. Source code in torch_choice/data/choice_dataset.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def apply_tensor ( self , func : callable ) -> \"ChoiceDataset\" : \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries. Args: func (callable): a callable function to be applied on tensors and tensor-values of dictionaries. Returns: ChoiceDataset: the modified dataset. \"\"\" for key , item in self . __dict__ . items (): if torch . is_tensor ( item ): setattr ( self , key , func ( item )) # boardcast func to dictionary of tensors as well. elif isinstance ( getattr ( self , key ), dict ): for obj_key , obj_item in getattr ( self , key ) . items (): if torch . is_tensor ( obj_item ): setattr ( getattr ( self , key ), obj_key , func ( obj_item )) return self","title":"apply_tensor()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.clone","text":"Creates a copy of self. Returns: Name Type Description ChoiceDataset ChoiceDataset a copy of self. Source code in torch_choice/data/choice_dataset.py 310 311 312 313 314 315 316 317 318 319 320 321 322 def clone ( self ) -> \"ChoiceDataset\" : \"\"\"Creates a copy of self. Returns: ChoiceDataset: a copy of self. \"\"\" dictionary = {} for k , v in self . __dict__ . items (): if torch . is_tensor ( v ): dictionary [ k ] = v . clone () else : dictionary [ k ] = copy . deepcopy ( v ) return self . __class__ . _from_dict ( dictionary )","title":"clone()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.summary","text":"A method to summarize the dataset. Returns: Name Type Description str None the string representation of the dataset. Source code in torch_choice/data/choice_dataset.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def summary ( self ) -> None : \"\"\"A method to summarize the dataset. Returns: str: the string representation of the dataset. \"\"\" summary = [ 'ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .' . format ( self . num_sessions , self . num_items , self . num_users if self . user_index is not None else 'single' , len ( self ))] # summarize users. if self . user_index is not None : unique , counts = self . unique ( self . user_index ) summary . append ( f \"The most frequent user is { unique [ 0 ] } with { counts [ 0 ] } observations; the least frequent user is { unique [ - 1 ] } with { counts [ - 1 ] } observations; on average, there are { counts . astype ( float ) . mean () : .2f } observations per user.\" ) N = len ( unique ) K = min ( 5 , N ) string = f ' { K } most frequent users are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent users are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) # summarize items. unique , counts = self . unique ( self . item_index ) N = len ( unique ) K = min ( 5 , N ) summary . append ( f \"The most frequent item is { unique [ 0 ] } , it was chosen { counts [ 0 ] } times; the least frequent item is { unique [ - 1 ] } it was { counts [ - 1 ] } times; on average, each item was purchased { counts . astype ( float ) . mean () : .2f } times.\" ) string = f ' { K } most frequent items are: ' + ', ' . join ([ f ' { unique [ i ] } ( { counts [ i ] } times)' for i in range ( K )]) + '.' summary . append ( string ) string = f ' { K } least frequent items are: ' + ', ' . join ([ f ' { unique [ N - i ] } ( { counts [ N - i ] } times)' for i in range ( 1 , K + 1 )]) + '.' summary . append ( string ) summary . append ( 'Attribute Summaries:' ) for key , item in self . __dict__ . items (): if self . _is_attribute ( key ) and torch . is_tensor ( item ): summary . append ( \"Observable Tensor ' {} ' with shape {} \" . format ( key , item . shape )) # price attributes are 3-dimensional tensors, ignore for cleanness here. if not self . _is_price_attribute ( key ): summary . append ( str ( pd . DataFrame ( item . to ( 'cpu' ) . float () . numpy ()) . describe ())) print ( ' \\n ' . join ( summary ) + f \" \\n device= { self . device } \" ) return None","title":"summary()"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.to","text":"Moves all tensors in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union [ str , torch . device ] the destination device. required Returns: Name Type Description ChoiceDataset ChoiceDataset the modified dataset on the new device. Source code in torch_choice/data/choice_dataset.py 299 300 301 302 303 304 305 306 307 308 def to ( self , device : Union [ str , torch . device ]) -> \"ChoiceDataset\" : \"\"\"Moves all tensors in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" return self . apply_tensor ( lambda x : x . to ( device )) Bases: torch . utils . data . Dataset A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. Source code in torch_choice/data/joint_dataset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class JointDataset ( torch . utils . data . Dataset ): \"\"\"A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets. The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class. \"\"\" def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d ) def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ()) def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out ) @property def device ( self ) -> str : \"\"\"Returns the device of datasets contained in the joint dataset. Returns: str: the device of the dataset. \"\"\" for d in self . datasets . values (): return d . device def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self","title":"to()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.device","text":"Returns the device of datasets contained in the joint dataset. Returns: Name Type Description str str the device of the dataset.","title":"device"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__getitem__","text":"Queries samples from the dataset by index. Parameters: Name Type Description Default indices Union [ int , torch . LongTensor ] an integer or a 1D tensor of multiple indices. required Returns: Type Description Dict [ str , ChoiceDataset ] Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the datasets argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. Source code in torch_choice/data/joint_dataset.py 55 56 57 58 59 60 61 62 63 64 65 66 def __getitem__ ( self , indices : Union [ int , torch . LongTensor ]) -> Dict [ str , ChoiceDataset ]: \"\"\"Queries samples from the dataset by index. Args: indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices. Returns: Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets of contained datasets, sliced using the provided indices. \"\"\" return dict (( name , d [ indices ]) for ( name , d ) in self . datasets . items ())","title":"__getitem__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__init__","text":"The initialize methods. Source code in torch_choice/data/joint_dataset.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , ** datasets ) -> None : \"\"\"The initialize methods. Args: Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct ``` dataset = JointDataset(food=FoodDataset, drink=DrinkDataset) ``` All datasets should have the same length. \"\"\" super ( JointDataset , self ) . __init__ () self . datasets = datasets # check the length of sub-datasets are the same. assert len ( set ([ len ( d ) for d in self . datasets . values ()])) == 1","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__len__","text":"Get the number of samples in the joint dataset. Returns: Name Type Description int int the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. Source code in torch_choice/data/joint_dataset.py 46 47 48 49 50 51 52 53 def __len__ ( self ) -> int : \"\"\"Get the number of samples in the joint dataset. Returns: int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained. \"\"\" for d in self . datasets . values (): return len ( d )","title":"__len__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__repr__","text":"A method to get a string representation of the dataset. Returns: Name Type Description str str the string representation of the dataset. Source code in torch_choice/data/joint_dataset.py 68 69 70 71 72 73 74 75 76 77 78 def __repr__ ( self ) -> str : \"\"\"A method to get a string representation of the dataset. Returns: str: the string representation of the dataset. \"\"\" out = [ f 'JointDataset with { len ( self . datasets ) } sub-datasets: (' ] for name , dataset in self . datasets . items (): out . append ( f ' \\t { name } : { str ( dataset ) } ' ) out . append ( ')' ) return ' \\n ' . join ( out )","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.to","text":"Moves all datasets in this dataset to the specified PyTorch device. Parameters: Name Type Description Default device Union [ str , torch . device ] the destination device. required Returns: Name Type Description ChoiceDataset JointDataset the modified dataset on the new device. Source code in torch_choice/data/joint_dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 def to ( self , device : Union [ str , torch . device ]) -> \"JointDataset\" : \"\"\"Moves all datasets in this dataset to the specified PyTorch device. Args: device (Union[str, torch.device]): the destination device. Returns: ChoiceDataset: the modified dataset on the new device. \"\"\" for d in self . datasets . values (): d = d . to ( device ) return self Bases: nn . Module The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: unless the -full flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. constant: constant over all users and items, user: user-specific parameters but constant across all items, item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. item-full: item-specific parameters but constant across all users, explicitly model for all items. user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full: parameters that are specific to both user and item, explicitly model for all items. Source code in torch_choice/model/conditional_logit_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 class ConditionalLogitModel ( nn . Module ): \"\"\"The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient. The model allows for the following levels for variable variations: NOTE: unless the `-full` flag is specified (which means we want to explicitly model coefficients for all items), for all variation levels related to item (item specific and user-item specific), the model force coefficients for the first item to be zero. This design follows standard econometric practice. - constant: constant over all users and items, - user: user-specific parameters but constant across all items, - item: item-specific parameters but constant across all users, parameters for the first item are forced to be zero. - item-full: item-specific parameters but constant across all users, explicitly model for all items. - user-item: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - user-item-full: parameters that are specific to both user and item, explicitly model for all items. \"\"\" def __init__ ( self , formula : Optional [ str ] = None , dataset : Optional [ ChoiceDataset ] = None , coef_variation_dict : Optional [ Dict [ str , str ]] = None , num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\" Args: formula (str): a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # ============================================================================================================== # Check that the model received a valid combination of inputs so that it can be initialized. # ============================================================================================================== if coef_variation_dict is None and formula is None : raise ValueError ( \"Either coef_variation_dict or formula should be provided to specify the model.\" ) if ( coef_variation_dict is not None ) and ( formula is not None ): raise ValueError ( \"Only one of coef_variation_dict or formula should be provided to specify the model.\" ) if ( formula is not None ) and ( dataset is None ): raise ValueError ( \"If formula is provided, data should be provided to specify the model.\" ) # ============================================================================================================== # Build necessary dictionaries for model initialization. # ============================================================================================================== if formula is None : # Use dictionaries to initialize the model. if num_param_dict is None : warnings . warn ( \"`num_param_dict` is not provided, all variables will be treated as having one parameter.\" ) num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () # variable `var` with variation `spec` to variable `var[spec]`. rename = dict () # old variable name --> new variable name. for variable , specificity in coef_variation_dict . items (): rename [ variable ] = f \" { variable } [ { specificity } ]\" for old_name , new_name in rename . items (): coef_variation_dict [ new_name ] = coef_variation_dict . pop ( old_name ) num_param_dict [ new_name ] = num_param_dict . pop ( old_name ) else : # Use the formula to infer model. coef_variation_dict , num_param_dict = parse_formula ( formula , dataset ) # ============================================================================================================== # Model Initialization. # ============================================================================================================== super ( ConditionalLogitModel , self ) . __init__ () self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ) and variable not in self . num_param_dict . keys (): warnings . warn ( f \"` { variable } ` key found in coef_variation_dict but not in num_param_dict, num_param_dict[' { variable } '] has been set to 1.\" ) self . num_param_dict [ variable ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict ) def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) + ' \\n ' + f 'device= { self . device } ' @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ): # intercept term has no input tensor from the ChoiceDataset data structure. # the tensor for intercept has only 1 feature, every entry is 1. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) break # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name. corresponding_observable = var_type . split ( \"[\" )[ 0 ] total_utility += coef ( x_dict [ corresponding_observable ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavailable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for param in self . parameters (): nll += self . regularization_weight * torch . norm ( param , p = L ) return nll @property def device ( self ) -> torch . device : \"\"\"Returns the device of the coefficient. Returns: torch.device: the device of the model. \"\"\" return next ( iter ( self . coef_dict . values ())) . device @staticmethod def is_intercept_term ( variable : str ): # check if the given variable is an intercept (fixed effect) term. # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc. return ( variable . startswith ( 'intercept[' ) and variable . endswith ( ']' )) def get_coefficient ( self , variable : str ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" return self . state_dict ()[ f \"coef_dict. { variable } .coef\" ] . detach () . clone ()","title":"to()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.device","text":"Returns the device of the coefficient. Returns: Type Description torch . device torch.device: the device of the model.","title":"device"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.num_params","text":"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Name Type Description int int the total number of learnable parameters.","title":"num_params"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__init__","text":"Parameters: Name Type Description Default formula str a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. None data ChoiceDataset a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. required coef_variation_dict Dict [ str , str ] variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with itemsession_ , price_ , user_ , etc), or intercept if the researcher requires an intercept term. For each variable name X_var (e.g., user_income ) or intercept , the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. constant : the coefficient constant over all users and items: \\(X \beta\\) . user : user-specific parameters but constant across all items: \\(X \beta_{u}\\) . item : item-specific parameters but constant across all users, \\(X \beta_{i}\\) . Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. item-full : the same configuration as item , but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - user-item : parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. user-item-full : parameters that are specific to both user and item, explicitly model for all items. None num_param_dict Optional [ Dict [ str , int ]] variable type to number of parameters dictionary with keys exactly the same as the coef_variation_dict . Values of num_param_dict records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the coef_variation_dict dictionary and values of all ones. Default to be None. None num_items int number of items in the dataset. None num_users int number of users in the dataset. None regularization Optional [ str ] this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. None regularization_weight Optional [ float ] the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. None Source code in torch_choice/model/conditional_logit_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , formula : Optional [ str ] = None , dataset : Optional [ ChoiceDataset ] = None , coef_variation_dict : Optional [ Dict [ str , str ]] = None , num_param_dict : Optional [ Dict [ str , int ]] = None , num_items : Optional [ int ] = None , num_users : Optional [ int ] = None , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\" Args: formula (str): a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset. coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept` if the researcher requires an intercept term. For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient. - `constant`: the coefficient constant over all users and items: $X \\beta$. - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$. - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$. Note that the coefficients for the first item are forced to be zero following the standard practice in econometrics. - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to be zeros. The following configurations are supported by the package, but we don't recommend using them due to the large number of parameters. - `user-item`: parameters that are specific to both user and item, parameter for the first item for all users are forced to be zero. - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items. num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary and values of all ones. Default to be None. num_items (int): number of items in the dataset. num_users (int): number of users in the dataset. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # ============================================================================================================== # Check that the model received a valid combination of inputs so that it can be initialized. # ============================================================================================================== if coef_variation_dict is None and formula is None : raise ValueError ( \"Either coef_variation_dict or formula should be provided to specify the model.\" ) if ( coef_variation_dict is not None ) and ( formula is not None ): raise ValueError ( \"Only one of coef_variation_dict or formula should be provided to specify the model.\" ) if ( formula is not None ) and ( dataset is None ): raise ValueError ( \"If formula is provided, data should be provided to specify the model.\" ) # ============================================================================================================== # Build necessary dictionaries for model initialization. # ============================================================================================================== if formula is None : # Use dictionaries to initialize the model. if num_param_dict is None : warnings . warn ( \"`num_param_dict` is not provided, all variables will be treated as having one parameter.\" ) num_param_dict = { key : 1 for key in coef_variation_dict . keys ()} assert coef_variation_dict . keys () == num_param_dict . keys () # variable `var` with variation `spec` to variable `var[spec]`. rename = dict () # old variable name --> new variable name. for variable , specificity in coef_variation_dict . items (): rename [ variable ] = f \" { variable } [ { specificity } ]\" for old_name , new_name in rename . items (): coef_variation_dict [ new_name ] = coef_variation_dict . pop ( old_name ) num_param_dict [ new_name ] = num_param_dict . pop ( old_name ) else : # Use the formula to infer model. coef_variation_dict , num_param_dict = parse_formula ( formula , dataset ) # ============================================================================================================== # Model Initialization. # ============================================================================================================== super ( ConditionalLogitModel , self ) . __init__ () self . coef_variation_dict = deepcopy ( coef_variation_dict ) self . num_param_dict = deepcopy ( num_param_dict ) self . num_items = num_items self . num_users = num_users self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) # check number of parameters specified are all positive. for var_type , num_params in self . num_param_dict . items (): assert num_params > 0 , f 'num_params needs to be positive, got: { num_params } .' # infer the number of parameters for intercept if the researcher forgets. for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ) and variable not in self . num_param_dict . keys (): warnings . warn ( f \"` { variable } ` key found in coef_variation_dict but not in num_param_dict, num_param_dict[' { variable } '] has been set to 1.\" ) self . num_param_dict [ variable ] = 1 # construct trainable parameters. coef_dict = dict () for var_type , variation in self . coef_variation_dict . items (): coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = self . num_items , num_users = self . num_users , num_params = self . num_param_dict [ var_type ]) # A ModuleDict is required to properly register all trainable parameters. # self.parameter() will fail if a python dictionary is used instead. self . coef_dict = nn . ModuleDict ( coef_dict )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__repr__","text":"Return a string representation of the model. Returns: Name Type Description str str the string representation of the model. Source code in torch_choice/model/conditional_logit_model.py 180 181 182 183 184 185 186 187 188 189 def __repr__ ( self ) -> str : \"\"\"Return a string representation of the model. Returns: str: the string representation of the model. \"\"\" out_str_lst = [ 'Conditional logistic discrete choice model, expects input features: \\n ' ] for var_type , num_params in self . num_param_dict . items (): out_str_lst . append ( f 'X[ { var_type } ] with { num_params } parameters, with { self . coef_variation_dict [ var_type ] } level variation.' ) return super () . __repr__ () + ' \\n ' + ' \\n ' . join ( out_str_lst ) + ' \\n ' + f 'device= { self . device } '","title":"__repr__()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.forward","text":"Forward pass of the model. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object. required manual_coef_value_dict Optional [ Dict [ str , torch . Tensor ]] a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. None Returns: Type Description torch . Tensor torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. Source code in torch_choice/model/conditional_logit_model.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def forward ( self , batch : ChoiceDataset , manual_coef_value_dict : Optional [ Dict [ str , torch . Tensor ]] = None ) -> torch . Tensor : \"\"\" Forward pass of the model. Args: batch: a `ChoiceDataset` object. manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents the utility from item i in trip t for the user involved in that trip. \"\"\" x_dict = batch . x_dict for variable in self . coef_variation_dict . keys (): if self . is_intercept_term ( variable ): # intercept term has no input tensor from the ChoiceDataset data structure. # the tensor for intercept has only 1 feature, every entry is 1. x_dict [ 'intercept' ] = torch . ones (( len ( batch ), self . num_items , 1 ), device = batch . device ) break # compute the utility from each item in each choice session. total_utility = torch . zeros (( len ( batch ), self . num_items ), device = batch . device ) # for each type of variables, apply the corresponding coefficient to input x. for var_type , coef in self . coef_dict . items (): # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name. corresponding_observable = var_type . split ( \"[\" )[ 0 ] total_utility += coef ( x_dict [ corresponding_observable ], batch . user_index , manual_coef_value = None if manual_coef_value_dict is None else manual_coef_value_dict [ var_type ]) assert total_utility . shape == ( len ( batch ), self . num_items ) if batch . item_availability is not None : # mask out unavailable items. total_utility [ ~ batch . item_availability [ batch . session_index , :]] = torch . finfo ( total_utility . dtype ) . min / 2 return total_utility","title":"forward()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.get_coefficient","text":"Retrieve the coefficient tensor for the given variable. Parameters: Name Type Description Default variable str the variable name. required Returns: Type Description torch . Tensor torch.Tensor: the corresponding coefficient tensor of the requested variable. Source code in torch_choice/model/conditional_logit_model.py 307 308 309 310 311 312 313 314 315 316 def get_coefficient ( self , variable : str ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" return self . state_dict ()[ f \"coef_dict. { variable } .coef\" ] . detach () . clone ()","title":"get_coefficient()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.loss","text":"The loss function to be optimized. This is a wrapper of negative_log_likelihood + regularization loss if required. Source code in torch_choice/model/conditional_logit_model.py 283 284 285 286 287 288 289 290 def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for param in self . parameters (): nll += self . regularization_weight * torch . norm ( param , p = L ) return nll","title":"loss()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.negative_log_likelihood","text":"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data. required y torch . Tensor the label. required is_train bool whether to trace the gradient. Defaults to True. True Returns: Type Description torch . Tensor torch.Tensor: the negative log-likelihood. Source code in torch_choice/model/conditional_logit_model.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . Tensor , is_train : bool = True ) -> torch . Tensor : \"\"\"Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility. Args: batch (ChoiceDataset): a ChoiceDataset object containing the data. y (torch.Tensor): the label. is_train (bool, optional): whether to trace the gradient. Defaults to True. Returns: torch.Tensor: the negative log-likelihood. \"\"\" if is_train : self . train () else : self . eval () # (num_trips, num_items) total_utility = self . forward ( batch ) logP = torch . log_softmax ( total_utility , dim = 1 ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll","title":"negative_log_likelihood()"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.summary","text":"Print out the current model parameter. Source code in torch_choice/model/conditional_logit_model.py 202 203 204 205 206 207 def summary ( self ): \"\"\"Print out the current model parameter.\"\"\" for var_type , coefficient in self . coef_dict . items (): if coefficient is not None : print ( 'Variable Type: ' , var_type ) print ( coefficient . coef ) Bases: nn . Module Source code in torch_choice/model/nested_logit_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class NestedLogitModel ( nn . Module ): def __init__ ( self , nest_to_item : Dict [ object , List [ int ]], # method 1: specify variation and num param. dictionary. nest_coef_variation_dict : Optional [ Dict [ str , str ]] = None , nest_num_param_dict : Optional [ Dict [ str , int ]] = None , item_coef_variation_dict : Optional [ Dict [ str , str ]] = None , item_num_param_dict : Optional [ Dict [ str , int ]] = None , # method 2: specify formula and dataset. item_formula : Optional [ str ] = None , nest_formula : Optional [ str ] = None , dataset : Optional [ JointDataset ] = None , num_users : Optional [ int ] = None , shared_lambda : bool = False , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\"Initialization method of the nested logit model. Args: nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list of items IDs of the queried nest. nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item features. {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # handle nest level model. using_formula_to_initiate = ( item_formula is not None ) and ( nest_formula is not None ) if using_formula_to_initiate : # make sure that the research does not specify duplicated information, which might cause conflict. if ( nest_coef_variation_dict is not None ) or ( item_coef_variation_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.' ) if ( nest_num_param_dict is not None ) or ( item_num_param_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.' ) if dataset is None : raise ValueError ( 'Dataset is required if {item, nest}_formula is specified to initiate the model.' ) nest_coef_variation_dict , nest_num_param_dict = parse_formula ( nest_formula , dataset . datasets [ 'nest' ]) item_coef_variation_dict , item_num_param_dict = parse_formula ( item_formula , dataset . datasets [ 'item' ]) else : # check for conflicting information. if ( nest_formula is not None ) or ( item_formula is not None ): raise ValueError ( 'You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.' ) # make sure that the research specifies all the required information. if ( nest_coef_variation_dict is None ) or ( item_coef_variation_dict is None ): raise ValueError ( 'You should specify the {item, nest}_coef_variation_dict to initiate the model.' ) if ( nest_num_param_dict is None ) or ( item_num_param_dict is None ): raise ValueError ( 'You should specify the {item, nest}_num_param_dict to initiate the model.' ) super ( NestedLogitModel , self ) . __init__ () self . nest_to_item = nest_to_item self . nest_coef_variation_dict = nest_coef_variation_dict self . nest_num_param_dict = nest_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . nests = list ( nest_to_item . keys ()) self . num_nests = len ( self . nests ) self . num_items = sum ( len ( items ) for items in nest_to_item . values ()) # nest coefficients. self . nest_coef_dict = self . _build_coef_dict ( self . nest_coef_variation_dict , self . nest_num_param_dict , self . num_nests ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_nests ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' ) @property def num_params ( self ) -> int : \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: int: the total number of learnable parameters. \"\"\" return sum ( w . numel () for w in self . parameters ()) def _build_coef_dict ( self , coef_variation_dict : Dict [ str , str ], num_param_dict : Dict [ str , int ], num_items : int ) -> nn . ModuleDict : \"\"\"Builds a coefficient dictionary containing all trainable components of the model, mapping coefficient names to the corresponding Coefficient Module. num_items could be the actual number of items or the number of nests depends on the use case. NOTE: torch-choice users don't directly interact with this method. Args: coef_variation_dict (Dict[str, str]): a dictionary mapping coefficient names (e.g., theta_user) to the level of variation (e.g., 'user'). num_param_dict (Dict[str, int]): a dictionary mapping coefficient names to the number of parameters in this coefficient. Be aware that, for example, if there is one K-dimensional coefficient for every user, then the `num_param` should be K instead of K x number of users. num_items (int): the total number of items in the prediction problem. `num_items` should be the number of nests if _build_coef_dict() is used for nest-level prediction. Returns: nn.ModuleDict: a PyTorch ModuleDict object mapping from coefficient names to training Coefficient. \"\"\" coef_dict = dict () for var_type , variation in coef_variation_dict . items (): num_params = num_param_dict [ var_type ] coef_dict [ var_type ] = Coefficient ( variation = variation , num_items = num_items , num_users = self . num_users , num_params = num_params ) return nn . ModuleDict ( coef_dict ) def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'nest' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability ) def _forward ( self , nest_x_dict : Dict [ str , torch . Tensor ], item_x_dict : Dict [ str , torch . Tensor ], user_index : Optional [ torch . LongTensor ] = None , item_availability : Optional [ torch . BoolTensor ] = None ) -> torch . Tensor : \"\"\"\"Computes log P[t, i] = the log probability for the user involved in trip t to choose item i. Let n denote the ID of the user involved in trip t, then P[t, i] = P_{ni} on page 86 of the book \"discrete choice methods with simulation\" by Train. The `_forward` method is an internal API, users should refer to the `forward` method. Args: nest_x_dict (torch.Tensor): a dictionary mapping from nest-level feature names to the corresponding feature tensor. item_x_dict (torch.Tensor): a dictionary mapping from item-level feature names to the corresponding feature tensor. More details on the shape of the tensors can be found in the docstring of the `x_dict` method of `ChoiceDataset`. user_index (torch.LongTensor): a tensor of shape (num_trips,) indicating which user is making decision in each trip. Setting user_index = None assumes the same user is making decisions in all trips. item_availability (torch.BoolTensor): a boolean tensor with shape (num_trips, num_items) indicating the aviliability of items in each trip. If item_availability[t, i] = False, the utility of choosing item i in trip t, V[t, i], will be set to -inf. Given the decomposition V[t, i] = W[t, k(i)] + Y[t, i] + eps, V[t, i] is set to -inf by setting Y[t, i] = -inf for unavilable items. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" if self . shared_lambda : self . lambdas = self . lambda_weight . expand ( self . num_nests ) else : self . lambdas = self . lambda_weight # if not self._clamp_called_flag: # warnings.warn('Did you forget to call clamp_lambdas() after optimizer.step()?') # The overall utility of item can be decomposed into V[item] = W[nest] + Y[item] + eps. T = list ( item_x_dict . values ())[ 0 ] . shape [ 0 ] device = list ( item_x_dict . values ())[ 0 ] . device # compute nest-specific utility with shape (T, num_nests). W = torch . zeros ( T , self . num_nests ) . to ( device ) for variable in self . nest_coef_variation_dict . keys (): if self . is_intercept_term ( variable ): nest_x_dict [ 'intercept' ] = torch . ones (( T , self . num_nests , 1 )) . to ( device ) break for variable in self . item_coef_variation_dict . keys (): if self . is_intercept_term ( variable ): item_x_dict [ 'intercept' ] = torch . ones (( T , self . num_items , 1 )) . to ( device ) break for var_type , coef in self . nest_coef_dict . items (): corresponding_observable = var_type . split ( \"[\" )[ 0 ] W += coef ( nest_x_dict [ corresponding_observable ], user_index ) # compute item-specific utility (T, num_items). Y = torch . zeros ( T , self . num_items ) . to ( device ) for var_type , coef in self . item_coef_dict . items (): corresponding_observable = var_type . split ( \"[\" )[ 0 ] Y += coef ( item_x_dict [ corresponding_observable ], user_index ) if item_availability is not None : Y [ ~ item_availability ] = torch . finfo ( Y . dtype ) . min / 2 # ============================================================================= # compute the inclusive value of each nest. inclusive_value = dict () for k , Bk in self . nest_to_item . items (): # for nest k, divide the Y of all items in Bk by lambda_k. Y [:, Bk ] /= self . lambdas [ k ] # compute inclusive value for nest k. # mask out unavilable items. inclusive_value [ k ] = torch . logsumexp ( Y [:, Bk ], dim = 1 , keepdim = False ) # (T,) # boardcast inclusive value from (T, num_nests) to (T, num_items). # for trip t, I[t, i] is the inclusive value of the nest item i belongs to. I = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . nest_to_item . items (): I [:, Bk ] = inclusive_value [ k ] . view ( - 1 , 1 ) # (T, |Bk|) # logP_item[t, i] = log P(ni|Bk), where Bk is the nest item i is in, n is the user in trip t. logP_item = Y - I # (T, num_items) # ============================================================================= # logP_nest[t, i] = log P(Bk), for item i in trip t, the probability of choosing the nest/bucket # item i belongs to. logP_nest has shape (T, num_items) # logit[t, i] = W[n, k] + lambda[k] I[n, k], where n is the user involved in trip t, k is # the nest item i belongs to. logit = torch . zeros ( T , self . num_items ) . to ( device ) for k , Bk in self . nest_to_item . items (): logit [:, Bk ] = ( W [:, k ] + self . lambdas [ k ] * inclusive_value [ k ]) . view ( - 1 , 1 ) # (T, |Bk|) # only count each nest once in the logsumexp within the nest level model. cols = [ x [ 0 ] for x in self . nest_to_item . values ()] logP_nest = logit - torch . logsumexp ( logit [:, cols ], dim = 1 , keepdim = True ) # ============================================================================= # compute the joint log P_{ni} as in the textbook. logP = logP_item + logP_nest self . _clamp_called_flag = False return logP def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args ) def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for name , param in self . named_parameters (): if name == 'lambda_weight' : # we don't regularize the lambda term, we only regularize coefficients. continue nll += self . regularization_weight * torch . norm ( param , p = L ) return nll @property def device ( self ) -> torch . device : \"\"\"Returns the device of the coefficient. Returns: torch.device: the device of the model. \"\"\" return next ( iter ( self . item_coef_dict . values ())) . device @staticmethod def is_intercept_term ( variable : str ): # check if the given variable is an intercept (fixed effect) term. # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc. return ( variable . startswith ( 'intercept[' ) and variable . endswith ( ']' )) def get_coefficient ( self , variable : str , level : Optional [ str ] = None ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" if variable == 'lambda' : return self . lambda_weight . detach () . clone () if level not in [ 'item' , 'nest' ]: raise ValueError ( f \"Level should be either 'item' or 'nest', got { level } .\" ) return self . state_dict ()[ f ' { level } _coef_dict. { variable } .coef' ] . detach () . clone ()","title":"summary()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.device","text":"Returns the device of the coefficient. Returns: Type Description torch . device torch.device: the device of the model.","title":"device"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.num_params","text":"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved. Returns: Name Type Description int int the total number of learnable parameters.","title":"num_params"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.__init__","text":"Initialization method of the nested logit model. Parameters: Name Type Description Default nest_to_item Dict [ object , List [ int ]] a dictionary maps a nest ID to a list of items IDs of the queried nest. required nest_coef_variation_dict Dict [ str , str ] a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. None nest_num_param_dict Dict [ str , int ] a dictionary maps a variable type name to the number of parameters in this variable group. None item_coef_variation_dict Dict [ str , str ] the same as nest_coef_variation_dict but for item features. None item_num_param_dict Dict [ str , int ] the same as nest_num_param_dict but for item features. None {nest, item}_formula (str a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. required dataset JointDataset a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. None num_users Optional [ int ] number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. None shared_lambda bool a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. False regularization Optional [ str ] this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. None regularization_weight Optional [ float ] the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. None Source code in torch_choice/model/nested_logit_model.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , nest_to_item : Dict [ object , List [ int ]], # method 1: specify variation and num param. dictionary. nest_coef_variation_dict : Optional [ Dict [ str , str ]] = None , nest_num_param_dict : Optional [ Dict [ str , int ]] = None , item_coef_variation_dict : Optional [ Dict [ str , str ]] = None , item_num_param_dict : Optional [ Dict [ str , int ]] = None , # method 2: specify formula and dataset. item_formula : Optional [ str ] = None , nest_formula : Optional [ str ] = None , dataset : Optional [ JointDataset ] = None , num_users : Optional [ int ] = None , shared_lambda : bool = False , regularization : Optional [ str ] = None , regularization_weight : Optional [ float ] = None ) -> None : \"\"\"Initialization method of the nested logit model. Args: nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list of items IDs of the queried nest. nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables. nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to the number of parameters in this variable group. item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but for item features. item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item features. {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names. dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item']. num_users (Optional[int], optional): number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None. shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False. regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None. regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None. \"\"\" # handle nest level model. using_formula_to_initiate = ( item_formula is not None ) and ( nest_formula is not None ) if using_formula_to_initiate : # make sure that the research does not specify duplicated information, which might cause conflict. if ( nest_coef_variation_dict is not None ) or ( item_coef_variation_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.' ) if ( nest_num_param_dict is not None ) or ( item_num_param_dict is not None ): raise ValueError ( 'You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.' ) if dataset is None : raise ValueError ( 'Dataset is required if {item, nest}_formula is specified to initiate the model.' ) nest_coef_variation_dict , nest_num_param_dict = parse_formula ( nest_formula , dataset . datasets [ 'nest' ]) item_coef_variation_dict , item_num_param_dict = parse_formula ( item_formula , dataset . datasets [ 'item' ]) else : # check for conflicting information. if ( nest_formula is not None ) or ( item_formula is not None ): raise ValueError ( 'You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.' ) # make sure that the research specifies all the required information. if ( nest_coef_variation_dict is None ) or ( item_coef_variation_dict is None ): raise ValueError ( 'You should specify the {item, nest}_coef_variation_dict to initiate the model.' ) if ( nest_num_param_dict is None ) or ( item_num_param_dict is None ): raise ValueError ( 'You should specify the {item, nest}_num_param_dict to initiate the model.' ) super ( NestedLogitModel , self ) . __init__ () self . nest_to_item = nest_to_item self . nest_coef_variation_dict = nest_coef_variation_dict self . nest_num_param_dict = nest_num_param_dict self . item_coef_variation_dict = item_coef_variation_dict self . item_num_param_dict = item_num_param_dict self . num_users = num_users self . nests = list ( nest_to_item . keys ()) self . num_nests = len ( self . nests ) self . num_items = sum ( len ( items ) for items in nest_to_item . values ()) # nest coefficients. self . nest_coef_dict = self . _build_coef_dict ( self . nest_coef_variation_dict , self . nest_num_param_dict , self . num_nests ) # item coefficients. self . item_coef_dict = self . _build_coef_dict ( self . item_coef_variation_dict , self . item_num_param_dict , self . num_items ) self . shared_lambda = shared_lambda if self . shared_lambda : self . lambda_weight = nn . Parameter ( torch . ones ( 1 ), requires_grad = True ) else : self . lambda_weight = nn . Parameter ( torch . ones ( self . num_nests ) / 2 , requires_grad = True ) # breakpoint() # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True) # used to warn users if forgot to call clamp. self . _clamp_called_flag = True self . regularization = regularization assert self . regularization in [ 'L1' , 'L2' , None ], f \"Provided regularization= { self . regularization } is not allowed, allowed values are ['L1', 'L2', None].\" self . regularization_weight = regularization_weight if ( self . regularization is not None ) and ( self . regularization_weight is None ): raise ValueError ( f 'You specified regularization type { self . regularization } without providing regularization_weight.' ) if ( self . regularization is None ) and ( self . regularization_weight is not None ): raise ValueError ( f 'You specified no regularization but you provide regularization_weight= { self . regularization_weight } , you should leave regularization_weight as None if you do not want to regularize the model.' )","title":"__init__()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward","text":"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method.","title":"forward()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward--todo-the-conditionalogitmodel-returns-predicted-utility-the-nestedlogitmodel-behaves-the-same","text":"Parameters: Name Type Description Default batch ChoiceDataset a ChoiceDataset object containing the data batch. required Returns: Type Description torch . Tensor torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability torch . Tensor of choosing item i in trip t. Source code in torch_choice/model/nested_logit_model.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def forward ( self , batch : ChoiceDataset ) -> torch . Tensor : \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument. For more details about the forward passing, please refer to the _forward() method. # TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same? Args: batch (ChoiceDataset): a ChoiceDataset object containing the data batch. Returns: torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t. \"\"\" return self . _forward ( batch [ 'nest' ] . x_dict , batch [ 'item' ] . x_dict , batch [ 'item' ] . user_index , batch [ 'item' ] . item_availability )","title":"TODO: the ConditionaLogitModel returns predicted utility, the NestedLogitModel behaves the same?"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.get_coefficient","text":"Retrieve the coefficient tensor for the given variable. Parameters: Name Type Description Default variable str the variable name. required level str from which level of model to extract the coefficient, can be 'item' or 'nest'. The level argument will be discarded if variable is lambda . None Returns: Type Description torch . Tensor torch.Tensor: the corresponding coefficient tensor of the requested variable. Source code in torch_choice/model/nested_logit_model.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_coefficient ( self , variable : str , level : Optional [ str ] = None ) -> torch . Tensor : \"\"\"Retrieve the coefficient tensor for the given variable. Args: variable (str): the variable name. level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`. Returns: torch.Tensor: the corresponding coefficient tensor of the requested variable. \"\"\" if variable == 'lambda' : return self . lambda_weight . detach () . clone () if level not in [ 'item' , 'nest' ]: raise ValueError ( f \"Level should be either 'item' or 'nest', got { level } .\" ) return self . state_dict ()[ f ' { level } _coef_dict. { variable } .coef' ] . detach () . clone ()","title":"get_coefficient()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.log_likelihood","text":"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: Name Type Description _type_ the log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py 319 320 321 322 323 324 325 def log_likelihood ( self , * args ): \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method. Returns: _type_: the log likelihood of the model. \"\"\" return - self . negative_log_likelihood ( * args )","title":"log_likelihood()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.loss","text":"The loss function to be optimized. This is a wrapper of negative_log_likelihood + regularization loss if required. Source code in torch_choice/model/nested_logit_model.py 355 356 357 358 359 360 361 362 363 364 365 def loss ( self , * args , ** kwargs ): \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\" nll = self . negative_log_likelihood ( * args , ** kwargs ) if self . regularization is not None : L = { 'L1' : 1 , 'L2' : 2 }[ self . regularization ] for name , param in self . named_parameters (): if name == 'lambda_weight' : # we don't regularize the lambda term, we only regularize coefficients. continue nll += self . regularization_weight * torch . norm ( param , p = L ) return nll","title":"loss()"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.negative_log_likelihood","text":"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Parameters: Name Type Description Default batch ChoiceDataset the ChoiceDataset object containing the data. required y torch . LongTensor the label. required is_train bool which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, is_train should be set to True. If we merely need a performance metric, then is_train can be set to False for better performance. Defaults to True. True Returns: Type Description torch . scalar_tensor torch.scalar_tensor: the negative log likelihood of the model. Source code in torch_choice/model/nested_logit_model.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def negative_log_likelihood ( self , batch : ChoiceDataset , y : torch . LongTensor , is_train : bool = True ) -> torch . scalar_tensor : \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples in batch instead of the average. Args: batch (ChoiceDataset): the ChoiceDataset object containing the data. y (torch.LongTensor): the label. is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric, then `is_train` can be set to False for better performance. Defaults to True. Returns: torch.scalar_tensor: the negative log likelihood of the model. \"\"\" # compute the negative log-likelihood loss directly. if is_train : self . train () else : self . eval () # (num_trips, num_items) logP = self . forward ( batch ) nll = - logP [ torch . arange ( len ( y )), y ] . sum () return nll","title":"negative_log_likelihood()"},{"location":"conditional_logit_model_mode_canada/","text":"Tutorial: Conditional Logit Model on ModeCanada Dataset Author: Tianyu Du (tianyudu@stanford.edu) Update: May. 3, 2022 Reference: This tutorial is modified from the Random utility model and the multinomial logit model in th documentation of mlogit package in R. Please note that the dataset involved in this example is fairly small (2,779 choice records), so we don't expect the performance to be faster than the R implementation. We provide this tutorial mainly to check the correctness of our prediction. The fully potential of PyTorch is better exploited on much larger dataset. The executable Jupyter notebook for this tutorial is located at Random Utility Model (RUM) 1: Conditional Logit Model . Let's first import essential Python packages. from time import time import numpy as np import pandas as pd import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , utils from torch_choice.model import ConditionalLogitModel from torch_choice.utils.run_helper import run This tutorial will run both with and without graphic processing unit (GPU). However, our package is much faster with GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) device = 'cuda' else : print ( 'Running tutorial on CPU.' ) device = 'cpu' Running tutorial on CPU. Load Dataset We have included the ModeCanada dataset in our package, which is located at ./public_datasets/ . The ModeCanada dataset contains individuals' choice on traveling methods. The raw dataset is in a long-format, in which the case variable identifies each choice. Using the terminology mentioned in the data management tutorial, each choice is called a purchasing record (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as \\(B\\) . For example, the first four row below (with case == 109 ) corresponds to the first choice, the alt column lists all alternatives/items available. The choice column identifies which alternative/item is chosen. The second row in the data snapshot below, we have choice == 1 and alt == 'air' for case == 109 . This indicates the travelling mode chosen in case = 109 was air . Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial. We focus on cases when four alternatives were available by filtering noalt == 4 . df = pd . read_csv ( './public_datasets/ModeCanada.csv' ) df = df . query ( 'noalt == 4' ) . reset_index ( drop = True ) df . sort_values ( by = 'case' , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 304 109 train 0 377 58.25 215 74 4 45 0 4 1 305 109 air 1 377 142.80 56 85 9 45 0 4 2 306 109 bus 0 377 27.52 301 63 8 45 0 4 3 307 109 car 0 377 71.63 262 0 0 45 0 4 4 308 110 train 0 377 58.25 215 74 4 70 0 4 Since there are 4 rows corresponding to each purchasing record , the length of the long-format data is \\(4 \\times B\\) . Please refer to the data management tutorial for notations. df . shape (11116, 12) Construct the item_index tensor The first thing is to construct the item_index tensor identifying which item (i.e., travel mode) was chosen in each purchasing record. We can now construct the item_index array containing which item was chosen in each purchasing record. item_index = df [ df [ 'choice' ] == 1 ] . sort_values ( by = 'case' )[ 'alt' ] . reset_index ( drop = True ) print ( item_index ) 0 air 1 air 2 air 3 air 4 air ... 2774 car 2775 car 2776 car 2777 car 2778 car Name: alt, Length: 2779, dtype: object Since we will be training our model using PyTorch , we need to encode {'air', 'bus', 'car', 'train'} into integer values. Travel Mode Name Encoded Integer Values air 0 bus 1 car 2 train 3 The generated item_index would be a tensor of shape 2,778 (i.e., number of purchasing records in this dataset) with values {0, 1, 2, 3} . item_names = [ 'air' , 'bus' , 'car' , 'train' ] num_items = 4 encoder = dict ( zip ( item_names , range ( num_items ))) print ( f \" { encoder =:} \" ) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) print ( f \" { item_index =:} \" ) encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3} item_index=tensor([0, 0, 0, ..., 2, 2, 2]) Construct Observables Then let's constrct tensors for observables. As mentioned in the data management tutorial, the session is capturing the temporal dimension of our data. Since we have different values cost , freq and ovt for each purchasing record and for each item, it's natural to say each purchasing record has its own session. Consequently, these three variables are price observables since they vary by both item and session. The tensor holding these observables has shape \\((\\text{numer of purchasing records}, \\text{number of items}, 3)\\) We do the same for variable ivt , we put ivt into a separate tensor because we want to model its coefficient differently later. price_cost_freq_ovt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = [ 'cost' , 'freq' , 'ovt' ]) print ( f ' { price_cost_freq_ovt . shape =:} ' ) price_ivt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = 'ivt' ) print ( f ' { price_ivt . shape =:} ' ) price_cost_freq_ovt.shape=torch.Size([2779, 4, 3]) price_ivt.shape=torch.Size([2779, 4, 1]) In contrast, the income variable varies only by session (i.e., purchasing record), but not by item. income is therefore naturally a session variable. session_income = df . groupby ( 'case' )[ 'income' ] . first () session_income = torch . Tensor ( session_income . values ) . view ( - 1 , 1 ) print ( f ' { session_income . shape =:} ' ) session_income.shape=torch.Size([2779, 1]) To summarize, the ChoiceDataset constructed contains 2779 choice records. Since the original dataset did not reveal the identity of each decision maker, we consider all 2779 choices were made by a single user but in 2779 different sessions to handle variations. In this case, the cost , freq and ovt are observables depending on both sessions and items, we created a price_cost_freq_ovt tensor with shape (num_sessions, num_items, 3) = (2779, 4, 3) to contain these variables. In contrast, the income information depends only on session but not on items, hence we create the session_income tensor to store it. Because we wish to fit item-specific coefficients for the ivt variable, which varies by both sessions and items as well, we create another price_ivt tensor in addition to the price_cost_freq_ovt tensor. Lastly, we put all tensors we created to a single ChoiceDataset object, and move the dataset to the appropriate device. dataset = ChoiceDataset ( item_index = item_index , price_cost_freq_ovt = price_cost_freq_ovt , session_income = session_income , price_ivt = price_ivt ) . to ( device ) No `session_index` is provided, assume each choice instance is in its own session. You can print(dataset) to check shapes of tensors contained in the ChoiceDataset . print ( dataset ) ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) Create the Model We now construct the ConditionalLogitModel to fit the dataset we constructed above. To start with, we aim to estimate the following model formulation: \\[ U_{uit} = \\beta^0_i + \\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{price:ivt} + \\epsilon_{uit} \\] We now initialize the ConditionalLogitModel to predict choices from the dataset. Please see the documentation for a complete description of the ConditionalLogitModel class. At it's core, the ConditionalLogitModel constructor requires the following four components. Define variation of each \\(\\beta\\) using coef_variation_dict The keyword coef_variation_dict is a dictionary with variable names (defined above while constructing the dataset) as keys and values from {constant, user, item, item-full} . For instance, since we wish to have constant coefficients for cost , freq and ovt observables, and these three observables are stored in the price_cost_freq_ovt tensor of the choice dataset, we set coef_variation_dict['price_cost_freq_ovt'] = 'constant' (corresponding to the \\(\\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it}\\) term above). The models allows for the option of zeroing coefficient for one item. The variation of \\(\\beta^3\\) above is specified as item-full which indicates 4 values of \\(\\beta^3\\) is learned (one for each item). In contrast, \\(\\beta^0, \\beta^2\\) are specified to have variation item instead of item-full . In this case, the \\(\\beta\\) correspond to the first item (i.e., the baseline item, which is encoded as 0 in the label tensor, air in our example) is force to be zero. The researcher needs to declare intercept explicitly for the model to fit an intercept as well, otherwise the model assumes zero intercept term. Define the dimension of each \\(\\beta\\) using num_param_dict The num_param_dict is a dictionary with keys exactly the same as the coef_variation_dict . Each of dictionary values tells the dimension of the corresponding observables, hence the dimension of the coefficient. For example, the price_cost_freq_ovt consists of three observables and we set the corresponding to three. Even the model can infer num_param_dict['intercept'] = 1 , but we recommend the research to include it for completeness. Number of items The num_items keyword informs the model how many alternatives users are choosing from. Number of users The num_users keyword is an optional integer informing the model how many users there are in the dataset. However, in this example we implicitly assume there is only one user making all the decisions and we do not have any user_obs involved, hence num_users argument is not supplied. model = ConditionalLogitModel ( coef_variation_dict = { 'price_cost_freq_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_param_dict = { 'price_cost_freq_ovt' : 3 , 'session_income' : 1 , 'price_ivt' : 1 , 'intercept' : 1 }, num_items = 4 ) Then we move the model to the appropriate device. model = model . to ( device ) One can print the ConditionalLogitModel object to obtain a summary of the model. print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu Creating Model using Formula Alternatively, researchers can create the model using a formula like in R. The formula consists of a list of additive terms separated by + sign, and each term looks like (variable_name|variation) . Where variable_name is the name of the variable in the dataset, and variation is one of constant , user , item , item-full . Initializing the model using formula requires you to pass in the ChoiceDataset object as well so that the model can infer the dimension of each variable. These two ways of creating models lead to equivalent models. model = model = ConditionalLogitModel ( formula = '(price_cost_freq_ovt|constant) + (session_income|item) + (price_ivt|item-full) + (intercept|item)' , dataset = dataset , num_items = 4 ) print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu Train the Model We provide an easy-to-use helper function run() imported from torch_choice.utils.run_helper to fit the model with a particular dataset. We provide an easy-to-use model runner for both ConditionalLogitModel and NestedLogitModel (see later) instances. The run() mehtod supports mini-batch updating as well, for small datasets like the one we are dealing right now, we can use batch_size = -1 to conduct full-batch gradient update. start_time = time () run ( model , dataset , num_epochs = 50000 , learning_rate = 0.01 , batch_size =- 1 ) print ( 'Time taken:' , time () - start_time ) ==================== received model ==================== ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu ==================== received dataset ==================== ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) ==================== training the model ==================== Epoch 5000: Log-likelihood=-1877.81640625 Epoch 10000: Log-likelihood=-1878.5775146484375 Epoch 15000: Log-likelihood=-1879.2830810546875 Epoch 20000: Log-likelihood=-1895.0306396484375 Epoch 25000: Log-likelihood=-1876.6690673828125 Epoch 30000: Log-likelihood=-1874.603759765625 Epoch 35000: Log-likelihood=-1877.6473388671875 Epoch 40000: Log-likelihood=-1891.16357421875 Epoch 45000: Log-likelihood=-1877.5592041015625 Epoch 50000: Log-likelihood=-1876.2728271484375 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.01 Batch Size: 2779 out of 2779 observations in total Final Log-likelihood: -1876.2728271484375 Coefficients: | Coefficient | Estimation | Std. Err. | |:--------------------------------|-------------:|------------:| | price_cost_freq_ovt[constant]_0 | -0.0334654 | 0.00716249 | | price_cost_freq_ovt[constant]_1 | 0.0924185 | 0.00512754 | | price_cost_freq_ovt[constant]_2 | -0.0432329 | 0.0032801 | | session_income[item]_0 | -0.0892292 | 0.0188848 | | session_income[item]_1 | -0.0278481 | 0.00386248 | | session_income[item]_2 | -0.0383362 | 0.00414 | | price_ivt[item-full]_0 | 0.0593935 | 0.0101313 | | price_ivt[item-full]_1 | -0.00697196 | 0.00456365 | | price_ivt[item-full]_2 | -0.00629515 | 0.0019084 | | price_ivt[item-full]_3 | -0.00164838 | 0.00119982 | | intercept[item]_0 | 0.701135 | 1.30931 | | intercept[item]_1 | 1.84954 | 0.714135 | | intercept[item]_2 | 3.27894 | 0.631329 | Time taken: 109.68721008300781 Parameter Estimation from R The following is the R-output from the mlogit implementation, the estimation, standard error, and log-likelihood from our torch_choice implementation is the same as the result from mlogit implementation. We see that the final log-likelihood of models estimated using two packages are all around -1874 . The run() method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters. Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages. R Output install.packages ( \"mlogit\" ) library ( \"mlogit\" ) data ( \"ModeCanada\" , package = \"mlogit\" ) MC <- dfidx ( ModeCanada , subset = noalt == 4 ) ml.MC1 <- mlogit ( choice ~ cost + freq + ovt | income | ivt , MC , reflevel = 'air' ) summary ( ml.MC1 ) Call: mlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, reflevel = \"air\", method = \"nr\") Frequencies of alternatives:choice air train bus car 0.3738755 0.1666067 0.0035984 0.4559194 nr method 9 iterations, 0h:0m:0s g'(-H)^-1g = 0.00014 successive function values within tolerance limits Coefficients : Estimate Std. Error z-value Pr(>|z|) (Intercept):train 3.2741952 0.6244152 5.2436 1.575e-07 *** (Intercept):bus 0.6983381 1.2802466 0.5455 0.5854292 (Intercept):car 1.8441129 0.7085089 2.6028 0.0092464 ** cost -0.0333389 0.0070955 -4.6986 2.620e-06 *** freq 0.0925297 0.0050976 18.1517 < 2.2e-16 *** ovt -0.0430036 0.0032247 -13.3356 < 2.2e-16 *** income:train -0.0381466 0.0040831 -9.3426 < 2.2e-16 *** income:bus -0.0890867 0.0183471 -4.8556 1.200e-06 *** income:car -0.0279930 0.0038726 -7.2286 4.881e-13 *** ivt:air 0.0595097 0.0100727 5.9080 3.463e-09 *** ivt:train -0.0014504 0.0011875 -1.2214 0.2219430 ivt:bus -0.0067835 0.0044334 -1.5301 0.1259938 ivt:car -0.0064603 0.0018985 -3.4029 0.0006668 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Log-Likelihood: -1874.3 McFadden R^2: 0.35443 Likelihood ratio test : chisq = 2058.1 (p.value = < 2.22e-16)","title":"Conditional Logit Model"},{"location":"conditional_logit_model_mode_canada/#tutorial-conditional-logit-model-on-modecanada-dataset","text":"Author: Tianyu Du (tianyudu@stanford.edu) Update: May. 3, 2022 Reference: This tutorial is modified from the Random utility model and the multinomial logit model in th documentation of mlogit package in R. Please note that the dataset involved in this example is fairly small (2,779 choice records), so we don't expect the performance to be faster than the R implementation. We provide this tutorial mainly to check the correctness of our prediction. The fully potential of PyTorch is better exploited on much larger dataset. The executable Jupyter notebook for this tutorial is located at Random Utility Model (RUM) 1: Conditional Logit Model . Let's first import essential Python packages. from time import time import numpy as np import pandas as pd import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , utils from torch_choice.model import ConditionalLogitModel from torch_choice.utils.run_helper import run This tutorial will run both with and without graphic processing unit (GPU). However, our package is much faster with GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) device = 'cuda' else : print ( 'Running tutorial on CPU.' ) device = 'cpu' Running tutorial on CPU.","title":"Tutorial: Conditional Logit Model on ModeCanada Dataset"},{"location":"conditional_logit_model_mode_canada/#load-dataset","text":"We have included the ModeCanada dataset in our package, which is located at ./public_datasets/ . The ModeCanada dataset contains individuals' choice on traveling methods. The raw dataset is in a long-format, in which the case variable identifies each choice. Using the terminology mentioned in the data management tutorial, each choice is called a purchasing record (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as \\(B\\) . For example, the first four row below (with case == 109 ) corresponds to the first choice, the alt column lists all alternatives/items available. The choice column identifies which alternative/item is chosen. The second row in the data snapshot below, we have choice == 1 and alt == 'air' for case == 109 . This indicates the travelling mode chosen in case = 109 was air . Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial. We focus on cases when four alternatives were available by filtering noalt == 4 . df = pd . read_csv ( './public_datasets/ModeCanada.csv' ) df = df . query ( 'noalt == 4' ) . reset_index ( drop = True ) df . sort_values ( by = 'case' , inplace = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 304 109 train 0 377 58.25 215 74 4 45 0 4 1 305 109 air 1 377 142.80 56 85 9 45 0 4 2 306 109 bus 0 377 27.52 301 63 8 45 0 4 3 307 109 car 0 377 71.63 262 0 0 45 0 4 4 308 110 train 0 377 58.25 215 74 4 70 0 4 Since there are 4 rows corresponding to each purchasing record , the length of the long-format data is \\(4 \\times B\\) . Please refer to the data management tutorial for notations. df . shape (11116, 12)","title":"Load Dataset"},{"location":"conditional_logit_model_mode_canada/#construct-the-item_index-tensor","text":"The first thing is to construct the item_index tensor identifying which item (i.e., travel mode) was chosen in each purchasing record. We can now construct the item_index array containing which item was chosen in each purchasing record. item_index = df [ df [ 'choice' ] == 1 ] . sort_values ( by = 'case' )[ 'alt' ] . reset_index ( drop = True ) print ( item_index ) 0 air 1 air 2 air 3 air 4 air ... 2774 car 2775 car 2776 car 2777 car 2778 car Name: alt, Length: 2779, dtype: object Since we will be training our model using PyTorch , we need to encode {'air', 'bus', 'car', 'train'} into integer values. Travel Mode Name Encoded Integer Values air 0 bus 1 car 2 train 3 The generated item_index would be a tensor of shape 2,778 (i.e., number of purchasing records in this dataset) with values {0, 1, 2, 3} . item_names = [ 'air' , 'bus' , 'car' , 'train' ] num_items = 4 encoder = dict ( zip ( item_names , range ( num_items ))) print ( f \" { encoder =:} \" ) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) print ( f \" { item_index =:} \" ) encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3} item_index=tensor([0, 0, 0, ..., 2, 2, 2])","title":"Construct the item_index tensor"},{"location":"conditional_logit_model_mode_canada/#construct-observables","text":"Then let's constrct tensors for observables. As mentioned in the data management tutorial, the session is capturing the temporal dimension of our data. Since we have different values cost , freq and ovt for each purchasing record and for each item, it's natural to say each purchasing record has its own session. Consequently, these three variables are price observables since they vary by both item and session. The tensor holding these observables has shape \\((\\text{numer of purchasing records}, \\text{number of items}, 3)\\) We do the same for variable ivt , we put ivt into a separate tensor because we want to model its coefficient differently later. price_cost_freq_ovt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = [ 'cost' , 'freq' , 'ovt' ]) print ( f ' { price_cost_freq_ovt . shape =:} ' ) price_ivt = utils . pivot3d ( df , dim0 = 'case' , dim1 = 'alt' , values = 'ivt' ) print ( f ' { price_ivt . shape =:} ' ) price_cost_freq_ovt.shape=torch.Size([2779, 4, 3]) price_ivt.shape=torch.Size([2779, 4, 1]) In contrast, the income variable varies only by session (i.e., purchasing record), but not by item. income is therefore naturally a session variable. session_income = df . groupby ( 'case' )[ 'income' ] . first () session_income = torch . Tensor ( session_income . values ) . view ( - 1 , 1 ) print ( f ' { session_income . shape =:} ' ) session_income.shape=torch.Size([2779, 1]) To summarize, the ChoiceDataset constructed contains 2779 choice records. Since the original dataset did not reveal the identity of each decision maker, we consider all 2779 choices were made by a single user but in 2779 different sessions to handle variations. In this case, the cost , freq and ovt are observables depending on both sessions and items, we created a price_cost_freq_ovt tensor with shape (num_sessions, num_items, 3) = (2779, 4, 3) to contain these variables. In contrast, the income information depends only on session but not on items, hence we create the session_income tensor to store it. Because we wish to fit item-specific coefficients for the ivt variable, which varies by both sessions and items as well, we create another price_ivt tensor in addition to the price_cost_freq_ovt tensor. Lastly, we put all tensors we created to a single ChoiceDataset object, and move the dataset to the appropriate device. dataset = ChoiceDataset ( item_index = item_index , price_cost_freq_ovt = price_cost_freq_ovt , session_income = session_income , price_ivt = price_ivt ) . to ( device ) No `session_index` is provided, assume each choice instance is in its own session. You can print(dataset) to check shapes of tensors contained in the ChoiceDataset . print ( dataset ) ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)","title":"Construct Observables"},{"location":"conditional_logit_model_mode_canada/#create-the-model","text":"We now construct the ConditionalLogitModel to fit the dataset we constructed above. To start with, we aim to estimate the following model formulation: \\[ U_{uit} = \\beta^0_i + \\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{price:ivt} + \\epsilon_{uit} \\] We now initialize the ConditionalLogitModel to predict choices from the dataset. Please see the documentation for a complete description of the ConditionalLogitModel class. At it's core, the ConditionalLogitModel constructor requires the following four components.","title":"Create the Model"},{"location":"conditional_logit_model_mode_canada/#define-variation-of-each-beta-using-coef_variation_dict","text":"The keyword coef_variation_dict is a dictionary with variable names (defined above while constructing the dataset) as keys and values from {constant, user, item, item-full} . For instance, since we wish to have constant coefficients for cost , freq and ovt observables, and these three observables are stored in the price_cost_freq_ovt tensor of the choice dataset, we set coef_variation_dict['price_cost_freq_ovt'] = 'constant' (corresponding to the \\(\\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it}\\) term above). The models allows for the option of zeroing coefficient for one item. The variation of \\(\\beta^3\\) above is specified as item-full which indicates 4 values of \\(\\beta^3\\) is learned (one for each item). In contrast, \\(\\beta^0, \\beta^2\\) are specified to have variation item instead of item-full . In this case, the \\(\\beta\\) correspond to the first item (i.e., the baseline item, which is encoded as 0 in the label tensor, air in our example) is force to be zero. The researcher needs to declare intercept explicitly for the model to fit an intercept as well, otherwise the model assumes zero intercept term.","title":"Define variation of each \\(\\beta\\) using coef_variation_dict"},{"location":"conditional_logit_model_mode_canada/#define-the-dimension-of-each-beta-using-num_param_dict","text":"The num_param_dict is a dictionary with keys exactly the same as the coef_variation_dict . Each of dictionary values tells the dimension of the corresponding observables, hence the dimension of the coefficient. For example, the price_cost_freq_ovt consists of three observables and we set the corresponding to three. Even the model can infer num_param_dict['intercept'] = 1 , but we recommend the research to include it for completeness.","title":"Define the dimension of each \\(\\beta\\) using num_param_dict"},{"location":"conditional_logit_model_mode_canada/#number-of-items","text":"The num_items keyword informs the model how many alternatives users are choosing from.","title":"Number of items"},{"location":"conditional_logit_model_mode_canada/#number-of-users","text":"The num_users keyword is an optional integer informing the model how many users there are in the dataset. However, in this example we implicitly assume there is only one user making all the decisions and we do not have any user_obs involved, hence num_users argument is not supplied. model = ConditionalLogitModel ( coef_variation_dict = { 'price_cost_freq_ovt' : 'constant' , 'session_income' : 'item' , 'price_ivt' : 'item-full' , 'intercept' : 'item' }, num_param_dict = { 'price_cost_freq_ovt' : 3 , 'session_income' : 1 , 'price_ivt' : 1 , 'intercept' : 1 }, num_items = 4 ) Then we move the model to the appropriate device. model = model . to ( device ) One can print the ConditionalLogitModel object to obtain a summary of the model. print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu","title":"Number of users"},{"location":"conditional_logit_model_mode_canada/#creating-model-using-formula","text":"Alternatively, researchers can create the model using a formula like in R. The formula consists of a list of additive terms separated by + sign, and each term looks like (variable_name|variation) . Where variable_name is the name of the variable in the dataset, and variation is one of constant , user , item , item-full . Initializing the model using formula requires you to pass in the ChoiceDataset object as well so that the model can infer the dimension of each variable. These two ways of creating models lead to equivalent models. model = model = ConditionalLogitModel ( formula = '(price_cost_freq_ovt|constant) + (session_income|item) + (price_ivt|item-full) + (intercept|item)' , dataset = dataset , num_items = 4 ) print ( model ) ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu","title":"Creating Model using Formula"},{"location":"conditional_logit_model_mode_canada/#train-the-model","text":"We provide an easy-to-use helper function run() imported from torch_choice.utils.run_helper to fit the model with a particular dataset. We provide an easy-to-use model runner for both ConditionalLogitModel and NestedLogitModel (see later) instances. The run() mehtod supports mini-batch updating as well, for small datasets like the one we are dealing right now, we can use batch_size = -1 to conduct full-batch gradient update. start_time = time () run ( model , dataset , num_epochs = 50000 , learning_rate = 0.01 , batch_size =- 1 ) print ( 'Time taken:' , time () - start_time ) ==================== received model ==================== ConditionalLogitModel( (coef_dict): ModuleDict( (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu). (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation. X[session_income[item]] with 1 parameters, with item level variation. X[price_ivt[item-full]] with 1 parameters, with item-full level variation. X[intercept[item]] with 1 parameters, with item level variation. device=cpu ==================== received dataset ==================== ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu) ==================== training the model ==================== Epoch 5000: Log-likelihood=-1877.81640625 Epoch 10000: Log-likelihood=-1878.5775146484375 Epoch 15000: Log-likelihood=-1879.2830810546875 Epoch 20000: Log-likelihood=-1895.0306396484375 Epoch 25000: Log-likelihood=-1876.6690673828125 Epoch 30000: Log-likelihood=-1874.603759765625 Epoch 35000: Log-likelihood=-1877.6473388671875 Epoch 40000: Log-likelihood=-1891.16357421875 Epoch 45000: Log-likelihood=-1877.5592041015625 Epoch 50000: Log-likelihood=-1876.2728271484375 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.01 Batch Size: 2779 out of 2779 observations in total Final Log-likelihood: -1876.2728271484375 Coefficients: | Coefficient | Estimation | Std. Err. | |:--------------------------------|-------------:|------------:| | price_cost_freq_ovt[constant]_0 | -0.0334654 | 0.00716249 | | price_cost_freq_ovt[constant]_1 | 0.0924185 | 0.00512754 | | price_cost_freq_ovt[constant]_2 | -0.0432329 | 0.0032801 | | session_income[item]_0 | -0.0892292 | 0.0188848 | | session_income[item]_1 | -0.0278481 | 0.00386248 | | session_income[item]_2 | -0.0383362 | 0.00414 | | price_ivt[item-full]_0 | 0.0593935 | 0.0101313 | | price_ivt[item-full]_1 | -0.00697196 | 0.00456365 | | price_ivt[item-full]_2 | -0.00629515 | 0.0019084 | | price_ivt[item-full]_3 | -0.00164838 | 0.00119982 | | intercept[item]_0 | 0.701135 | 1.30931 | | intercept[item]_1 | 1.84954 | 0.714135 | | intercept[item]_2 | 3.27894 | 0.631329 | Time taken: 109.68721008300781","title":"Train the Model"},{"location":"conditional_logit_model_mode_canada/#parameter-estimation-from-r","text":"The following is the R-output from the mlogit implementation, the estimation, standard error, and log-likelihood from our torch_choice implementation is the same as the result from mlogit implementation. We see that the final log-likelihood of models estimated using two packages are all around -1874 . The run() method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters. Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages.","title":"Parameter Estimation from R"},{"location":"conditional_logit_model_mode_canada/#r-output","text":"install.packages ( \"mlogit\" ) library ( \"mlogit\" ) data ( \"ModeCanada\" , package = \"mlogit\" ) MC <- dfidx ( ModeCanada , subset = noalt == 4 ) ml.MC1 <- mlogit ( choice ~ cost + freq + ovt | income | ivt , MC , reflevel = 'air' ) summary ( ml.MC1 ) Call: mlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, reflevel = \"air\", method = \"nr\") Frequencies of alternatives:choice air train bus car 0.3738755 0.1666067 0.0035984 0.4559194 nr method 9 iterations, 0h:0m:0s g'(-H)^-1g = 0.00014 successive function values within tolerance limits Coefficients : Estimate Std. Error z-value Pr(>|z|) (Intercept):train 3.2741952 0.6244152 5.2436 1.575e-07 *** (Intercept):bus 0.6983381 1.2802466 0.5455 0.5854292 (Intercept):car 1.8441129 0.7085089 2.6028 0.0092464 ** cost -0.0333389 0.0070955 -4.6986 2.620e-06 *** freq 0.0925297 0.0050976 18.1517 < 2.2e-16 *** ovt -0.0430036 0.0032247 -13.3356 < 2.2e-16 *** income:train -0.0381466 0.0040831 -9.3426 < 2.2e-16 *** income:bus -0.0890867 0.0183471 -4.8556 1.200e-06 *** income:car -0.0279930 0.0038726 -7.2286 4.881e-13 *** ivt:air 0.0595097 0.0100727 5.9080 3.463e-09 *** ivt:train -0.0014504 0.0011875 -1.2214 0.2219430 ivt:bus -0.0067835 0.0044334 -1.5301 0.1259938 ivt:car -0.0064603 0.0018985 -3.4029 0.0006668 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Log-Likelihood: -1874.3 McFadden R^2: 0.35443 Likelihood ratio test : chisq = 2058.1 (p.value = < 2.22e-16)","title":"R Output"},{"location":"data_management/","text":"Tutorial: Data Management Author: Tianyu Du (tianyudu@stanford.edu) Note : please go through the introduction tutorial here before proceeding. This notebook aims to help users understand the functionality of ChoiceDataset object. The ChoiceDataset is an instance of the more general PyTorch dataset object holding information of consumer choices. The ChoiceDataset offers easy, clean and efficient data management. The Jupyter-notebook version of this tutorial can be found here . This tutorial provides in-depth explanations on how the torch-choice library manages data. We are also providing an easy-to-use data wrapper converting long-format dataset to ChoiceDataset here , you can harness the torch-choice library without going through this tutorial. Note : since this package was initially proposed for modelling consumer choices, attribute names of ChoiceDataset are borrowed from the consumer choice literature. Note : PyTorch uses the term tensor to denote high dimensional matrices, we will be using tensor and matrix interchangeably. After walking through this tutorial, you should be abel to initiate a ChoiceDataset object as the following and use it to manage data. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs ) Observables Observables are tensors with specific shapes, we classify observables into four categories based on their variations. Basic Usage Optionally, the researcher can incorporate observables of, for example, users and items. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. The researcher should supply them with as appropriate keyword arguments while constructing the ChoiceDataset object. (Optional) Advanced Usage: Additional Observables In some cases, the researcher have multiple sets of user (or item, or session, or price) observables, say user income (a scalar variable) and user market membership . The user income a matrix in \\(\\mathbb{R}^{U\\times 1}\\) . Further, suppose there are four types of market membership: no-membership, silver-membership, gold-membership, and diamond-membership. The user market membership is a binary matrix in \\(\\{0, 1\\}^{U\\times 4}\\) if we one-hot encode users' membership status. In this case, the researcher can either 1. concatenate user_income and user_market_membership to a \\(\\mathbb{R}^{U\\times (1+4)}\\) matrix and supply it as a single user_obs as the following: dataset = ChoiceDataset ( ... , user_obs = torch . cat ([ user_income , user_market_membership ], dim = 1 ), ... ) 2. Or, supply these two sets of observables separately, namely a user_income \\(\\in \\mathbb{R}^{U \\times 1}\\) matrix and a user_market_membership \\(\\in \\mathbb{R}^{U \\times 4}\\) matrix as the following: dataset = ChoiceDataset ( ... , user_income = user_income , user_market_membership = user_market_membership , ... ) Supplying two separate sets of observables is particularly useful when the researcher wants different kinds of coefficients for different kinds of observables. For example, the researcher wishes to model the utility for user \\(u\\) to purchase item \\(i\\) in session \\(s\\) as the following: \\[ U_{usi} = \\beta_{i} X^{(u)}_{user\\ income} + \\gamma X^{(u)}_{user\\ market\\ membership} + \\varepsilon \\] Please note that the \\(\\beta_i\\) coefficient has an \\(i\\) subscript, which means it's item specific. The \\(\\gamma\\) coefficient has no subscript, which means it's the same for all items. The coefficient for user income is item-specific so that it captures the nature of the product (i.e., a luxury or an essential good). Additionally, the utility representation admits an user market membership becomes shoppers with active memberships tend to purchase more, and the coefficient of this term is constant across all items. As we will cover later in the modelling section, we need to supply two user observable tensors in this case for the model to build coefficient with different levels of variations (i.e., item-specific coefficients versus constant coefficients). In this case, the researcher needs to supply two tensors user_income and user_market_membership as keyword arguments to the ChoiceDataset constructor. Generally, the ChoiceDataset handles multiple user/item/session/price observables internally, the ChoiceDataset class identifies the variation of observables by their prefixes. For example, every keyword arguments passed into ChoiceDataset with name starting with item_ (except for the reserved item_availability ) will be treated as item observable tensors. Similarly, all keywords with names starting user_ , session_ and price_ (except for reserved names like user_index and session_index mentioned above) will be interpreted as user/session/price observable tensors. # import required dependencies. import numpy as np import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' ) Creating ChoiceDataset Object # Feel free to modify it as you want. num_users = 10 num_items = 4 num_sessions = 500 length_of_dataset = 10000 Step 1: Generate some random purchase records and observables We will be creating a randomly generated dataset with 10000 purchase records from 10 users, 4 items and 500 sessions. We use the term purchase record to denote the observation in the dataset due to the convention in Stata documentation (because observation meant something else in the Stata documentation and we don't want to confuse existing Stata users). As mentioned in the introduction tutorial, one purchase record consists of who (i.e., user) bought what (i.e., item) when and where (i.e., session). The length of the dataset equals the number of purchase records in it. The first step is to randomly generate the purchase records using the following code. For simplicity, we assume all items are available in all sessions. # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. price_obs = torch . randn ( num_sessions , num_items , 12 ) We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen. Notes on Encodings Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool () Step 2: Initialize the ChoiceDataset . You can construct a choice set using the following code, which manage all information for you. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs ) What you can do with the ChoiceDataset ? print(dataset) and dataset.__str__ The command print(dataset) will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory). print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) dataset.summary() The summary method provides preliminary summarization of the dataset. print ( pd . DataFrame ( dataset . user_index ) . value_counts ()) 4 1038 8 1035 5 1024 1 1010 2 997 0 990 6 981 9 980 3 974 7 971 dtype: int64 print ( pd . DataFrame ( dataset . item_index ) . value_counts ()) 0 2575 1 2539 2 2467 3 2419 dtype: int64 dataset . summary () ChoiceDataset with 500 sessions, 4 items, 10 users, 10000 purchase records (observations) . The most frequent user is 4 with 1038 observations; the least frequent user is 7 with 971 observations; on average, there are 1000.00 observations per user. 5 most frequent users are: 4(1038 times), 8(1035 times), 5(1024 times), 1(1010 times), 2(997 times). 5 least frequent users are: 7(971 times), 3(974 times), 9(980 times), 6(981 times), 0(990 times). The most frequent item is 0, it was chosen 2575 times; the least frequent item is 3 it was 2419 times; on average, each item was purchased 2500.00 times. 4 most frequent items are: 0(2575 times), 1(2539 times), 2(2467 times), 3(2419 times). 4 least frequent items are: 3(2419 times), 2(2467 times), 1(2539 times), 0(2575 times). Attribute Summaries: Observable Tensor 'user_obs' with shape torch.Size([10, 128]) 0 1 2 3 4 5 \\ count 10.000000 10.000000 10.000000 10.000000 10.000000 10.000000 mean 0.687878 -0.339077 -0.375829 0.086242 0.250604 -0.344643 std 0.738520 1.259936 0.844018 0.766233 0.802785 0.645239 min -0.578577 -2.135251 -1.335928 -0.911508 -1.396776 -1.519729 25% 0.264708 -0.889820 -0.845100 -0.414891 -0.132619 -0.699887 50% 0.902505 -0.603065 -0.638757 -0.289223 0.297693 -0.405371 75% 1.155211 0.021188 -0.190907 0.712183 0.768554 0.117107 max 1.623162 2.217712 1.624211 1.252059 1.273116 0.571998 6 7 8 9 ... 118 119 \\ count 10.000000 10.000000 10.000000 10.000000 ... 10.000000 10.000000 mean 0.423672 0.325855 0.258114 -0.199072 ... -0.165618 -0.378175 std 1.304160 0.815934 0.938925 1.344848 ... 1.135625 0.940863 min -1.440672 -1.068176 -1.280547 -2.819688 ... -1.567793 -1.604171 25% -0.535055 0.051598 -0.178302 -0.801871 ... -1.114392 -1.066492 50% 0.502826 0.369002 0.230939 -0.576039 ... -0.114789 -0.587483 75% 1.227700 0.899518 0.740881 0.820789 ... 0.602045 0.160254 max 2.462891 1.440098 1.828760 1.866570 ... 1.854828 1.386001 120 121 122 123 124 125 \\ count 10.000000 10.000000 10.000000 10.000000 10.000000 10.000000 mean -0.557321 0.402392 -0.070746 -0.770201 0.594842 0.572671 std 1.128886 0.899030 0.757537 1.044478 0.956856 0.883374 min -3.131332 -0.907885 -1.296398 -2.159384 -1.244177 -0.462607 25% -0.834223 -0.059528 -0.222124 -1.332558 0.234198 -0.008799 50% -0.613761 0.117478 -0.109676 -0.984450 0.656855 0.466357 75% 0.040239 1.136383 0.416972 -0.285216 1.246513 0.772441 max 1.087999 1.757588 1.022053 1.486507 2.010775 2.162550 126 127 count 10.000000 10.000000 mean 0.226993 -0.064205 std 1.463179 0.602277 min -1.731004 -0.865115 25% -0.951169 -0.418553 50% 0.174763 -0.112277 75% 0.773072 0.353951 max 2.991696 0.804881 [8 rows x 128 columns] Observable Tensor 'item_obs' with shape torch.Size([4, 64]) 0 1 2 3 4 5 6 \\ count 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 mean 0.287015 -0.180256 -0.239000 0.169168 0.159036 0.385342 -1.142672 std 1.339318 1.603530 0.722772 0.473407 0.392562 1.327739 0.566069 min -1.138152 -2.212473 -1.051363 -0.538771 -0.330795 -0.517352 -1.770297 25% -0.558802 -0.990083 -0.745828 0.132031 -0.006671 -0.485835 -1.397787 50% 0.170810 -0.012201 -0.154058 0.385432 0.174086 -0.125969 -1.199654 75% 1.016628 0.797626 0.352770 0.422569 0.339793 0.745208 -0.944538 max 1.944591 1.515852 0.403479 0.444577 0.618768 2.310656 -0.401083 7 8 9 ... 54 55 56 \\ count 4.000000 4.000000 4.000000 ... 4.000000 4.000000 4.000000 mean 0.581071 -0.169341 0.076562 ... 0.055457 -0.002887 -0.160406 std 0.972295 0.978922 1.116274 ... 0.777132 0.903879 1.140101 min -0.596834 -1.309131 -1.563906 ... -0.481757 -0.997574 -1.721709 25% -0.025344 -0.718815 -0.153971 ... -0.442894 -0.340660 -0.631280 50% 0.745386 -0.177989 0.514336 ... -0.240767 -0.105541 0.117918 75% 1.351801 0.371485 0.744870 ... 0.257583 0.232232 0.588793 max 1.430348 0.987744 0.841483 ... 1.185118 1.197110 0.844249 57 58 59 60 61 62 63 count 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 mean 0.149579 0.199678 0.088542 -0.356379 1.004674 0.095064 -0.548665 std 0.963564 0.744614 1.170228 0.833992 0.559029 0.912057 0.730697 min -0.760765 -0.419252 -1.038935 -0.989042 0.442226 -0.989018 -1.445138 25% -0.268040 -0.383280 -0.604213 -0.970008 0.592259 -0.492793 -0.790356 50% -0.075941 0.036190 -0.142981 -0.611959 0.966522 0.230826 -0.546745 75% 0.341678 0.619148 0.549774 0.001670 1.378937 0.818683 -0.305054 max 1.510964 1.145585 1.679067 0.787444 1.643426 0.907622 0.343970 [8 rows x 64 columns] Observable Tensor 'session_obs' with shape torch.Size([500, 10]) 0 1 2 3 4 5 \\ count 500.000000 500.000000 500.000000 500.000000 500.000000 500.000000 mean -0.025211 -0.018355 -0.002907 0.091295 -0.061911 -0.046364 std 0.976283 1.029875 0.959884 0.968500 1.020114 1.010222 min -2.642895 -3.091050 -3.572037 -2.406249 -3.147900 -3.357277 25% -0.745162 -0.685578 -0.636044 -0.629955 -0.754234 -0.732924 50% -0.018775 0.017807 -0.018642 0.112322 -0.090321 -0.070502 75% 0.652438 0.646001 0.601829 0.722870 0.640275 0.652521 max 3.044069 3.191774 2.521059 2.695970 3.166039 2.714594 6 7 8 9 count 500.000000 500.000000 500.000000 500.000000 mean 0.000907 0.001370 0.070499 -0.007936 std 1.015561 1.032878 1.036212 0.936091 min -2.677915 -3.489751 -2.953354 -2.424499 25% -0.679291 -0.671086 -0.582997 -0.681405 50% 0.002569 -0.009368 0.087901 0.010856 75% 0.703671 0.732814 0.737692 0.618773 max 2.528283 3.259835 2.827300 2.492085 Observable Tensor 'price_obs' with shape torch.Size([500, 4, 12]) device=cpu dataset.num_{users, items, sessions} You can use the num_{users, items, sessions} attribute to obtain the number of users, items, and sessions, they are determined automatically from the {user, item, session}_obs tensors provided while initializing the dataset object. Note : the print =: operator requires Python3.8 or higher, you can remove =: if you are using an earlier copy of Python. print ( f ' { dataset . num_users =:} ' ) print ( f ' { dataset . num_items =:} ' ) print ( f ' { dataset . num_sessions =:} ' ) print ( f ' { len ( dataset ) =:} ' ) dataset.num_users=10 dataset.num_items=4 dataset.num_sessions=500 len(dataset)=10000 dataset.clone() The ChoiceDataset offers a clone method allow you to make copy of the dataset, you can modify the cloned dataset arbitrarily without changing the original dataset. # clone print ( dataset . item_index [: 10 ]) dataset_cloned = dataset . clone () dataset_cloned . item_index = 99 * torch . ones ( num_sessions ) print ( dataset_cloned . item_index [: 10 ]) print ( dataset . item_index [: 10 ]) # does not change the original dataset. tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) tensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.]) tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) dataset.to('cuda') and dataset._check_device_consistency() . One key advantage of the torch_choice and bemb is their compatibility with GPUs, you can easily move tensors in a ChoiceDataset object between host memory (i.e., cpu memory) and device memory (i.e., GPU memory) using dataset.to() method. Please note that the following code runs only if your machine has a compatible GPU and GPU-compatible version of PyTorch installed. Similarly, one can move data to host-memory using dataset.to('cpu') . The dataset also provides a dataset._check_device_consistency() method to check if all tensors are on the same device. If we only move the label to cpu without moving other tensors, this will result in an error message. # move to device print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset = dataset . to ( 'cuda' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . item_index . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset.device=cpu dataset.device=cpu dataset.user_index.device=cpu dataset.session_index.device=cpu dataset.device=cuda:0 dataset.item_index.device=cuda:0 dataset.user_index.device=cuda:0 dataset.session_index.device=cuda:0 dataset . _check_device_consistency () # # NOTE: this cell will result errors, this is intentional. dataset . item_index = dataset . item_index . to ( 'cpu' ) dataset . _check_device_consistency () --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-56-40d626c6d436> in <module> 1 # # NOTE: this cell will result errors, this is intentional. 2 dataset.item_index = dataset.item_index.to('cpu') ----> 3 dataset._check_device_consistency() ~/Development/torch-choice/torch_choice/data/choice_dataset.py in _check_device_consistency(self) 180 devices.append(val.device) 181 if len(set(devices)) > 1: --> 182 raise Exception(f'Found tensors on different devices: {set(devices)}.', 183 'Use dataset.to() method to align devices.') 184 Exception: (\"Found tensors on different devices: {device(type='cuda', index=0), device(type='cpu')}.\", 'Use dataset.to() method to align devices.') # create dictionary inputs for model.forward() # collapse to a dictionary object. print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) Subset method One can use dataset[indices] with indices as an integer-valued tensor or array to get the corresponding rows of the dataset. The example code block below queries the 6256-th, 4119-th, 453-th, 5520-th, and 1877-th row of the dataset object. The item_index , user_index , session_index of the resulted subset will be different from the original dataset, but other tensors will be the same. # __getitem__ to get batch. # pick 5 random sessions as the mini-batch. dataset = dataset . to ( 'cpu' ) indices = torch . Tensor ( np . random . choice ( len ( dataset ), size = 5 , replace = False )) . long () print ( indices ) subset = dataset [ indices ] print ( dataset ) print ( subset ) # print_dict_shape(subset.x_dict) # assert torch.all(dataset.x_dict['price_obs'][indices, :, :] == subset.x_dict['price_obs']) # assert torch.all(dataset.item_index[indices] == subset.item_index) tensor([1118, 976, 1956, 290, 8283]) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) ChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) The subset method internally creates a copy of the datasets so that any modification applied on the subset will not be reflected on the original dataset. The researcher can feel free to do in-place modification to the subset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) subset . item_index += 1 # modifying the batch does not change the original dataset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) tensor([0, 1, 0, 0, 0]) tensor([0, 1, 0, 0, 0]) tensor([1, 2, 1, 1, 1]) tensor([0, 1, 0, 0, 0]) print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) subset . item_obs += 1 print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) tensor(-1.5811) tensor(-1.5811) tensor(-0.5811) tensor(-1.5811) print ( id ( subset . item_index )) print ( id ( dataset . item_index [ indices ])) 140339656298640 140339656150528 Using Pytorch dataloader for the training loop. The ChoiceDataset object natively support batch samplers from PyTorch. For demonstration purpose, we turned off the shuffling option. from torch.utils.data.sampler import BatchSampler , SequentialSampler , RandomSampler shuffle = False # for demonstration purpose. batch_size = 32 # Create sampler. sampler = BatchSampler ( RandomSampler ( dataset ) if shuffle else SequentialSampler ( dataset ), batch_size = batch_size , drop_last = False ) dataloader = torch . utils . data . DataLoader ( dataset , sampler = sampler , num_workers = 1 , collate_fn = lambda x : x [ 0 ], pin_memory = ( dataset . device == 'cpu' )) print ( f ' { item_obs . shape =:} ' ) item_obs_all = item_obs . view ( 1 , num_items , - 1 ) . expand ( len ( dataset ), - 1 , - 1 ) item_obs_all = item_obs_all . to ( dataset . device ) item_index_all = item_index . to ( dataset . device ) print ( f ' { item_obs_all . shape =:} ' ) item_obs.shape=torch.Size([4, 64]) item_obs_all.shape=torch.Size([10000, 4, 64]) for i , batch in enumerate ( dataloader ): first , last = i * batch_size , min ( len ( dataset ), ( i + 1 ) * batch_size ) idx = torch . arange ( first , last ) assert torch . all ( item_obs_all [ idx , :, :] == batch . x_dict [ 'item_obs' ]) assert torch . all ( item_index_all [ idx ] == batch . item_index ) batch . x_dict [ 'item_obs' ] . shape torch.Size([16, 4, 64]) print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) dataset . __len__ () 10000 Chaining Multiple Datasets: JointDataset Examples dataset1 = dataset . clone () dataset2 = dataset . clone () joint_dataset = JointDataset ( the_dataset = dataset1 , another_dataset = dataset2 ) joint_dataset JointDataset with 2 sub-datasets: ( the_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) another_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) )","title":"Data Management"},{"location":"data_management/#tutorial-data-management","text":"Author: Tianyu Du (tianyudu@stanford.edu) Note : please go through the introduction tutorial here before proceeding. This notebook aims to help users understand the functionality of ChoiceDataset object. The ChoiceDataset is an instance of the more general PyTorch dataset object holding information of consumer choices. The ChoiceDataset offers easy, clean and efficient data management. The Jupyter-notebook version of this tutorial can be found here . This tutorial provides in-depth explanations on how the torch-choice library manages data. We are also providing an easy-to-use data wrapper converting long-format dataset to ChoiceDataset here , you can harness the torch-choice library without going through this tutorial. Note : since this package was initially proposed for modelling consumer choices, attribute names of ChoiceDataset are borrowed from the consumer choice literature. Note : PyTorch uses the term tensor to denote high dimensional matrices, we will be using tensor and matrix interchangeably. After walking through this tutorial, you should be abel to initiate a ChoiceDataset object as the following and use it to manage data. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs )","title":"Tutorial: Data Management"},{"location":"data_management/#observables","text":"Observables are tensors with specific shapes, we classify observables into four categories based on their variations.","title":"Observables"},{"location":"data_management/#basic-usage","text":"Optionally, the researcher can incorporate observables of, for example, users and items. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. price_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\) , price observables are values depending on both session and item such as the price of item. The researcher should supply them with as appropriate keyword arguments while constructing the ChoiceDataset object.","title":"Basic Usage"},{"location":"data_management/#optional-advanced-usage-additional-observables","text":"In some cases, the researcher have multiple sets of user (or item, or session, or price) observables, say user income (a scalar variable) and user market membership . The user income a matrix in \\(\\mathbb{R}^{U\\times 1}\\) . Further, suppose there are four types of market membership: no-membership, silver-membership, gold-membership, and diamond-membership. The user market membership is a binary matrix in \\(\\{0, 1\\}^{U\\times 4}\\) if we one-hot encode users' membership status. In this case, the researcher can either 1. concatenate user_income and user_market_membership to a \\(\\mathbb{R}^{U\\times (1+4)}\\) matrix and supply it as a single user_obs as the following: dataset = ChoiceDataset ( ... , user_obs = torch . cat ([ user_income , user_market_membership ], dim = 1 ), ... ) 2. Or, supply these two sets of observables separately, namely a user_income \\(\\in \\mathbb{R}^{U \\times 1}\\) matrix and a user_market_membership \\(\\in \\mathbb{R}^{U \\times 4}\\) matrix as the following: dataset = ChoiceDataset ( ... , user_income = user_income , user_market_membership = user_market_membership , ... ) Supplying two separate sets of observables is particularly useful when the researcher wants different kinds of coefficients for different kinds of observables. For example, the researcher wishes to model the utility for user \\(u\\) to purchase item \\(i\\) in session \\(s\\) as the following: \\[ U_{usi} = \\beta_{i} X^{(u)}_{user\\ income} + \\gamma X^{(u)}_{user\\ market\\ membership} + \\varepsilon \\] Please note that the \\(\\beta_i\\) coefficient has an \\(i\\) subscript, which means it's item specific. The \\(\\gamma\\) coefficient has no subscript, which means it's the same for all items. The coefficient for user income is item-specific so that it captures the nature of the product (i.e., a luxury or an essential good). Additionally, the utility representation admits an user market membership becomes shoppers with active memberships tend to purchase more, and the coefficient of this term is constant across all items. As we will cover later in the modelling section, we need to supply two user observable tensors in this case for the model to build coefficient with different levels of variations (i.e., item-specific coefficients versus constant coefficients). In this case, the researcher needs to supply two tensors user_income and user_market_membership as keyword arguments to the ChoiceDataset constructor. Generally, the ChoiceDataset handles multiple user/item/session/price observables internally, the ChoiceDataset class identifies the variation of observables by their prefixes. For example, every keyword arguments passed into ChoiceDataset with name starting with item_ (except for the reserved item_availability ) will be treated as item observable tensors. Similarly, all keywords with names starting user_ , session_ and price_ (except for reserved names like user_index and session_index mentioned above) will be interpreted as user/session/price observable tensors. # import required dependencies. import numpy as np import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' )","title":"(Optional) Advanced Usage: Additional Observables"},{"location":"data_management/#creating-choicedataset-object","text":"# Feel free to modify it as you want. num_users = 10 num_items = 4 num_sessions = 500 length_of_dataset = 10000","title":"Creating  ChoiceDataset Object"},{"location":"data_management/#step-1-generate-some-random-purchase-records-and-observables","text":"We will be creating a randomly generated dataset with 10000 purchase records from 10 users, 4 items and 500 sessions. We use the term purchase record to denote the observation in the dataset due to the convention in Stata documentation (because observation meant something else in the Stata documentation and we don't want to confuse existing Stata users). As mentioned in the introduction tutorial, one purchase record consists of who (i.e., user) bought what (i.e., item) when and where (i.e., session). The length of the dataset equals the number of purchase records in it. The first step is to randomly generate the purchase records using the following code. For simplicity, we assume all items are available in all sessions. # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. price_obs = torch . randn ( num_sessions , num_items , 12 ) We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen. Notes on Encodings Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool ()","title":"Step 1: Generate some random purchase records and observables"},{"location":"data_management/#step-2-initialize-the-choicedataset","text":"You can construct a choice set using the following code, which manage all information for you. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , price_obs = price_obs )","title":"Step 2: Initialize the ChoiceDataset."},{"location":"data_management/#what-you-can-do-with-the-choicedataset","text":"","title":"What you can do with the ChoiceDataset?"},{"location":"data_management/#printdataset-and-dataset__str__","text":"The command print(dataset) will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory). print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)","title":"print(dataset) and dataset.__str__"},{"location":"data_management/#datasetsummary","text":"The summary method provides preliminary summarization of the dataset. print ( pd . DataFrame ( dataset . user_index ) . value_counts ()) 4 1038 8 1035 5 1024 1 1010 2 997 0 990 6 981 9 980 3 974 7 971 dtype: int64 print ( pd . DataFrame ( dataset . item_index ) . value_counts ()) 0 2575 1 2539 2 2467 3 2419 dtype: int64 dataset . summary () ChoiceDataset with 500 sessions, 4 items, 10 users, 10000 purchase records (observations) . The most frequent user is 4 with 1038 observations; the least frequent user is 7 with 971 observations; on average, there are 1000.00 observations per user. 5 most frequent users are: 4(1038 times), 8(1035 times), 5(1024 times), 1(1010 times), 2(997 times). 5 least frequent users are: 7(971 times), 3(974 times), 9(980 times), 6(981 times), 0(990 times). The most frequent item is 0, it was chosen 2575 times; the least frequent item is 3 it was 2419 times; on average, each item was purchased 2500.00 times. 4 most frequent items are: 0(2575 times), 1(2539 times), 2(2467 times), 3(2419 times). 4 least frequent items are: 3(2419 times), 2(2467 times), 1(2539 times), 0(2575 times). Attribute Summaries: Observable Tensor 'user_obs' with shape torch.Size([10, 128]) 0 1 2 3 4 5 \\ count 10.000000 10.000000 10.000000 10.000000 10.000000 10.000000 mean 0.687878 -0.339077 -0.375829 0.086242 0.250604 -0.344643 std 0.738520 1.259936 0.844018 0.766233 0.802785 0.645239 min -0.578577 -2.135251 -1.335928 -0.911508 -1.396776 -1.519729 25% 0.264708 -0.889820 -0.845100 -0.414891 -0.132619 -0.699887 50% 0.902505 -0.603065 -0.638757 -0.289223 0.297693 -0.405371 75% 1.155211 0.021188 -0.190907 0.712183 0.768554 0.117107 max 1.623162 2.217712 1.624211 1.252059 1.273116 0.571998 6 7 8 9 ... 118 119 \\ count 10.000000 10.000000 10.000000 10.000000 ... 10.000000 10.000000 mean 0.423672 0.325855 0.258114 -0.199072 ... -0.165618 -0.378175 std 1.304160 0.815934 0.938925 1.344848 ... 1.135625 0.940863 min -1.440672 -1.068176 -1.280547 -2.819688 ... -1.567793 -1.604171 25% -0.535055 0.051598 -0.178302 -0.801871 ... -1.114392 -1.066492 50% 0.502826 0.369002 0.230939 -0.576039 ... -0.114789 -0.587483 75% 1.227700 0.899518 0.740881 0.820789 ... 0.602045 0.160254 max 2.462891 1.440098 1.828760 1.866570 ... 1.854828 1.386001 120 121 122 123 124 125 \\ count 10.000000 10.000000 10.000000 10.000000 10.000000 10.000000 mean -0.557321 0.402392 -0.070746 -0.770201 0.594842 0.572671 std 1.128886 0.899030 0.757537 1.044478 0.956856 0.883374 min -3.131332 -0.907885 -1.296398 -2.159384 -1.244177 -0.462607 25% -0.834223 -0.059528 -0.222124 -1.332558 0.234198 -0.008799 50% -0.613761 0.117478 -0.109676 -0.984450 0.656855 0.466357 75% 0.040239 1.136383 0.416972 -0.285216 1.246513 0.772441 max 1.087999 1.757588 1.022053 1.486507 2.010775 2.162550 126 127 count 10.000000 10.000000 mean 0.226993 -0.064205 std 1.463179 0.602277 min -1.731004 -0.865115 25% -0.951169 -0.418553 50% 0.174763 -0.112277 75% 0.773072 0.353951 max 2.991696 0.804881 [8 rows x 128 columns] Observable Tensor 'item_obs' with shape torch.Size([4, 64]) 0 1 2 3 4 5 6 \\ count 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 mean 0.287015 -0.180256 -0.239000 0.169168 0.159036 0.385342 -1.142672 std 1.339318 1.603530 0.722772 0.473407 0.392562 1.327739 0.566069 min -1.138152 -2.212473 -1.051363 -0.538771 -0.330795 -0.517352 -1.770297 25% -0.558802 -0.990083 -0.745828 0.132031 -0.006671 -0.485835 -1.397787 50% 0.170810 -0.012201 -0.154058 0.385432 0.174086 -0.125969 -1.199654 75% 1.016628 0.797626 0.352770 0.422569 0.339793 0.745208 -0.944538 max 1.944591 1.515852 0.403479 0.444577 0.618768 2.310656 -0.401083 7 8 9 ... 54 55 56 \\ count 4.000000 4.000000 4.000000 ... 4.000000 4.000000 4.000000 mean 0.581071 -0.169341 0.076562 ... 0.055457 -0.002887 -0.160406 std 0.972295 0.978922 1.116274 ... 0.777132 0.903879 1.140101 min -0.596834 -1.309131 -1.563906 ... -0.481757 -0.997574 -1.721709 25% -0.025344 -0.718815 -0.153971 ... -0.442894 -0.340660 -0.631280 50% 0.745386 -0.177989 0.514336 ... -0.240767 -0.105541 0.117918 75% 1.351801 0.371485 0.744870 ... 0.257583 0.232232 0.588793 max 1.430348 0.987744 0.841483 ... 1.185118 1.197110 0.844249 57 58 59 60 61 62 63 count 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 4.000000 mean 0.149579 0.199678 0.088542 -0.356379 1.004674 0.095064 -0.548665 std 0.963564 0.744614 1.170228 0.833992 0.559029 0.912057 0.730697 min -0.760765 -0.419252 -1.038935 -0.989042 0.442226 -0.989018 -1.445138 25% -0.268040 -0.383280 -0.604213 -0.970008 0.592259 -0.492793 -0.790356 50% -0.075941 0.036190 -0.142981 -0.611959 0.966522 0.230826 -0.546745 75% 0.341678 0.619148 0.549774 0.001670 1.378937 0.818683 -0.305054 max 1.510964 1.145585 1.679067 0.787444 1.643426 0.907622 0.343970 [8 rows x 64 columns] Observable Tensor 'session_obs' with shape torch.Size([500, 10]) 0 1 2 3 4 5 \\ count 500.000000 500.000000 500.000000 500.000000 500.000000 500.000000 mean -0.025211 -0.018355 -0.002907 0.091295 -0.061911 -0.046364 std 0.976283 1.029875 0.959884 0.968500 1.020114 1.010222 min -2.642895 -3.091050 -3.572037 -2.406249 -3.147900 -3.357277 25% -0.745162 -0.685578 -0.636044 -0.629955 -0.754234 -0.732924 50% -0.018775 0.017807 -0.018642 0.112322 -0.090321 -0.070502 75% 0.652438 0.646001 0.601829 0.722870 0.640275 0.652521 max 3.044069 3.191774 2.521059 2.695970 3.166039 2.714594 6 7 8 9 count 500.000000 500.000000 500.000000 500.000000 mean 0.000907 0.001370 0.070499 -0.007936 std 1.015561 1.032878 1.036212 0.936091 min -2.677915 -3.489751 -2.953354 -2.424499 25% -0.679291 -0.671086 -0.582997 -0.681405 50% 0.002569 -0.009368 0.087901 0.010856 75% 0.703671 0.732814 0.737692 0.618773 max 2.528283 3.259835 2.827300 2.492085 Observable Tensor 'price_obs' with shape torch.Size([500, 4, 12]) device=cpu","title":"dataset.summary()"},{"location":"data_management/#datasetnum_users-items-sessions","text":"You can use the num_{users, items, sessions} attribute to obtain the number of users, items, and sessions, they are determined automatically from the {user, item, session}_obs tensors provided while initializing the dataset object. Note : the print =: operator requires Python3.8 or higher, you can remove =: if you are using an earlier copy of Python. print ( f ' { dataset . num_users =:} ' ) print ( f ' { dataset . num_items =:} ' ) print ( f ' { dataset . num_sessions =:} ' ) print ( f ' { len ( dataset ) =:} ' ) dataset.num_users=10 dataset.num_items=4 dataset.num_sessions=500 len(dataset)=10000","title":"dataset.num_{users, items, sessions}"},{"location":"data_management/#datasetclone","text":"The ChoiceDataset offers a clone method allow you to make copy of the dataset, you can modify the cloned dataset arbitrarily without changing the original dataset. # clone print ( dataset . item_index [: 10 ]) dataset_cloned = dataset . clone () dataset_cloned . item_index = 99 * torch . ones ( num_sessions ) print ( dataset_cloned . item_index [: 10 ]) print ( dataset . item_index [: 10 ]) # does not change the original dataset. tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1]) tensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.]) tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])","title":"dataset.clone()"},{"location":"data_management/#datasettocuda-and-dataset_check_device_consistency","text":"One key advantage of the torch_choice and bemb is their compatibility with GPUs, you can easily move tensors in a ChoiceDataset object between host memory (i.e., cpu memory) and device memory (i.e., GPU memory) using dataset.to() method. Please note that the following code runs only if your machine has a compatible GPU and GPU-compatible version of PyTorch installed. Similarly, one can move data to host-memory using dataset.to('cpu') . The dataset also provides a dataset._check_device_consistency() method to check if all tensors are on the same device. If we only move the label to cpu without moving other tensors, this will result in an error message. # move to device print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset = dataset . to ( 'cuda' ) print ( f ' { dataset . device =:} ' ) print ( f ' { dataset . item_index . device =:} ' ) print ( f ' { dataset . user_index . device =:} ' ) print ( f ' { dataset . session_index . device =:} ' ) dataset.device=cpu dataset.device=cpu dataset.user_index.device=cpu dataset.session_index.device=cpu dataset.device=cuda:0 dataset.item_index.device=cuda:0 dataset.user_index.device=cuda:0 dataset.session_index.device=cuda:0 dataset . _check_device_consistency () # # NOTE: this cell will result errors, this is intentional. dataset . item_index = dataset . item_index . to ( 'cpu' ) dataset . _check_device_consistency () --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-56-40d626c6d436> in <module> 1 # # NOTE: this cell will result errors, this is intentional. 2 dataset.item_index = dataset.item_index.to('cpu') ----> 3 dataset._check_device_consistency() ~/Development/torch-choice/torch_choice/data/choice_dataset.py in _check_device_consistency(self) 180 devices.append(val.device) 181 if len(set(devices)) > 1: --> 182 raise Exception(f'Found tensors on different devices: {set(devices)}.', 183 'Use dataset.to() method to align devices.') 184 Exception: (\"Found tensors on different devices: {device(type='cuda', index=0), device(type='cpu')}.\", 'Use dataset.to() method to align devices.') # create dictionary inputs for model.forward() # collapse to a dictionary object. print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12])","title":"dataset.to('cuda') and dataset._check_device_consistency()."},{"location":"data_management/#subset-method","text":"One can use dataset[indices] with indices as an integer-valued tensor or array to get the corresponding rows of the dataset. The example code block below queries the 6256-th, 4119-th, 453-th, 5520-th, and 1877-th row of the dataset object. The item_index , user_index , session_index of the resulted subset will be different from the original dataset, but other tensors will be the same. # __getitem__ to get batch. # pick 5 random sessions as the mini-batch. dataset = dataset . to ( 'cpu' ) indices = torch . Tensor ( np . random . choice ( len ( dataset ), size = 5 , replace = False )) . long () print ( indices ) subset = dataset [ indices ] print ( dataset ) print ( subset ) # print_dict_shape(subset.x_dict) # assert torch.all(dataset.x_dict['price_obs'][indices, :, :] == subset.x_dict['price_obs']) # assert torch.all(dataset.item_index[indices] == subset.item_index) tensor([1118, 976, 1956, 290, 8283]) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) ChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) The subset method internally creates a copy of the datasets so that any modification applied on the subset will not be reflected on the original dataset. The researcher can feel free to do in-place modification to the subset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) subset . item_index += 1 # modifying the batch does not change the original dataset. print ( subset . item_index ) print ( dataset . item_index [ indices ]) tensor([0, 1, 0, 0, 0]) tensor([0, 1, 0, 0, 0]) tensor([1, 2, 1, 1, 1]) tensor([0, 1, 0, 0, 0]) print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) subset . item_obs += 1 print ( subset . item_obs [ 0 , 0 ]) print ( dataset . item_obs [ 0 , 0 ]) tensor(-1.5811) tensor(-1.5811) tensor(-0.5811) tensor(-1.5811) print ( id ( subset . item_index )) print ( id ( dataset . item_index [ indices ])) 140339656298640 140339656150528","title":"Subset method"},{"location":"data_management/#using-pytorch-dataloader-for-the-training-loop","text":"The ChoiceDataset object natively support batch samplers from PyTorch. For demonstration purpose, we turned off the shuffling option. from torch.utils.data.sampler import BatchSampler , SequentialSampler , RandomSampler shuffle = False # for demonstration purpose. batch_size = 32 # Create sampler. sampler = BatchSampler ( RandomSampler ( dataset ) if shuffle else SequentialSampler ( dataset ), batch_size = batch_size , drop_last = False ) dataloader = torch . utils . data . DataLoader ( dataset , sampler = sampler , num_workers = 1 , collate_fn = lambda x : x [ 0 ], pin_memory = ( dataset . device == 'cpu' )) print ( f ' { item_obs . shape =:} ' ) item_obs_all = item_obs . view ( 1 , num_items , - 1 ) . expand ( len ( dataset ), - 1 , - 1 ) item_obs_all = item_obs_all . to ( dataset . device ) item_index_all = item_index . to ( dataset . device ) print ( f ' { item_obs_all . shape =:} ' ) item_obs.shape=torch.Size([4, 64]) item_obs_all.shape=torch.Size([10000, 4, 64]) for i , batch in enumerate ( dataloader ): first , last = i * batch_size , min ( len ( dataset ), ( i + 1 ) * batch_size ) idx = torch . arange ( first , last ) assert torch . all ( item_obs_all [ idx , :, :] == batch . x_dict [ 'item_obs' ]) assert torch . all ( item_index_all [ idx ] == batch . item_index ) batch . x_dict [ 'item_obs' ] . shape torch.Size([16, 4, 64]) print_dict_shape ( dataset . x_dict ) dict.user_obs.shape=torch.Size([10000, 4, 128]) dict.item_obs.shape=torch.Size([10000, 4, 64]) dict.session_obs.shape=torch.Size([10000, 4, 10]) dict.price_obs.shape=torch.Size([10000, 4, 12]) dataset . __len__ () 10000","title":"Using Pytorch dataloader for the training loop."},{"location":"data_management/#chaining-multiple-datasets-jointdataset-examples","text":"dataset1 = dataset . clone () dataset2 = dataset . clone () joint_dataset = JointDataset ( the_dataset = dataset1 , another_dataset = dataset2 ) joint_dataset JointDataset with 2 sub-datasets: ( the_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) another_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu) )","title":"Chaining Multiple Datasets: JointDataset Examples"},{"location":"easy_data_management/","text":"Easy Data Wrapper Tutorial The data construction covered in the Data Management tutorial might be too complicated for users without prior experience in PyTorch. This tutorial offers a helper class to wrap the dataset, all the user needs to know is (1) loading data-frames to Python, Pandas provides one-line solution to loading various types of data files including CSV, TSV, Stata, and Excel. (2) basic usage of pandas. We aim to make this tutorial as self-contained as possible, so you don't need to be worried if you haven't went through the Data Management tutorial . But we invite you to go through that tutorial to obtain a more in-depth understanding of data management in this project. Author: Tianyu Du Date: May. 20, 2022 Update: Jul. 9, 2022 __author__ = 'Tianyu Du' Let's import a few necessary packages. import pandas as pd import torch from torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper References and Background for Stata Users This tutorial aim to show how to manage choice datasets using the torch-choice package, we will follow the Stata documentation here to offer a seamless experience for the user to transfer prior knowledge in other packages to our package. From Stata Documentation : Choice models (CM) are models for data with outcomes that are choices. The choices are selected by a decision maker, such as a person or a business (i.e., the user ), from a set of possible alternatives (i.e., the items ). For instance, we could model choices made by consumers who select a breakfast cereal from several different brands. Or we could model choices made by businesses who chose whether to buy TV, radio, Internet, or newspaper advertising. Models for choice data come in two varieties\u2014models for discrete choices and models for rank-ordered alternatives. When each individual selects a single alternative, say, he or she purchases one box of cereal, the data are discrete choice data. When each individual ranks the choices, say, he or she orders cereals from most favorite to least favorite, the data are rank-ordered data. Stata has commands for fitting both discrete choice models and rank-ordered models. Our torch-choice package handles the discrete choice models in the Stata document above. Motivations In the following parts, we demonstrate how to convert a long-format data (e.g., the one used in Stata) to the ChoiceDataset data format expected by our package. But first, Why do we want another ChoiceDataset object instead of just one long-format data-frame? In earlier versions of Stata, we can only have one single data-frame loaded in memory, this would introduce memory error especially when teh dataset is large. For example, you have a dataset of a million decisions recorded, each consists of four items, and each item has a persistent built quality that stay the same in all observations. The Stata format would make a million copy of these variables, which is very inefficient. We would need to collect a couple of data-frames as the essential pieces to build our ChoiceDataset . Don't worry, as soon as you have the data-frames ready, the EasyDataWrapper helper class would take care of the rest. We call a single statistical observation a \"purchase record\" and use this terminology throughout the tutorial. df = pd . read_stata ( 'https://www.stata-press.com/data/r17/carchoice.dta' ) We load the artificial dataset from the Stata website. Here we borrow the description of dataset reported from the describe command in Stata. Contains data from https://www.stata-press.com/data/r17/carchoice.dta Observations: 3,160 Car choice data Variables: 6 30 Jul 2020 14:58 --------------------------------------------------------------------------------------------------------------------------------------------------- Variable Storage Display Value name type format label Variable label --------------------------------------------------------------------------------------------------------------------------------------------------- consumerid int %8.0g ID of individual consumer car byte %9.0g nation Nationality of car purchase byte %10.0g Indicator of car purchased gender byte %9.0g gender Gender: 0 = Female, 1 = Male income float %9.0g Income (in $1,000) dealers byte %9.0g No. of dealerships in community --------------------------------------------------------------------------------------------------------------------------------------------------- Sorted by: consumerid car In this dataset, the first four rows with consumerid == 1 corresponds to the first purchasing record , it means the consumer with ID 1 was making the decision among four types of cars (i.e., items ) and chose American car (since the purchase == 1 in that row of American car). Even though there were four types of cars, not all of them were available all the time. For example, for the purchase record by consumer with ID 4, only American, Japanese, and European cars were available (note that there is no row in the dataset with consumerid == 4 and car == 'Korean' , this indicates unavailability of a certain item.) df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 Male 46.699997 9 1 1 Japanese 0 Male 46.699997 11 2 1 European 0 Male 46.699997 5 3 1 Korean 0 Male 46.699997 1 4 2 American 1 Male 26.100000 10 5 2 Japanese 0 Male 26.100000 7 6 2 European 0 Male 26.100000 2 7 2 Korean 0 Male 26.100000 1 8 3 American 0 Male 32.700001 8 9 3 Japanese 1 Male 32.700001 6 10 3 European 0 Male 32.700001 2 11 4 American 1 Female 49.199997 5 12 4 Japanese 0 Female 49.199997 4 13 4 European 0 Female 49.199997 3 14 5 American 0 Male 24.299999 8 15 5 Japanese 0 Male 24.299999 3 16 5 European 1 Male 24.299999 3 17 6 American 1 Female 39.000000 10 18 6 Japanese 0 Female 39.000000 6 19 6 European 0 Female 39.000000 1 20 7 American 0 Male 33.000000 10 21 7 Japanese 0 Male 33.000000 6 22 7 European 1 Male 33.000000 4 23 7 Korean 0 Male 33.000000 1 24 8 American 1 Male 20.299999 6 25 8 Japanese 0 Male 20.299999 5 26 8 European 0 Male 20.299999 3 27 9 American 0 Male 38.000000 9 28 9 Japanese 1 Male 38.000000 9 29 9 European 0 Male 38.000000 2 Components of the Consumer Choice Modelling Problem We begin with essential component of the consumer choice modelling problem. Walking through these components should help you understand what kind of data our models are working on. Purchasing Record Each row (record) of the dataset is called a purchasing record , which includes who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record (i.e., who bought what at where and when ). Items and Categories To begin with, there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) under our consideration. Further, the researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) , it is easy to verify that \\[ \\bigcup_{c \\in \\{1, 2, \\dots, C\\}} I_c = \\{1, 2, \\dots I\\} \\] If the researcher does not wish to model different categories differently, the researcher can simply put all items in one single category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so that all items belong to the same category. Note : since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24 inch LCD monitor). Raw item names can be encoded easily with sklearn.preprocessing.OrdinalEncoder . Users Each purchaing reocrd is naturally associated with an user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) ( who ) as well. Sessions Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, when the data came from a single store over the period of a year. In this case, the notion of where does not matter that much, and session \\(s\\) is simply the date of purchase. Another example is that we have the purchase record from different stores, the session \\(s\\) can be defined as a pair of (date, store) instead. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all rows of the dataset. To summarize, each purchasing record \\(b\\) in the dataset is characterized by a user-session-item tuple \\((u, s, i)\\) . When there are multiple items bought by the same user in the same session, there will be multiple rows in the dataset with the same \\((u, s)\\) corresponding to the same receipt. Format the Dataset a Little Bit The wrapper we built requires several data frames, providing the correct information is all we need to do in this tutorial, the data wrapper will handle the construction of ChoiceDataset for you. Note : The dataset in this tutorial is a bit over-simplified, we only have one purchase record for each user in each session, so the consumerid column identifies all of the user, the session, and the purchase record (because we have different dealers for the same type of car, we define each purchase record of it's session instead of assigning all purchase records to the same session). That is, we have a single user makes a single choice in each single session. The main dataset should contain the following columns: purchase_record_column : a column identifies purchase record (also called case in Stata syntax). this tutorial, the consumerid column is the identifier. For example, the first 4 rows of the dataset (see above) has consumerid == 1 , this means we should look at the first 4 rows together and they constitute the first purchase record. item_name_column : a column identifies names of items , which is car in the dataset above. This column provides information above the availability as well. As mentioned above, there is no column with car == Korean in the fourth purchasing record ( consumerid == 4 ), so we know that Korean car was not available that time. choice_column : a column identifies the choice made by the consumer in each purchase record, which is the purchase column in our example. Exactly one row per purchase record (i.e., rows with the same values in purchase_record_column ) should have 1, while the values are zeros for all other rows. user_index_column : a optional column identifies the user making the choice, which is also consumerid in our case. session_index_column : a optional column identifies the session of the choice, which is also consumerid in our case. As you might have noticed, the consumerid column in the data-frame identifies multiple pieces of information: purchase_record , user_index , and session_index . This is not a mistake, you can use the same column in df to supply multiple pieces of information. df . gender . value_counts ( dropna = False ) Male 2283 Female 854 NaN 23 Name: gender, dtype: int64 The only modification required is to convert gender (with values of Male , Female or NaN ) to integers because PyTorch does not handle strings. For simplicity, we will assume all NaN gender to be Female (you should not do this in a real application!) and re-define the gender variable as \\(\\mathbb{I}\\{\\texttt{gender} == \\texttt{Male}\\}\\) . # we change gender to binary 0/1 because pytorch doesn't handle strings. df [ 'gender' ] = ( df [ 'gender' ] == 'Male' ) . astype ( int ) Now the gender column contains only binary integers. df . gender . value_counts ( dropna = False ) 1 2283 0 877 Name: gender, dtype: int64 The data-frame looks like the following right now: df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 Adding the Observables The next step is to identify observables going into the model. Specifically, we would want to add: 1. gender and income as user-specific observables 2. and dealers as (session, item)-specific observable. Such observables are called price observables in our setting, why? because price is the most typical (session, item)-specific observable. Method 1: Adding Observables by Extracting Columns of the Dataset As you can see, gender , income and dealers are already encompassed in df , the first way to add observables is simply mentioning these columns while initializing the EasyDatasetWrapper object. You can supply a list of names of columns to each of {user, item, session, price}_observable_columns keyword argument. For example, we use user_observable_columns=['gender', 'income'] to inform the EasyDatasetWrapper that we wish to derive user-specific observables from the gender and income columns of df . Also, we inform the EasyDatasetWrapper that we want to derive (session, item)-specific (i.e., price observable) by specifying price_observable_columns=['dealers'] . Since our package leverages GPU-acceleration, it is necessary to supply the device on which the dataset should reside. The EasyDatasetWrapper also takes a device keyword, which can be either 'cpu' or an appropriate CUDA device. if torch . cuda . is_available (): device = 'cuda' # use GPU if available else : device = 'cpu' # use CPU otherwise data_1 = EasyDatasetWrapper ( main_data = df , # TODO: better naming convention? Need to discuss. # after discussion, we add it to the default value # in the data wrapper class. # these are just names. purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # it can be derived from columns of the dataframe or supplied as user_observable_columns = [ 'gender' , 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. The dataset has a summary() method, which can be used to print out the summary of the dataset. data_1 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) You can access the ChoiceDataset object constructed by calling the data.choice_dataset object. data_1 . choice_dataset ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Method 2: Adding Observables as Data Frames We can also construct data frames and use data frames to supply different observables. This is useful when you have a large dataset, for example, if there are many purchase records for the same user (to be concrete, say \\(U\\) users and \\(N\\) purchase records for each user, resulting \\(U \\times N\\) total purchase records). Using a single data-frame requires a lot of memory: you need to store \\(U \\times N\\) entires of user genders in total. However, user genders should be persistent across all purchasing records, if we use a separate data-frame mapping user index to gender of the user, we only need to store \\(U\\) entries (i.e., one for each user) of gender information. Similarly, the long-format data requires storing each piece of item-specific information for number of purchase records times, which leads to inefficient usage of disk/memory space. How Do Observable Data-frame Look Like? Our package natively support the following four types of observables: User Observables : user-specific observables (e.g., gender and income) should (1) have length equal to the number of unique users in the dataset (885 here); (2) contains a column named as user_index_column ( user_index_column is a variable, the actual column name should be the value of variable user_index_column ! E.g., here the user observable data-frame should have a column named 'consumerid' ); (3) the user observable can have any number of other columns beside the user_index_column column, each of them corresponding to a user-specific observable. For example, a data-frame containing \\(X\\) user-specific observables has shape (num_users, X + 1) . Item Observables item-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique items in the dataset (4 here); (2) contain a column named as item_index_column ( item_index_column is a variable, the actual column name should be the value of variable item_index_column ! E.g., here the item observable data-frame should have a column named 'car' ); (3) the item observable can have any number of other columns beside the item_index_column column, each of them corresponding to a item-specific observable. Session Observable session-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique sessions in the dataset; (2) contain a column named as session_index_column ( session_index_column is a variable, the actual column name should be the value of variable session_index_column ! E.g., here the session observable data-frame should have a column named 'consumerid' ); (3) the session observable can have any number of other columns beside the session_index_column column, each of them corresponding to a session-specific observable. Price Observables (session, item)-specific observables (e.g., dealers) should be (1) contains a column named as session_index_column (e.g., consumerid in our example) and a column named as item_name_column (e.g., car in our example), (2) the price observable can have any number of other columns beside the session_index_column and item_name_column columns, each of them corresponding to a (session, item)-specific observable. For example, a data-frame containing \\(X\\) (session, item)-specific observables has shape (num_sessions, num_items, X + 2) . We encourage the reader to review the Data Management Tutorial for more details on types of observables. Suggested Procedure of Storing and Loading Data Suppose SESSION_INDEX column in df_main is the index of the session, ALTERNATIVES column is the index of the car. For user-specific observables, you should have a CSV on disk with columns { consumerid , var_1 , var_2 , ...}. You load the user-specific dataset as user_obs = pd.read_csv(..., index='consumerid') . Let's first construct the data frame for user genders first. gender = df . groupby ( 'consumerid' )[ 'gender' ] . first () . reset_index () The user-observable data-frame contains a column of user IDs (the consumerid column), this column should have exactly the same name as the column containing user indices. Otherwise, the wrapper won't know which column corresponds to user IDs and which column corresponds to variables. gender . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender 0 1 1 1 2 1 2 3 1 3 4 0 4 5 1 Then, let's build the data-frame for user-specific income variables. income = df . groupby ( 'consumerid' )[ 'income' ] . first () . reset_index () income . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid income 0 1 46.699997 1 2 26.100000 2 3 32.700001 3 4 49.199997 4 5 24.299999 Please note that we can have multiple observables contained in the same data-frame as well. gender_and_income = df . groupby ( 'consumerid' )[[ 'gender' , 'income' ]] . first () . reset_index () gender_and_income .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender income 0 1 1 46.699997 1 2 1 26.100000 2 3 1 32.700001 3 4 0 49.199997 4 5 1 24.299999 ... ... ... ... 880 881 1 45.700001 881 882 1 69.800003 882 883 0 45.599998 883 884 1 20.900000 884 885 1 30.600000 885 rows \u00d7 3 columns The price observable data-frame contains two columns identifying session (i.e., the consumerid column) and item (i.e., the car column). The session index column should have exactly the same name as the session index column in df and the column indexing columns should have exactly the same name as the item-name-column in df . dealers = df [[ 'consumerid' , 'car' , 'dealers' ]] dealers . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car dealers 0 1 American 9 1 1 Japanese 11 2 1 European 5 3 1 Korean 1 4 2 American 10 Build Datasets using EasyDatasetWrapper with Observables as Data-Frames We can observables as data-frames using {user, item, session, price}_observable_data keyword arguments. data_2 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender , 'income' : income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. # Use summary to see what's inside the data wrapper. data_2 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Alternatively, we can supply user income and gender as a single dataframe, instead of user_gender and user_income tensors, now the constructed ChoiceDataset contains a single user_gender_and_income tensor with shape (885, 2) encompassing both income and gender of users. data_3 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender_and_income' : gender_and_income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_3 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender_and_income=[885, 2], price_dealers=[885, 4, 1], device=cuda:0) Method 3: Mixing Method 1 and Method 2 The EasyDataWrapper also support supplying observables as a mixture of above methods. The following example supplies gender user observable as a data-frame but income and dealers as column names. data_4 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender }, user_observable_columns = [ 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_4 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Sanity Checks Lastly, let's check choice datasets constructed via different methods are actually the same. The == method of choice datasets will compare the non-NAN entries of all tensors in datasets. print ( data_1 . choice_dataset == data_2 . choice_dataset ) print ( data_1 . choice_dataset == data_4 . choice_dataset ) True True For data_3 , we have income and gender combined: data_3 . choice_dataset . user_gender_and_income == torch . cat ([ data_1 . choice_dataset . user_gender , data_1 . choice_dataset . user_income ], dim = 1 ) tensor([[True, True], [True, True], [True, True], ..., [True, True], [True, True], [True, True]], device='cuda:0') Now let's compare what's inside the data structure and our raw data. bought_raw = df [ df [ 'purchase' ] == 1 ][ 'car' ] . values bought_data = list () encoder = { 0 : 'American' , 1 : 'European' , 2 : 'Japanese' , 3 : 'Korean' } for b in data_1 . choice_dataset . item_index : bought_data . append ( encoder [ float ( b )]) all ( bought_raw == bought_data ) True Then, let's compare the income and gender variable contained in the dataset. X = df . groupby ( 'consumerid' )[ 'income' ] . first () . values Y = data_1 . choice_dataset . user_income . cpu () . numpy () . squeeze () all ( X == Y ) True X = df . groupby ( 'consumerid' )[ 'gender' ] . first () . values Y = data_1 . choice_dataset . user_gender . cpu () . numpy () . squeeze () all ( X == Y ) True Lastly, let's compare the price_dealer variable. Since there are NAN-values in it for unavailable cars, we can't not use all(X == Y) to compare them. We will first fill NANs values with -1 and then compare resulted data-frames. # rearrange columns to align it with the internal encoding scheme of the data wrapper. X = df . pivot ( 'consumerid' , 'car' , 'dealers' )[[ 'American' , 'European' , 'Japanese' , 'Korean' ]] X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } car American European Japanese Korean consumerid 1 9.0 5.0 11.0 1.0 2 10.0 2.0 7.0 1.0 3 8.0 2.0 6.0 NaN 4 5.0 3.0 4.0 NaN 5 8.0 3.0 3.0 NaN ... ... ... ... ... 881 8.0 2.0 10.0 NaN 882 8.0 6.0 8.0 1.0 883 9.0 5.0 8.0 1.0 884 12.0 4.0 10.0 NaN 885 10.0 4.0 5.0 NaN 885 rows \u00d7 4 columns Y = data_1 . choice_dataset . price_dealers . squeeze ( dim =- 1 ) Y tensor([[ 9., 5., 11., 1.], [10., 2., 7., 1.], [ 8., 2., 6., nan], ..., [ 9., 5., 8., 1.], [12., 4., 10., nan], [10., 4., 5., nan]], device='cuda:0') print ( X . fillna ( - 1 ) . values == torch . nan_to_num ( Y , - 1 ) . cpu () . numpy ()) [[ True True True True] [ True True True True] [ True True True True] ... [ True True True True] [ True True True True] [ True True True True]] This concludes our tutorial on building the dataset, if you wish more in-depth understanding of the data structure, please refer to the Data Management Tutorial .","title":"Easy Data Management and Stata Users"},{"location":"easy_data_management/#easy-data-wrapper-tutorial","text":"The data construction covered in the Data Management tutorial might be too complicated for users without prior experience in PyTorch. This tutorial offers a helper class to wrap the dataset, all the user needs to know is (1) loading data-frames to Python, Pandas provides one-line solution to loading various types of data files including CSV, TSV, Stata, and Excel. (2) basic usage of pandas. We aim to make this tutorial as self-contained as possible, so you don't need to be worried if you haven't went through the Data Management tutorial . But we invite you to go through that tutorial to obtain a more in-depth understanding of data management in this project. Author: Tianyu Du Date: May. 20, 2022 Update: Jul. 9, 2022 __author__ = 'Tianyu Du' Let's import a few necessary packages. import pandas as pd import torch from torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper","title":"Easy Data Wrapper Tutorial"},{"location":"easy_data_management/#references-and-background-for-stata-users","text":"This tutorial aim to show how to manage choice datasets using the torch-choice package, we will follow the Stata documentation here to offer a seamless experience for the user to transfer prior knowledge in other packages to our package. From Stata Documentation : Choice models (CM) are models for data with outcomes that are choices. The choices are selected by a decision maker, such as a person or a business (i.e., the user ), from a set of possible alternatives (i.e., the items ). For instance, we could model choices made by consumers who select a breakfast cereal from several different brands. Or we could model choices made by businesses who chose whether to buy TV, radio, Internet, or newspaper advertising. Models for choice data come in two varieties\u2014models for discrete choices and models for rank-ordered alternatives. When each individual selects a single alternative, say, he or she purchases one box of cereal, the data are discrete choice data. When each individual ranks the choices, say, he or she orders cereals from most favorite to least favorite, the data are rank-ordered data. Stata has commands for fitting both discrete choice models and rank-ordered models. Our torch-choice package handles the discrete choice models in the Stata document above.","title":"References and Background for Stata Users"},{"location":"easy_data_management/#motivations","text":"In the following parts, we demonstrate how to convert a long-format data (e.g., the one used in Stata) to the ChoiceDataset data format expected by our package. But first, Why do we want another ChoiceDataset object instead of just one long-format data-frame? In earlier versions of Stata, we can only have one single data-frame loaded in memory, this would introduce memory error especially when teh dataset is large. For example, you have a dataset of a million decisions recorded, each consists of four items, and each item has a persistent built quality that stay the same in all observations. The Stata format would make a million copy of these variables, which is very inefficient. We would need to collect a couple of data-frames as the essential pieces to build our ChoiceDataset . Don't worry, as soon as you have the data-frames ready, the EasyDataWrapper helper class would take care of the rest. We call a single statistical observation a \"purchase record\" and use this terminology throughout the tutorial. df = pd . read_stata ( 'https://www.stata-press.com/data/r17/carchoice.dta' ) We load the artificial dataset from the Stata website. Here we borrow the description of dataset reported from the describe command in Stata. Contains data from https://www.stata-press.com/data/r17/carchoice.dta Observations: 3,160 Car choice data Variables: 6 30 Jul 2020 14:58 --------------------------------------------------------------------------------------------------------------------------------------------------- Variable Storage Display Value name type format label Variable label --------------------------------------------------------------------------------------------------------------------------------------------------- consumerid int %8.0g ID of individual consumer car byte %9.0g nation Nationality of car purchase byte %10.0g Indicator of car purchased gender byte %9.0g gender Gender: 0 = Female, 1 = Male income float %9.0g Income (in $1,000) dealers byte %9.0g No. of dealerships in community --------------------------------------------------------------------------------------------------------------------------------------------------- Sorted by: consumerid car In this dataset, the first four rows with consumerid == 1 corresponds to the first purchasing record , it means the consumer with ID 1 was making the decision among four types of cars (i.e., items ) and chose American car (since the purchase == 1 in that row of American car). Even though there were four types of cars, not all of them were available all the time. For example, for the purchase record by consumer with ID 4, only American, Japanese, and European cars were available (note that there is no row in the dataset with consumerid == 4 and car == 'Korean' , this indicates unavailability of a certain item.) df . head ( 30 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 Male 46.699997 9 1 1 Japanese 0 Male 46.699997 11 2 1 European 0 Male 46.699997 5 3 1 Korean 0 Male 46.699997 1 4 2 American 1 Male 26.100000 10 5 2 Japanese 0 Male 26.100000 7 6 2 European 0 Male 26.100000 2 7 2 Korean 0 Male 26.100000 1 8 3 American 0 Male 32.700001 8 9 3 Japanese 1 Male 32.700001 6 10 3 European 0 Male 32.700001 2 11 4 American 1 Female 49.199997 5 12 4 Japanese 0 Female 49.199997 4 13 4 European 0 Female 49.199997 3 14 5 American 0 Male 24.299999 8 15 5 Japanese 0 Male 24.299999 3 16 5 European 1 Male 24.299999 3 17 6 American 1 Female 39.000000 10 18 6 Japanese 0 Female 39.000000 6 19 6 European 0 Female 39.000000 1 20 7 American 0 Male 33.000000 10 21 7 Japanese 0 Male 33.000000 6 22 7 European 1 Male 33.000000 4 23 7 Korean 0 Male 33.000000 1 24 8 American 1 Male 20.299999 6 25 8 Japanese 0 Male 20.299999 5 26 8 European 0 Male 20.299999 3 27 9 American 0 Male 38.000000 9 28 9 Japanese 1 Male 38.000000 9 29 9 European 0 Male 38.000000 2","title":"Motivations"},{"location":"easy_data_management/#components-of-the-consumer-choice-modelling-problem","text":"We begin with essential component of the consumer choice modelling problem. Walking through these components should help you understand what kind of data our models are working on.","title":"Components of the Consumer Choice Modelling Problem"},{"location":"easy_data_management/#purchasing-record","text":"Each row (record) of the dataset is called a purchasing record , which includes who bought what at when and where . Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record (i.e., who bought what at where and when ).","title":"Purchasing Record"},{"location":"easy_data_management/#items-and-categories","text":"To begin with, there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) under our consideration. Further, the researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) , it is easy to verify that \\[ \\bigcup_{c \\in \\{1, 2, \\dots, C\\}} I_c = \\{1, 2, \\dots I\\} \\] If the researcher does not wish to model different categories differently, the researcher can simply put all items in one single category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so that all items belong to the same category. Note : since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24 inch LCD monitor). Raw item names can be encoded easily with sklearn.preprocessing.OrdinalEncoder .","title":"Items and Categories"},{"location":"easy_data_management/#users","text":"Each purchaing reocrd is naturally associated with an user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) ( who ) as well.","title":"Users"},{"location":"easy_data_management/#sessions","text":"Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, when the data came from a single store over the period of a year. In this case, the notion of where does not matter that much, and session \\(s\\) is simply the date of purchase. Another example is that we have the purchase record from different stores, the session \\(s\\) can be defined as a pair of (date, store) instead. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all rows of the dataset. To summarize, each purchasing record \\(b\\) in the dataset is characterized by a user-session-item tuple \\((u, s, i)\\) . When there are multiple items bought by the same user in the same session, there will be multiple rows in the dataset with the same \\((u, s)\\) corresponding to the same receipt.","title":"Sessions"},{"location":"easy_data_management/#format-the-dataset-a-little-bit","text":"The wrapper we built requires several data frames, providing the correct information is all we need to do in this tutorial, the data wrapper will handle the construction of ChoiceDataset for you. Note : The dataset in this tutorial is a bit over-simplified, we only have one purchase record for each user in each session, so the consumerid column identifies all of the user, the session, and the purchase record (because we have different dealers for the same type of car, we define each purchase record of it's session instead of assigning all purchase records to the same session). That is, we have a single user makes a single choice in each single session. The main dataset should contain the following columns: purchase_record_column : a column identifies purchase record (also called case in Stata syntax). this tutorial, the consumerid column is the identifier. For example, the first 4 rows of the dataset (see above) has consumerid == 1 , this means we should look at the first 4 rows together and they constitute the first purchase record. item_name_column : a column identifies names of items , which is car in the dataset above. This column provides information above the availability as well. As mentioned above, there is no column with car == Korean in the fourth purchasing record ( consumerid == 4 ), so we know that Korean car was not available that time. choice_column : a column identifies the choice made by the consumer in each purchase record, which is the purchase column in our example. Exactly one row per purchase record (i.e., rows with the same values in purchase_record_column ) should have 1, while the values are zeros for all other rows. user_index_column : a optional column identifies the user making the choice, which is also consumerid in our case. session_index_column : a optional column identifies the session of the choice, which is also consumerid in our case. As you might have noticed, the consumerid column in the data-frame identifies multiple pieces of information: purchase_record , user_index , and session_index . This is not a mistake, you can use the same column in df to supply multiple pieces of information. df . gender . value_counts ( dropna = False ) Male 2283 Female 854 NaN 23 Name: gender, dtype: int64 The only modification required is to convert gender (with values of Male , Female or NaN ) to integers because PyTorch does not handle strings. For simplicity, we will assume all NaN gender to be Female (you should not do this in a real application!) and re-define the gender variable as \\(\\mathbb{I}\\{\\texttt{gender} == \\texttt{Male}\\}\\) . # we change gender to binary 0/1 because pytorch doesn't handle strings. df [ 'gender' ] = ( df [ 'gender' ] == 'Male' ) . astype ( int ) Now the gender column contains only binary integers. df . gender . value_counts ( dropna = False ) 1 2283 0 877 Name: gender, dtype: int64 The data-frame looks like the following right now: df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10","title":"Format the Dataset a Little Bit"},{"location":"easy_data_management/#adding-the-observables","text":"The next step is to identify observables going into the model. Specifically, we would want to add: 1. gender and income as user-specific observables 2. and dealers as (session, item)-specific observable. Such observables are called price observables in our setting, why? because price is the most typical (session, item)-specific observable.","title":"Adding the Observables"},{"location":"easy_data_management/#method-1-adding-observables-by-extracting-columns-of-the-dataset","text":"As you can see, gender , income and dealers are already encompassed in df , the first way to add observables is simply mentioning these columns while initializing the EasyDatasetWrapper object. You can supply a list of names of columns to each of {user, item, session, price}_observable_columns keyword argument. For example, we use user_observable_columns=['gender', 'income'] to inform the EasyDatasetWrapper that we wish to derive user-specific observables from the gender and income columns of df . Also, we inform the EasyDatasetWrapper that we want to derive (session, item)-specific (i.e., price observable) by specifying price_observable_columns=['dealers'] . Since our package leverages GPU-acceleration, it is necessary to supply the device on which the dataset should reside. The EasyDatasetWrapper also takes a device keyword, which can be either 'cpu' or an appropriate CUDA device. if torch . cuda . is_available (): device = 'cuda' # use GPU if available else : device = 'cpu' # use CPU otherwise data_1 = EasyDatasetWrapper ( main_data = df , # TODO: better naming convention? Need to discuss. # after discussion, we add it to the default value # in the data wrapper class. # these are just names. purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # it can be derived from columns of the dataframe or supplied as user_observable_columns = [ 'gender' , 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. The dataset has a summary() method, which can be used to print out the summary of the dataset. data_1 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) You can access the ChoiceDataset object constructed by calling the data.choice_dataset object. data_1 . choice_dataset ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0)","title":"Method 1: Adding Observables by Extracting Columns of the Dataset"},{"location":"easy_data_management/#method-2-adding-observables-as-data-frames","text":"We can also construct data frames and use data frames to supply different observables. This is useful when you have a large dataset, for example, if there are many purchase records for the same user (to be concrete, say \\(U\\) users and \\(N\\) purchase records for each user, resulting \\(U \\times N\\) total purchase records). Using a single data-frame requires a lot of memory: you need to store \\(U \\times N\\) entires of user genders in total. However, user genders should be persistent across all purchasing records, if we use a separate data-frame mapping user index to gender of the user, we only need to store \\(U\\) entries (i.e., one for each user) of gender information. Similarly, the long-format data requires storing each piece of item-specific information for number of purchase records times, which leads to inefficient usage of disk/memory space.","title":"Method 2: Adding Observables as Data Frames"},{"location":"easy_data_management/#how-do-observable-data-frame-look-like","text":"Our package natively support the following four types of observables: User Observables : user-specific observables (e.g., gender and income) should (1) have length equal to the number of unique users in the dataset (885 here); (2) contains a column named as user_index_column ( user_index_column is a variable, the actual column name should be the value of variable user_index_column ! E.g., here the user observable data-frame should have a column named 'consumerid' ); (3) the user observable can have any number of other columns beside the user_index_column column, each of them corresponding to a user-specific observable. For example, a data-frame containing \\(X\\) user-specific observables has shape (num_users, X + 1) . Item Observables item-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique items in the dataset (4 here); (2) contain a column named as item_index_column ( item_index_column is a variable, the actual column name should be the value of variable item_index_column ! E.g., here the item observable data-frame should have a column named 'car' ); (3) the item observable can have any number of other columns beside the item_index_column column, each of them corresponding to a item-specific observable. Session Observable session-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique sessions in the dataset; (2) contain a column named as session_index_column ( session_index_column is a variable, the actual column name should be the value of variable session_index_column ! E.g., here the session observable data-frame should have a column named 'consumerid' ); (3) the session observable can have any number of other columns beside the session_index_column column, each of them corresponding to a session-specific observable. Price Observables (session, item)-specific observables (e.g., dealers) should be (1) contains a column named as session_index_column (e.g., consumerid in our example) and a column named as item_name_column (e.g., car in our example), (2) the price observable can have any number of other columns beside the session_index_column and item_name_column columns, each of them corresponding to a (session, item)-specific observable. For example, a data-frame containing \\(X\\) (session, item)-specific observables has shape (num_sessions, num_items, X + 2) . We encourage the reader to review the Data Management Tutorial for more details on types of observables.","title":"How Do Observable Data-frame Look Like?"},{"location":"easy_data_management/#suggested-procedure-of-storing-and-loading-data","text":"Suppose SESSION_INDEX column in df_main is the index of the session, ALTERNATIVES column is the index of the car. For user-specific observables, you should have a CSV on disk with columns { consumerid , var_1 , var_2 , ...}. You load the user-specific dataset as user_obs = pd.read_csv(..., index='consumerid') . Let's first construct the data frame for user genders first. gender = df . groupby ( 'consumerid' )[ 'gender' ] . first () . reset_index () The user-observable data-frame contains a column of user IDs (the consumerid column), this column should have exactly the same name as the column containing user indices. Otherwise, the wrapper won't know which column corresponds to user IDs and which column corresponds to variables. gender . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender 0 1 1 1 2 1 2 3 1 3 4 0 4 5 1 Then, let's build the data-frame for user-specific income variables. income = df . groupby ( 'consumerid' )[ 'income' ] . first () . reset_index () income . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid income 0 1 46.699997 1 2 26.100000 2 3 32.700001 3 4 49.199997 4 5 24.299999 Please note that we can have multiple observables contained in the same data-frame as well. gender_and_income = df . groupby ( 'consumerid' )[[ 'gender' , 'income' ]] . first () . reset_index () gender_and_income .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid gender income 0 1 1 46.699997 1 2 1 26.100000 2 3 1 32.700001 3 4 0 49.199997 4 5 1 24.299999 ... ... ... ... 880 881 1 45.700001 881 882 1 69.800003 882 883 0 45.599998 883 884 1 20.900000 884 885 1 30.600000 885 rows \u00d7 3 columns The price observable data-frame contains two columns identifying session (i.e., the consumerid column) and item (i.e., the car column). The session index column should have exactly the same name as the session index column in df and the column indexing columns should have exactly the same name as the item-name-column in df . dealers = df [[ 'consumerid' , 'car' , 'dealers' ]] dealers . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } consumerid car dealers 0 1 American 9 1 1 Japanese 11 2 1 European 5 3 1 Korean 1 4 2 American 10","title":"Suggested Procedure of Storing and Loading Data"},{"location":"easy_data_management/#build-datasets-using-easydatasetwrapper-with-observables-as-data-frames","text":"We can observables as data-frames using {user, item, session, price}_observable_data keyword arguments. data_2 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender , 'income' : income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. # Use summary to see what's inside the data wrapper. data_2 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0) Alternatively, we can supply user income and gender as a single dataframe, instead of user_gender and user_income tensors, now the constructed ChoiceDataset contains a single user_gender_and_income tensor with shape (885, 2) encompassing both income and gender of users. data_3 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender_and_income' : gender_and_income }, price_observable_data = { 'dealers' : dealers }, device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_3 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender_and_income=[885, 2], price_dealers=[885, 4, 1], device=cuda:0)","title":"Build Datasets using EasyDatasetWrapper with Observables as Data-Frames"},{"location":"easy_data_management/#method-3-mixing-method-1-and-method-2","text":"The EasyDataWrapper also support supplying observables as a mixture of above methods. The following example supplies gender user observable as a data-frame but income and dealers as column names. data_4 = EasyDatasetWrapper ( main_data = df , purchase_record_column = 'consumerid' , choice_column = 'purchase' , item_name_column = 'car' , user_index_column = 'consumerid' , session_index_column = 'consumerid' , # above are the same as before, but we update the following. user_observable_data = { 'gender' : gender }, user_observable_columns = [ 'income' ], price_observable_columns = [ 'dealers' ], device = device ) Creating choice dataset from stata format data-frames... Note: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'} Finished Creating Choice Dataset. data_4 . summary () * purchase record index range: [1 2 3] ... [883 884 885] * Space of 4 items: 0 1 2 3 item name American European Japanese Korean * Number of purchase records/cases: 885. * Preview of main data frame: consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10 ... ... ... ... ... ... ... 3155 884 Japanese 1 1 20.900000 10 3156 884 European 0 1 20.900000 4 3157 885 American 1 1 30.600000 10 3158 885 Japanese 0 1 30.600000 5 3159 885 European 0 1 30.600000 4 [3160 rows x 6 columns] * Preview of ChoiceDataset: ChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], price_dealers=[885, 4, 1], device=cuda:0)","title":"Method 3: Mixing Method 1 and Method 2"},{"location":"easy_data_management/#sanity-checks","text":"Lastly, let's check choice datasets constructed via different methods are actually the same. The == method of choice datasets will compare the non-NAN entries of all tensors in datasets. print ( data_1 . choice_dataset == data_2 . choice_dataset ) print ( data_1 . choice_dataset == data_4 . choice_dataset ) True True For data_3 , we have income and gender combined: data_3 . choice_dataset . user_gender_and_income == torch . cat ([ data_1 . choice_dataset . user_gender , data_1 . choice_dataset . user_income ], dim = 1 ) tensor([[True, True], [True, True], [True, True], ..., [True, True], [True, True], [True, True]], device='cuda:0') Now let's compare what's inside the data structure and our raw data. bought_raw = df [ df [ 'purchase' ] == 1 ][ 'car' ] . values bought_data = list () encoder = { 0 : 'American' , 1 : 'European' , 2 : 'Japanese' , 3 : 'Korean' } for b in data_1 . choice_dataset . item_index : bought_data . append ( encoder [ float ( b )]) all ( bought_raw == bought_data ) True Then, let's compare the income and gender variable contained in the dataset. X = df . groupby ( 'consumerid' )[ 'income' ] . first () . values Y = data_1 . choice_dataset . user_income . cpu () . numpy () . squeeze () all ( X == Y ) True X = df . groupby ( 'consumerid' )[ 'gender' ] . first () . values Y = data_1 . choice_dataset . user_gender . cpu () . numpy () . squeeze () all ( X == Y ) True Lastly, let's compare the price_dealer variable. Since there are NAN-values in it for unavailable cars, we can't not use all(X == Y) to compare them. We will first fill NANs values with -1 and then compare resulted data-frames. # rearrange columns to align it with the internal encoding scheme of the data wrapper. X = df . pivot ( 'consumerid' , 'car' , 'dealers' )[[ 'American' , 'European' , 'Japanese' , 'Korean' ]] X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } car American European Japanese Korean consumerid 1 9.0 5.0 11.0 1.0 2 10.0 2.0 7.0 1.0 3 8.0 2.0 6.0 NaN 4 5.0 3.0 4.0 NaN 5 8.0 3.0 3.0 NaN ... ... ... ... ... 881 8.0 2.0 10.0 NaN 882 8.0 6.0 8.0 1.0 883 9.0 5.0 8.0 1.0 884 12.0 4.0 10.0 NaN 885 10.0 4.0 5.0 NaN 885 rows \u00d7 4 columns Y = data_1 . choice_dataset . price_dealers . squeeze ( dim =- 1 ) Y tensor([[ 9., 5., 11., 1.], [10., 2., 7., 1.], [ 8., 2., 6., nan], ..., [ 9., 5., 8., 1.], [12., 4., 10., nan], [10., 4., 5., nan]], device='cuda:0') print ( X . fillna ( - 1 ) . values == torch . nan_to_num ( Y , - 1 ) . cpu () . numpy ()) [[ True True True True] [ True True True True] [ True True True True] ... [ True True True True] [ True True True True] [ True True True True]] This concludes our tutorial on building the dataset, if you wish more in-depth understanding of the data structure, please refer to the Data Management Tutorial .","title":"Sanity Checks"},{"location":"install/","text":"Installation This page will guide you through the installation procedure of torch-choice and bemb . There are two parts of this project: the torch_choice library consisting of data management modules, logit and nested-logit models for consumer choice modelling. For researchers wish to use the Bayesian Embedding (BEMB) model, they need to install an additional bemb package, which was built on the top of torch_choice . Note Since this project is still on its pre-release stage and subject to changes, we have not uploaded our packages to PIP or CONDA. Researchers need to install these packages from Github source code. Option 1: Install using Source Code from Github Repository To install torch_choice and bemb from source, 1. Clone the repositories of both torch_choice and bemb to your local machine. 2. Install required dependencies (e.g., PyTorch and PyTorch-Lightning). 3. For each of repositories, run python3 ./setup.py develop to add the package to your Python environment. 4. Check installation by running python3 -c \"import torch_choice; print(torch_choice.__version__)\" . 5. Check installation by running python3 -c \"import bemb; print(bemb.__version__)\" . Option 2: Install using Pip The torch-choice is available on PIP now here ! You can use pip install torch-choice to install it. Note : We are working on publishing BEMB to PIP.","title":"Installation"},{"location":"install/#installation","text":"This page will guide you through the installation procedure of torch-choice and bemb . There are two parts of this project: the torch_choice library consisting of data management modules, logit and nested-logit models for consumer choice modelling. For researchers wish to use the Bayesian Embedding (BEMB) model, they need to install an additional bemb package, which was built on the top of torch_choice . Note Since this project is still on its pre-release stage and subject to changes, we have not uploaded our packages to PIP or CONDA. Researchers need to install these packages from Github source code.","title":"Installation"},{"location":"install/#option-1-install-using-source-code-from-github-repository","text":"To install torch_choice and bemb from source, 1. Clone the repositories of both torch_choice and bemb to your local machine. 2. Install required dependencies (e.g., PyTorch and PyTorch-Lightning). 3. For each of repositories, run python3 ./setup.py develop to add the package to your Python environment. 4. Check installation by running python3 -c \"import torch_choice; print(torch_choice.__version__)\" . 5. Check installation by running python3 -c \"import bemb; print(bemb.__version__)\" .","title":"Option 1: Install using Source Code from Github Repository"},{"location":"install/#option-2-install-using-pip","text":"The torch-choice is available on PIP now here ! You can use pip install torch-choice to install it. Note : We are working on publishing BEMB to PIP.","title":"Option 2: Install using Pip"},{"location":"intro/","text":"Introduction Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] where, \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] here \\(X\\) , \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\) , \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\) . When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\) , as \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] where \\(\\epsilon\\) is an unobserved random error term. If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\) , this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\) , as shown by McFadden , and as often studied in Econometrics. Package We implement a fully flexible setup, where we allow 1. coefficients ( \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) , \\(\\dots\\) ) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\) ), item-specific (i.e., \\(\\alpha=\\alpha_i\\) ), session-specific (i.e., \\(\\alpha=\\alpha_t\\) ), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\) ). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables ( \\(X\\) , \\(Y\\) , \\(\\dots\\) ) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\) This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\) , which enables modelling rich structures. As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial) \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3) MNIST classification (Upcoming Detailed Tutorial) \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1) This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form. Notes on Encodings Since we will be using PyTorch to train our model, we accept user and item identities with integer values from [0, 1, .. num_users - 1] and [0, 1, .. num_items - 1] instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor) or any other encoding. The user is responsible to encode user indices, item indices and session indices, wherever appliable (some setups do not require session and/or user identifiers) Raw item/user/session names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). Here is an example of encoding generic item names to integers using sklearn.preprocessing.LabelEncoder : from sklearn.preprocessing import LabelEncoder enc = LabelEncoder () raw_items = [ 'Macbook Laptop' , 'Dell 24-inch Monitor' , 'Orange' , 'Apple (Fruit)' ] encoded_items = enc . fit_transform ( raw_items ) print ( encoded_items ) # output: [2 1 3 0] # for each 0 <= i <= 3, enc.classes_[i] reveals the raw name of item encoded to i. print ( enc . classes_ ) # output: ['Apple (Fruit)' 'Dell 24-inch Monitor' 'Macbook Laptop' 'Orange'] # For example, the first entry of enc.classes_ is 'Apple (Fruit)', this means 'Apple (Fruit)' was encoded to 0 in this process. # The last item in the `raw_item` list was 'Apple (Fruit)', and the last item in the `encoded_item` list was 0 as we expected. Components of the Choice Modeling Problem For the rest of this tutorial, we will consider retail supermarket choice as the concrete setting. We aim to predict users' choices while choosing between multiple available items, e.g., which brand of milk the user will purchase in the supermarket. We begin with essential components of the choice modeling problem. Walking through these components helps understand what kind of data our models are working on. Purchase Record A purchase record is a record describing who bought what at when and where . Let \\(B\\) denote the number of purchase records in the dataset (i.e., number of rows/observation of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record. What : Items and Categories To begin with, suppose there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) . The researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) . It's easy to see that the union of all \\(I_c\\) is the entire set of items \\(\\{1, 2, \\dots I\\}\\) . Suppose the researcher does not wish to model different categories differently. In that case, the researcher can put all items in one category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so all items belong to the same category. For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(i_b \\in \\{1,2,\\dots,I\\}\\) saying which item was chosen in this record. Who : Users The agent which makes choices in our setting is a user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) as well. For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(u_b \\in \\{1,2,\\dots,I\\}\\) describing which user was making the decision. When and Where : Sessions Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, we had the purchase record from five different stores for every day in 2021, then a session \\(s\\) is defined as a pair of (date, storeID) , and there are \\(5 \\times 365\\) sessions in total. In another example, suppose the data came from a single store for over a year. In this case, the notion of where is immaterial, and session \\(s\\) is simply the date of purchase. The notion of sessions can be more flexible than just date and location. For example, if we want to distinguish between online ordering and in-store purchasing, we can define the session as (date, storeID, IsOnlineOrdering). The session variable serves as a tool for the researcher to split the dataset; the usefulness of the session will be more evident after introducing observables (features) later. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all dataset rows. Putting Everything Together To summarize, each purchase record \\(b \\in \\{1, 2, \\dots, B\\}\\) in the dataset is characterized by a user-session-item tuple \\((u_b, s_b, i_b)\\) . The totality of \\(B\\) purchase records consists of the dataset we are modeling. When the same user buys multiple items in the same session, the dataset will have multiple purchase records with the same \\((u, s)\\) corresponding to the same receipt. In this case, the modeling assumption is that the user buys at most one item from each category available to choose from. Item Availability It is not necessarily that all items are available in every session; items can get out of stock in particular sessions. To handle these cases, the researcher can optionally provide a boolean tensor \\(A \\in \\{\\texttt{True}, \\texttt{False}\\}^{S\\times I}\\) to indicate which items are available for purchase in each session. \\(A_{s, i} = \\texttt{True}\\) if and only if item \\(i\\) was available in session \\(s\\) . While predicting the purchase probabilities, the model sets the probability for these unavailable items to zero and normalizes probabilities among available items. If the item availability is not provided, the model assumes all items are available in all sessions. Observables Next, let's talk about observables. This is the same as a feature in machine learning literature, commonly denoted using \\(X\\) . The researcher can incorporate observables of, for example, users and/or items into the model. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. itemsession_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{itemsession}}\\) , item-session observables are values depending on both session and item such as the price of item. These can also be called price_obs usersession_obs \\(\\in \\mathbb{R}^{S \\times U \\times K_{usersession}}\\) , user-session observables are values depending on both session and user such as the income of the user. Please note that we consider these four types as definitions of observable types. For example, whenever a variable is user-specific, then we call it an user_obs . This package defines observables in the above way so that the package can easily track the variation of variables and handle these observable tensors correctly. A Toy Example Suppose we have a dataset of purchase history from two stores (Store A and B) on two dates (Sep 16 and 17), both stores sell {apple, banana, orange} ( num_items=3 ) and there are three people came to those stores between Sep 16 and 17. user_index session_index item_index Amy Sep-17-2021-Store-A banana Ben Sep-17-2021-Store-B apple Ben Sep-16-2021-Store-A orange Charlie Sep-16-2021-Store-B apple Charlie Sep-16-2021-Store-B orange NOTE : For demonstration purposes, the example dataset has user_index , session_index and item_index as strings, they should be consecutive integers in actual production. One can easily convert them to integers using sklearn.preprocessing.LabelEncoder . In the example above, - user_index=[0,1,1,2,2] (with encoding 0=Amy, 1=Ben, 2=Charlie ), - session_index=[0,1,2,3,3] (with encoding 0=Sep-17-2021-Store-A, 1=Sep-17-2021-Store-B, 2=Sep-16-2021-Store-A, 3=Sep-16-2021-Store-B ), - item_index=[0,1,2,1,2] (with encoding 0=banana, 1=apple, 2=orange ). Suppose we believe people's purchase decision depends on the nutrition levels of these fruits; suppose apple has the highest nutrition level and banana has the lowest one, we can add item_obs=[[1.5], [12.0], [3.3]] \\(\\in \\mathbb{R}^{3\\times 1}\\) . The shape of this tensor is number-of-items by number-of-observable. NOTE : If someone went to one store and bought multiple items (e.g., Charlie bought both apple and orange at Store B on Sep-16), we include them as separate rows in the dataset and model them independently. Models The torch-choice library provides two models, the conditional logit model and the nested logit model, for modeling the dataset. Each model takes in \\((u_b, s_b)\\) altogether with observables and outputs a probability of purchasing each \\(\\tilde{i} \\in \\{1, 2, \\dots, I\\}\\) , denoted as \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) . In cases when not all items are available, the model sets the probability of unavailable items to zero and normalizes probabilities among available items. \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) is the predicted probability of purchasing item \\(\\tilde{i}\\) in session \\(s_b\\) by user \\(u_b\\) given all information we know. Model parameters are trained using gradient descent algorithm and the loss function is the negative log-likelihood of the model \\(-\\sum_{b=1}^B \\log(\\hat{p}_{u_b, s_b, i_b})\\) . The major difference among models lies in the way they compute predicted probabilities.","title":"Introduction"},{"location":"intro/#introduction","text":"Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] where, \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] here \\(X\\) , \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\) , \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\) . When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\) , as \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] where \\(\\epsilon\\) is an unobserved random error term. If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\) , this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\) , as shown by McFadden , and as often studied in Econometrics.","title":"Introduction"},{"location":"intro/#package","text":"We implement a fully flexible setup, where we allow 1. coefficients ( \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) , \\(\\dots\\) ) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\) ), item-specific (i.e., \\(\\alpha=\\alpha_i\\) ), session-specific (i.e., \\(\\alpha=\\alpha_t\\) ), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\) ). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables ( \\(X\\) , \\(Y\\) , \\(\\dots\\) ) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\) This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\) , which enables modelling rich structures. As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial) \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3) MNIST classification (Upcoming Detailed Tutorial) \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1) This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form.","title":"Package"},{"location":"intro/#notes-on-encodings","text":"Since we will be using PyTorch to train our model, we accept user and item identities with integer values from [0, 1, .. num_users - 1] and [0, 1, .. num_items - 1] instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor) or any other encoding. The user is responsible to encode user indices, item indices and session indices, wherever appliable (some setups do not require session and/or user identifiers) Raw item/user/session names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well). Here is an example of encoding generic item names to integers using sklearn.preprocessing.LabelEncoder : from sklearn.preprocessing import LabelEncoder enc = LabelEncoder () raw_items = [ 'Macbook Laptop' , 'Dell 24-inch Monitor' , 'Orange' , 'Apple (Fruit)' ] encoded_items = enc . fit_transform ( raw_items ) print ( encoded_items ) # output: [2 1 3 0] # for each 0 <= i <= 3, enc.classes_[i] reveals the raw name of item encoded to i. print ( enc . classes_ ) # output: ['Apple (Fruit)' 'Dell 24-inch Monitor' 'Macbook Laptop' 'Orange'] # For example, the first entry of enc.classes_ is 'Apple (Fruit)', this means 'Apple (Fruit)' was encoded to 0 in this process. # The last item in the `raw_item` list was 'Apple (Fruit)', and the last item in the `encoded_item` list was 0 as we expected.","title":"Notes on Encodings"},{"location":"intro/#components-of-the-choice-modeling-problem","text":"For the rest of this tutorial, we will consider retail supermarket choice as the concrete setting. We aim to predict users' choices while choosing between multiple available items, e.g., which brand of milk the user will purchase in the supermarket. We begin with essential components of the choice modeling problem. Walking through these components helps understand what kind of data our models are working on.","title":"Components of the Choice Modeling Problem"},{"location":"intro/#purchase-record","text":"A purchase record is a record describing who bought what at when and where . Let \\(B\\) denote the number of purchase records in the dataset (i.e., number of rows/observation of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record.","title":"Purchase Record"},{"location":"intro/#what-items-and-categories","text":"To begin with, suppose there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) . The researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\) . Let \\(I_c\\) denote the collection of items in category \\(c\\) . It's easy to see that the union of all \\(I_c\\) is the entire set of items \\(\\{1, 2, \\dots I\\}\\) . Suppose the researcher does not wish to model different categories differently. In that case, the researcher can put all items in one category: \\(I_1 = \\{1, 2, \\dots I\\}\\) , so all items belong to the same category. For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(i_b \\in \\{1,2,\\dots,I\\}\\) saying which item was chosen in this record.","title":"What: Items and Categories"},{"location":"intro/#who-users","text":"The agent which makes choices in our setting is a user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) as well. For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\) , there is a corresponding \\(u_b \\in \\{1,2,\\dots,I\\}\\) describing which user was making the decision.","title":"Who: Users"},{"location":"intro/#when-and-where-sessions","text":"Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\) . For example, we had the purchase record from five different stores for every day in 2021, then a session \\(s\\) is defined as a pair of (date, storeID) , and there are \\(5 \\times 365\\) sessions in total. In another example, suppose the data came from a single store for over a year. In this case, the notion of where is immaterial, and session \\(s\\) is simply the date of purchase. The notion of sessions can be more flexible than just date and location. For example, if we want to distinguish between online ordering and in-store purchasing, we can define the session as (date, storeID, IsOnlineOrdering). The session variable serves as a tool for the researcher to split the dataset; the usefulness of the session will be more evident after introducing observables (features) later. If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all dataset rows.","title":"When and Where: Sessions"},{"location":"intro/#putting-everything-together","text":"To summarize, each purchase record \\(b \\in \\{1, 2, \\dots, B\\}\\) in the dataset is characterized by a user-session-item tuple \\((u_b, s_b, i_b)\\) . The totality of \\(B\\) purchase records consists of the dataset we are modeling. When the same user buys multiple items in the same session, the dataset will have multiple purchase records with the same \\((u, s)\\) corresponding to the same receipt. In this case, the modeling assumption is that the user buys at most one item from each category available to choose from.","title":"Putting Everything Together"},{"location":"intro/#item-availability","text":"It is not necessarily that all items are available in every session; items can get out of stock in particular sessions. To handle these cases, the researcher can optionally provide a boolean tensor \\(A \\in \\{\\texttt{True}, \\texttt{False}\\}^{S\\times I}\\) to indicate which items are available for purchase in each session. \\(A_{s, i} = \\texttt{True}\\) if and only if item \\(i\\) was available in session \\(s\\) . While predicting the purchase probabilities, the model sets the probability for these unavailable items to zero and normalizes probabilities among available items. If the item availability is not provided, the model assumes all items are available in all sessions.","title":"Item Availability"},{"location":"intro/#observables","text":"Next, let's talk about observables. This is the same as a feature in machine learning literature, commonly denoted using \\(X\\) . The researcher can incorporate observables of, for example, users and/or items into the model. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables. user_obs \\(\\in \\mathbb{R}^{U\\times K_{user}}\\) : user observables such as user age. item_obs \\(\\in \\mathbb{R}^{I\\times K_{item}}\\) : item observables such as item quality. session_obs \\(\\in \\mathbb{R}^{S \\times K_{session}}\\) : session observable such as whether the purchase was made on weekdays. itemsession_obs \\(\\in \\mathbb{R}^{S \\times I \\times K_{itemsession}}\\) , item-session observables are values depending on both session and item such as the price of item. These can also be called price_obs usersession_obs \\(\\in \\mathbb{R}^{S \\times U \\times K_{usersession}}\\) , user-session observables are values depending on both session and user such as the income of the user. Please note that we consider these four types as definitions of observable types. For example, whenever a variable is user-specific, then we call it an user_obs . This package defines observables in the above way so that the package can easily track the variation of variables and handle these observable tensors correctly.","title":"Observables"},{"location":"intro/#a-toy-example","text":"Suppose we have a dataset of purchase history from two stores (Store A and B) on two dates (Sep 16 and 17), both stores sell {apple, banana, orange} ( num_items=3 ) and there are three people came to those stores between Sep 16 and 17. user_index session_index item_index Amy Sep-17-2021-Store-A banana Ben Sep-17-2021-Store-B apple Ben Sep-16-2021-Store-A orange Charlie Sep-16-2021-Store-B apple Charlie Sep-16-2021-Store-B orange NOTE : For demonstration purposes, the example dataset has user_index , session_index and item_index as strings, they should be consecutive integers in actual production. One can easily convert them to integers using sklearn.preprocessing.LabelEncoder . In the example above, - user_index=[0,1,1,2,2] (with encoding 0=Amy, 1=Ben, 2=Charlie ), - session_index=[0,1,2,3,3] (with encoding 0=Sep-17-2021-Store-A, 1=Sep-17-2021-Store-B, 2=Sep-16-2021-Store-A, 3=Sep-16-2021-Store-B ), - item_index=[0,1,2,1,2] (with encoding 0=banana, 1=apple, 2=orange ). Suppose we believe people's purchase decision depends on the nutrition levels of these fruits; suppose apple has the highest nutrition level and banana has the lowest one, we can add item_obs=[[1.5], [12.0], [3.3]] \\(\\in \\mathbb{R}^{3\\times 1}\\) . The shape of this tensor is number-of-items by number-of-observable. NOTE : If someone went to one store and bought multiple items (e.g., Charlie bought both apple and orange at Store B on Sep-16), we include them as separate rows in the dataset and model them independently.","title":"A Toy Example"},{"location":"intro/#models","text":"The torch-choice library provides two models, the conditional logit model and the nested logit model, for modeling the dataset. Each model takes in \\((u_b, s_b)\\) altogether with observables and outputs a probability of purchasing each \\(\\tilde{i} \\in \\{1, 2, \\dots, I\\}\\) , denoted as \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) . In cases when not all items are available, the model sets the probability of unavailable items to zero and normalizes probabilities among available items. \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) is the predicted probability of purchasing item \\(\\tilde{i}\\) in session \\(s_b\\) by user \\(u_b\\) given all information we know. Model parameters are trained using gradient descent algorithm and the loss function is the negative log-likelihood of the model \\(-\\sum_{b=1}^B \\log(\\hat{p}_{u_b, s_b, i_b})\\) . The major difference among models lies in the way they compute predicted probabilities.","title":"Models"},{"location":"nested_logit_model_house_cooling/","text":"Random Utility Model (RUM) Part II: Nested Logit Model Author: Tianyu Du The package implements the nested logit model as well, which allows researchers to model choices as a two-stage process: the user first picks a nest of purchase and then picks the item from the chosen nest that generates the most utility. Examples here are modified from Exercise 2: Nested logit model by Kenneth Train and Yves Croissant . The House Cooling (HC) dataset from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California. The dataset is small and serve as a demonstration of the nested logit model. The alternatives are: Gas central heat with cooling gcc , Electric central resistence heat with cooling ecc , Electric room resistence heat with cooling erc , Electric heat pump, which provides cooling also hpc , Gas central heat without cooling gc , Electric central resistence heat without cooling ec , Electric room resistence heat without cooling er . Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative. The variables are: depvar gives the name of the chosen alternative, ich.alt are the installation cost for the heating portion of the system, icca is the installation cost for cooling och.alt are the operating cost for the heating portion of the system occa is the operating cost for cooling income is the annual income of the household Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling. Nested Logit Model: Background The following code block provides an example initialization of the NestedLogitModel (please refer to examples below for details). model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) The nested logit model decompose the utility of choosing item \\(i\\) into the (1) item-specific values and (2) nest specify values. For simplicity, suppose item \\(i\\) belongs to nest \\(k \\in \\{1, \\dots, K\\}\\) : \\(i \\in B_k\\) . \\[ U_{uit} = W_{ukt} + Y_{uit} \\] Where both \\(W\\) and \\(Y\\) are estimated using linear models from as in the conditional logit model. The log-likelihood for user \\(u\\) to choose item \\(i\\) at time/session \\(t\\) decomposes into the item-level likelihood and nest-level likelihood. \\[ \\log P(i \\mid u, t) = \\log P(i \\mid u, t, B_k) + \\log P(k \\mid u, t) \\\\ = \\log \\left(\\frac{\\exp(Y_{uit}/\\lambda_k)}{\\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)}\\right) + \\log \\left( \\frac{\\exp(W_{ukt} + \\lambda_k I_{ukt})}{\\sum_{\\ell=1}^K \\exp(W_{u\\ell t} + \\lambda_\\ell I_{u\\ell t})}\\right) \\] The inclusive value of nest \\(k\\) , \\(I_{ukt}\\) is defined as \\(\\log \\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)\\) , which is the expected utility from choosing the best alternative from nest \\(k\\) . The nest_to_item keyword defines a dictionary of the mapping \\(k \\mapsto B_k\\) , where keys of nest_to_item are integer \\(k\\) 's and nest_to_item[k] is a list consisting of IDs of items in \\(B_k\\) . The {nest, item}_coef_variation_dict provides specification to \\(W_{ukt}\\) and \\(Y_{uit}\\) respectively, torch_choice allows for empty nest level models by providing an empty dictionary (in this case, \\(W_{ukt} = \\epsilon_{ukt}\\) ) since the inclusive value term \\(\\lambda_k I_{ukt}\\) will be used to model the choice over nests. However, by specifying an empty second stage model ( \\(Y_{uit} = \\epsilon_{uit}\\) ), the nested logit model reduces to a conditional logit model of choices over nests. Hence, one should never use the NestedLogitModel class with an empty item-level model. Similar to the conditional logit model, {nest, item}_num_param_dict specify the dimension (number of observables to be multiplied with the coefficient) of coefficients. The above code initializes a simple model built upon item-time-specific observables \\(X_{it} \\in \\mathbb{R}^7\\) , \\[ Y_{uit} = \\beta^\\top X_{it} + \\epsilon_{uit} \\\\ W_{ukt} = \\epsilon_{ukt} \\] The research may wish to enfoce the elasiticity \\(\\lambda_k\\) to be constant across nests, setting shared_lambda=True enforces \\(\\lambda_k = \\lambda\\ \\forall k \\in [K]\\) . Load Essential Packages We firstly read essential packages for this tutorial. import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model.nested_logit_model import NestedLogitModel from torch_choice.utils.run_helper import run print ( torch . __version__ ) 2.0.0 We then select the appropriate device to run the model on, our package supports both CPU and GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) DEVICE = 'cuda' else : print ( 'Running tutorial on CPU' ) DEVICE = 'cpu' Running tutorial on CPU Load Datasets We firstly read the dataset for this tutorial, the csv file can be found at ./public_datasets/HC.csv . Alternatively, we load the dataset directly from the Github website. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/HC.csv' , index_col = 0 ) df = df . reset_index ( drop = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } depvar icca occa income ich och idx.id1 idx.id2 inc.room inc.cooling int.cooling cooling.modes room.modes 0 False 0.00 0.00 20 24.50 4.09 1 ec 0 0 0 False False 1 False 27.28 2.95 20 7.86 4.09 1 ecc 0 20 1 True False 2 False 0.00 0.00 20 7.37 3.85 1 er 20 0 0 False True 3 True 27.28 2.95 20 8.79 3.85 1 erc 20 20 1 True True 4 False 0.00 0.00 20 24.08 2.26 1 gc 0 0 0 False False The raw dataset is in a long-format (i.e., each row contains information of one item). df [ 'idx.id2' ] . value_counts () ec 250 ecc 250 er 250 erc 250 gc 250 gcc 250 hpc 250 Name: idx.id2, dtype: int64 # what was actually chosen. item_index = df [ df [ 'depvar' ] == True ] . sort_values ( by = 'idx.id1' )[ 'idx.id2' ] . reset_index ( drop = True ) item_names = [ 'ec' , 'ecc' , 'er' , 'erc' , 'gc' , 'gcc' , 'hpc' ] num_items = df [ 'idx.id2' ] . nunique () # cardinal encoder. encoder = dict ( zip ( item_names , range ( num_items ))) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) Because we will be training our model with PyTorch , we need to encode item names to integers (from 0 to 6). We do this manually in this exercise given the small amount of items, for more items, one can use sklearn.preprocessing.OrdinalEncoder to encode. Raw item names will be encoded as the following. encoder {'ec': 0, 'ecc': 1, 'er': 2, 'erc': 3, 'gc': 4, 'gcc': 5, 'hpc': 6} Nest Level Dataset We firstly construct the nest-level dataset, however, there is no observable that is constant within the same nest, so we don't need to include any observable tensor to the nest_dataset . All we need to do is adding the item_index (i.e., which item is chosen) to the dataset, so that nest_dataset knows the total number of choices made. # nest feature: no nest feature, all features are item-level. nest_dataset = ChoiceDataset ( item_index = item_index . clone ()) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Item Level Dataset For simplicity, we treat each purchasing record as its own session. Moreover, we treat all observables as price observables (i.e., varying by both session and item). Since there are 7 observables in total, the resulted price_obs has shape (250, 7, 7) corresponding to number_of_sessions by number_of_items by number_of_observables . # item feature. item_feat_cols = [ 'ich' , 'och' , 'icca' , 'occa' , 'inc.room' , 'inc.cooling' , 'int.cooling' ] price_obs = utils . pivot3d ( df , dim0 = 'idx.id1' , dim1 = 'idx.id2' , values = item_feat_cols ) price_obs . shape torch.Size([250, 7, 7]) Then, we construct the item level dataset by providing both item_index and price_obs . We move item_dataset to the appropriate device as well. This is only necessary if we are using GPU to accelerate the model. item_dataset = ChoiceDataset ( item_index = item_index , price_obs = price_obs ) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Finally, we chain the nest-level and item-level dataset into a single JointDataset . dataset = JointDataset ( nest = nest_dataset , item = item_dataset ) One can print the joint dataset to see its contents, and tensors contained in each of these sub-datasets. print ( dataset ) JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) Examples There are multiple ways to group 7 items into nests, different classification will result in different utility functions and estimations (see the background of nested logit models). We will demonstrate the usage of our package by presenting three different categorization schemes and corresponding model estimations. Example 1 In the first example, the model is specified to have the cooling alternatives {gcc, ecc, erc, hpc} in one nest and the non-cooling alternatives {gc, ec, er} in another nest. We create a nest_to_item dictionary to inform the model our categorization scheme. The dictionary should have keys ranging from 0 to number_of_nests - 1 , each integer corresponds to a nest. The value of each key is a list of item IDs in the nest, the encoding of item names should be exactly the same as in the construction of item_index . nest_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' , 'hpc' ], 1 : [ 'gc' , 'ec' , 'er' ]} # encode items to integers. for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) In this example, we have item [1, 3, 5, 6] in the first nest (i.e., the nest with ID 0 ) and the rest of items in the second nest (i.e., the nest with ID 1 ). print ( nest_to_item ) {0: [1, 3, 5, 6], 1: [0, 2, 4]} Next, let's create the NestedLogitModel class! The first thing to put in is the nest_to_item dictionary we just built. For nest_coef_variation_dict , nest_num_param_dict , since we don't have any nest-specific observables, we can simply put an empty dictionary there. Coefficients for all observables are constant across items, and there are 7 observables in total. As for shared_lambda=True , please refer to the background recap for nested logit model. model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) model = model . to ( DEVICE ) You can print the model to get summary information of the NestedLogitModel class. print ( model ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) NOTE : We are computing the standard errors using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with respect to model parameters. This leads to slight different results compared with R implementation. run ( model , dataset , num_epochs = 10000 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 1000: Log-likelihood=-179.78282165527344 Epoch 2000: Log-likelihood=-178.6439666748047 Epoch 3000: Log-likelihood=-178.45376586914062 Epoch 4000: Log-likelihood=-178.30226135253906 Epoch 5000: Log-likelihood=-178.19009399414062 Epoch 6000: Log-likelihood=-178.1377716064453 Epoch 7000: Log-likelihood=-178.1256866455078 Epoch 8000: Log-likelihood=-178.124755859375 Epoch 9000: Log-likelihood=-178.12757873535156 Epoch 10000: Log-likelihood=-178.12527465820312 ==================== model results ==================== Training Epochs: 10000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -178.12527465820312 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 0.585844 | 0.166706 | | item_price_obs[constant]_0 | -0.555026 | 0.144731 | | item_price_obs[constant]_1 | -0.858004 | 0.237756 | | item_price_obs[constant]_2 | -0.224923 | 0.110701 | | item_price_obs[constant]_3 | -1.08933 | 1.03791 | | item_price_obs[constant]_4 | -0.379122 | 0.100874 | | item_price_obs[constant]_5 | 0.249721 | 0.051977 | | item_price_obs[constant]_6 | -5.99982 | 4.83646 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) R Output Here we provide the output from mlogit model in R for estimation reference. Coefficient names reported are slightly different in Python and R , please use the following table for comparison. Please note that the lambda_weight_0 in Python (at the top) corresponds to the iv (inclusive value) in R (at the bottom). Orderings of coefficients for observables should be the same in both languages. Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", ## \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 11 iterations, 0h:0m:0s ## g'(-H)^-1g = 7.26E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.554878 0.144205 -3.8478 0.0001192 *** ## och -0.857886 0.255313 -3.3601 0.0007791 *** ## icca -0.225079 0.144423 -1.5585 0.1191212 ## occa -1.089458 1.219821 -0.8931 0.3717882 ## inc.room -0.378971 0.099631 -3.8038 0.0001425 *** ## inc.cooling 0.249575 0.059213 4.2149 2.499e-05 *** ## int.cooling -6.000415 5.562423 -1.0787 0.2807030 ## iv 0.585922 0.179708 3.2604 0.0011125 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -178.12 Example 2 The second example is similar to the first one, but we change the way we group items into different nests. Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.) nest_to_item = { 0 : [ 'ec' , 'ecc' , 'gc' , 'gcc' , 'hpc' ], 1 : [ 'er' , 'erc' ]} for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) # these two initializations are equivalent. model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) print ( model ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) print ( model ) model = model . to ( DEVICE ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) run <function torch_choice.utils.run_helper.run(model, dataset, dataset_test=None, batch_size=-1, learning_rate=0.01, num_epochs=5000, report_frequency=None, compute_std=True, return_final_training_log_likelihood=False)> run ( model , dataset , num_epochs = 50000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 5000: Log-likelihood=-180.560791015625 Epoch 10000: Log-likelihood=-180.80062866210938 Epoch 15000: Log-likelihood=-181.21275329589844 Epoch 20000: Log-likelihood=-180.3982696533203 Epoch 25000: Log-likelihood=-180.29925537109375 Epoch 30000: Log-likelihood=-182.28366088867188 Epoch 35000: Log-likelihood=-180.1341552734375 Epoch 40000: Log-likelihood=-182.2633514404297 Epoch 45000: Log-likelihood=-180.19305419921875 Epoch 50000: Log-likelihood=-180.68240356445312 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -180.68240356445312 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 1.63154 | 0.678117 | | item_price_obs[constant]_0 | -1.34966 | 0.531558 | | item_price_obs[constant]_1 | -2.17924 | 0.894518 | | item_price_obs[constant]_2 | -0.412631 | 0.243317 | | item_price_obs[constant]_3 | -2.61227 | 2.06289 | | item_price_obs[constant]_4 | -0.885769 | 0.337734 | | item_price_obs[constant]_5 | 0.49301 | 0.199931 | | item_price_obs[constant]_6 | -16.0524 | 9.32373 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) R Output You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", ## \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 10 iterations, 0h:0m:0s ## g'(-H)^-1g = 5.87E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -1.13818 0.54216 -2.0993 0.03579 * ## och -1.82532 0.93228 -1.9579 0.05024 . ## icca -0.33746 0.26934 -1.2529 0.21024 ## occa -2.06328 1.89726 -1.0875 0.27681 ## inc.room -0.75722 0.34292 -2.2081 0.02723 * ## inc.cooling 0.41689 0.20742 2.0099 0.04444 * ## int.cooling -13.82487 7.94031 -1.7411 0.08167 . ## iv 1.36201 0.65393 2.0828 0.03727 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.02 Example 3 For the third example, we now group items into three nests. Specifically, we have items gcc , ecc and erc in the first nest (nest 0 in the nest_to_item dictionary), hpc in a nest (nest 1 ) alone, and items gc , ec and er in the last nest (nest 2 ). nest_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' ], 1 : [ 'hpc' ], 2 : [ 'gc' , 'ec' , 'er' ]} for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset , num_epochs = 50000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 5000: Log-likelihood=-180.84153747558594 Epoch 10000: Log-likelihood=-182.17794799804688 Epoch 15000: Log-likelihood=-181.74029541015625 Epoch 20000: Log-likelihood=-182.3179931640625 Epoch 25000: Log-likelihood=-182.50352478027344 Epoch 30000: Log-likelihood=-181.481201171875 Epoch 35000: Log-likelihood=-181.8275604248047 Epoch 40000: Log-likelihood=-180.5753173828125 Epoch 45000: Log-likelihood=-182.4506072998047 Epoch 50000: Log-likelihood=-185.08358764648438 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -185.08358764648438 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 0.949264 | 0.19245 | | item_price_obs[constant]_0 | -0.852556 | 0.100724 | | item_price_obs[constant]_1 | -1.35082 | 0.188374 | | item_price_obs[constant]_2 | -0.248292 | 0.14014 | | item_price_obs[constant]_3 | -1.41068 | 1.2839 | | item_price_obs[constant]_4 | -0.581716 | 0.0771356 | | item_price_obs[constant]_5 | 0.336492 | 0.0656387 | | item_price_obs[constant]_6 | -10.5186 | 5.71641 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) R Output You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", ## \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), ## un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 8 iterations, 0h:0m:0s ## g'(-H)^-1g = 3.71E-08 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.838394 0.100546 -8.3384 < 2.2e-16 *** ## och -1.331598 0.252069 -5.2827 1.273e-07 *** ## icca -0.256131 0.145564 -1.7596 0.07848 . ## occa -1.405656 1.207281 -1.1643 0.24430 ## inc.room -0.571352 0.077950 -7.3297 2.307e-13 *** ## inc.cooling 0.311355 0.056357 5.5247 3.301e-08 *** ## int.cooling -10.413384 5.612445 -1.8554 0.06354 . ## iv 0.956544 0.180722 5.2929 1.204e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.26","title":"Nested Logit Model"},{"location":"nested_logit_model_house_cooling/#random-utility-model-rum-part-ii-nested-logit-model","text":"Author: Tianyu Du The package implements the nested logit model as well, which allows researchers to model choices as a two-stage process: the user first picks a nest of purchase and then picks the item from the chosen nest that generates the most utility. Examples here are modified from Exercise 2: Nested logit model by Kenneth Train and Yves Croissant . The House Cooling (HC) dataset from mlogit contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California. The dataset is small and serve as a demonstration of the nested logit model. The alternatives are: Gas central heat with cooling gcc , Electric central resistence heat with cooling ecc , Electric room resistence heat with cooling erc , Electric heat pump, which provides cooling also hpc , Gas central heat without cooling gc , Electric central resistence heat without cooling ec , Electric room resistence heat without cooling er . Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative. The variables are: depvar gives the name of the chosen alternative, ich.alt are the installation cost for the heating portion of the system, icca is the installation cost for cooling och.alt are the operating cost for the heating portion of the system occa is the operating cost for cooling income is the annual income of the household Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling.","title":"Random Utility Model (RUM) Part II: Nested Logit Model"},{"location":"nested_logit_model_house_cooling/#nested-logit-model-background","text":"The following code block provides an example initialization of the NestedLogitModel (please refer to examples below for details). model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) The nested logit model decompose the utility of choosing item \\(i\\) into the (1) item-specific values and (2) nest specify values. For simplicity, suppose item \\(i\\) belongs to nest \\(k \\in \\{1, \\dots, K\\}\\) : \\(i \\in B_k\\) . \\[ U_{uit} = W_{ukt} + Y_{uit} \\] Where both \\(W\\) and \\(Y\\) are estimated using linear models from as in the conditional logit model. The log-likelihood for user \\(u\\) to choose item \\(i\\) at time/session \\(t\\) decomposes into the item-level likelihood and nest-level likelihood. \\[ \\log P(i \\mid u, t) = \\log P(i \\mid u, t, B_k) + \\log P(k \\mid u, t) \\\\ = \\log \\left(\\frac{\\exp(Y_{uit}/\\lambda_k)}{\\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)}\\right) + \\log \\left( \\frac{\\exp(W_{ukt} + \\lambda_k I_{ukt})}{\\sum_{\\ell=1}^K \\exp(W_{u\\ell t} + \\lambda_\\ell I_{u\\ell t})}\\right) \\] The inclusive value of nest \\(k\\) , \\(I_{ukt}\\) is defined as \\(\\log \\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)\\) , which is the expected utility from choosing the best alternative from nest \\(k\\) . The nest_to_item keyword defines a dictionary of the mapping \\(k \\mapsto B_k\\) , where keys of nest_to_item are integer \\(k\\) 's and nest_to_item[k] is a list consisting of IDs of items in \\(B_k\\) . The {nest, item}_coef_variation_dict provides specification to \\(W_{ukt}\\) and \\(Y_{uit}\\) respectively, torch_choice allows for empty nest level models by providing an empty dictionary (in this case, \\(W_{ukt} = \\epsilon_{ukt}\\) ) since the inclusive value term \\(\\lambda_k I_{ukt}\\) will be used to model the choice over nests. However, by specifying an empty second stage model ( \\(Y_{uit} = \\epsilon_{uit}\\) ), the nested logit model reduces to a conditional logit model of choices over nests. Hence, one should never use the NestedLogitModel class with an empty item-level model. Similar to the conditional logit model, {nest, item}_num_param_dict specify the dimension (number of observables to be multiplied with the coefficient) of coefficients. The above code initializes a simple model built upon item-time-specific observables \\(X_{it} \\in \\mathbb{R}^7\\) , \\[ Y_{uit} = \\beta^\\top X_{it} + \\epsilon_{uit} \\\\ W_{ukt} = \\epsilon_{ukt} \\] The research may wish to enfoce the elasiticity \\(\\lambda_k\\) to be constant across nests, setting shared_lambda=True enforces \\(\\lambda_k = \\lambda\\ \\forall k \\in [K]\\) .","title":"Nested Logit Model: Background"},{"location":"nested_logit_model_house_cooling/#load-essential-packages","text":"We firstly read essential packages for this tutorial. import pandas as pd import torch from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model.nested_logit_model import NestedLogitModel from torch_choice.utils.run_helper import run print ( torch . __version__ ) 2.0.0 We then select the appropriate device to run the model on, our package supports both CPU and GPU. if torch . cuda . is_available (): print ( f 'CUDA device used: { torch . cuda . get_device_name () } ' ) DEVICE = 'cuda' else : print ( 'Running tutorial on CPU' ) DEVICE = 'cpu' Running tutorial on CPU","title":"Load Essential Packages"},{"location":"nested_logit_model_house_cooling/#load-datasets","text":"We firstly read the dataset for this tutorial, the csv file can be found at ./public_datasets/HC.csv . Alternatively, we load the dataset directly from the Github website. df = pd . read_csv ( 'https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/HC.csv' , index_col = 0 ) df = df . reset_index ( drop = True ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } depvar icca occa income ich och idx.id1 idx.id2 inc.room inc.cooling int.cooling cooling.modes room.modes 0 False 0.00 0.00 20 24.50 4.09 1 ec 0 0 0 False False 1 False 27.28 2.95 20 7.86 4.09 1 ecc 0 20 1 True False 2 False 0.00 0.00 20 7.37 3.85 1 er 20 0 0 False True 3 True 27.28 2.95 20 8.79 3.85 1 erc 20 20 1 True True 4 False 0.00 0.00 20 24.08 2.26 1 gc 0 0 0 False False The raw dataset is in a long-format (i.e., each row contains information of one item). df [ 'idx.id2' ] . value_counts () ec 250 ecc 250 er 250 erc 250 gc 250 gcc 250 hpc 250 Name: idx.id2, dtype: int64 # what was actually chosen. item_index = df [ df [ 'depvar' ] == True ] . sort_values ( by = 'idx.id1' )[ 'idx.id2' ] . reset_index ( drop = True ) item_names = [ 'ec' , 'ecc' , 'er' , 'erc' , 'gc' , 'gcc' , 'hpc' ] num_items = df [ 'idx.id2' ] . nunique () # cardinal encoder. encoder = dict ( zip ( item_names , range ( num_items ))) item_index = item_index . map ( lambda x : encoder [ x ]) item_index = torch . LongTensor ( item_index ) Because we will be training our model with PyTorch , we need to encode item names to integers (from 0 to 6). We do this manually in this exercise given the small amount of items, for more items, one can use sklearn.preprocessing.OrdinalEncoder to encode. Raw item names will be encoded as the following. encoder {'ec': 0, 'ecc': 1, 'er': 2, 'erc': 3, 'gc': 4, 'gcc': 5, 'hpc': 6}","title":"Load Datasets"},{"location":"nested_logit_model_house_cooling/#nest-level-dataset","text":"We firstly construct the nest-level dataset, however, there is no observable that is constant within the same nest, so we don't need to include any observable tensor to the nest_dataset . All we need to do is adding the item_index (i.e., which item is chosen) to the dataset, so that nest_dataset knows the total number of choices made. # nest feature: no nest feature, all features are item-level. nest_dataset = ChoiceDataset ( item_index = item_index . clone ()) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session.","title":"Nest Level Dataset"},{"location":"nested_logit_model_house_cooling/#item-level-dataset","text":"For simplicity, we treat each purchasing record as its own session. Moreover, we treat all observables as price observables (i.e., varying by both session and item). Since there are 7 observables in total, the resulted price_obs has shape (250, 7, 7) corresponding to number_of_sessions by number_of_items by number_of_observables . # item feature. item_feat_cols = [ 'ich' , 'och' , 'icca' , 'occa' , 'inc.room' , 'inc.cooling' , 'int.cooling' ] price_obs = utils . pivot3d ( df , dim0 = 'idx.id1' , dim1 = 'idx.id2' , values = item_feat_cols ) price_obs . shape torch.Size([250, 7, 7]) Then, we construct the item level dataset by providing both item_index and price_obs . We move item_dataset to the appropriate device as well. This is only necessary if we are using GPU to accelerate the model. item_dataset = ChoiceDataset ( item_index = item_index , price_obs = price_obs ) . to ( DEVICE ) No `session_index` is provided, assume each choice instance is in its own session. Finally, we chain the nest-level and item-level dataset into a single JointDataset . dataset = JointDataset ( nest = nest_dataset , item = item_dataset ) One can print the joint dataset to see its contents, and tensors contained in each of these sub-datasets. print ( dataset ) JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) )","title":"Item Level Dataset"},{"location":"nested_logit_model_house_cooling/#examples","text":"There are multiple ways to group 7 items into nests, different classification will result in different utility functions and estimations (see the background of nested logit models). We will demonstrate the usage of our package by presenting three different categorization schemes and corresponding model estimations.","title":"Examples"},{"location":"nested_logit_model_house_cooling/#example-1","text":"In the first example, the model is specified to have the cooling alternatives {gcc, ecc, erc, hpc} in one nest and the non-cooling alternatives {gc, ec, er} in another nest. We create a nest_to_item dictionary to inform the model our categorization scheme. The dictionary should have keys ranging from 0 to number_of_nests - 1 , each integer corresponds to a nest. The value of each key is a list of item IDs in the nest, the encoding of item names should be exactly the same as in the construction of item_index . nest_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' , 'hpc' ], 1 : [ 'gc' , 'ec' , 'er' ]} # encode items to integers. for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) In this example, we have item [1, 3, 5, 6] in the first nest (i.e., the nest with ID 0 ) and the rest of items in the second nest (i.e., the nest with ID 1 ). print ( nest_to_item ) {0: [1, 3, 5, 6], 1: [0, 2, 4]} Next, let's create the NestedLogitModel class! The first thing to put in is the nest_to_item dictionary we just built. For nest_coef_variation_dict , nest_num_param_dict , since we don't have any nest-specific observables, we can simply put an empty dictionary there. Coefficients for all observables are constant across items, and there are 7 observables in total. As for shared_lambda=True , please refer to the background recap for nested logit model. model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) model = model . to ( DEVICE ) You can print the model to get summary information of the NestedLogitModel class. print ( model ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) NOTE : We are computing the standard errors using \\(\\sqrt{\\text{diag}(H^{-1})}\\) , where \\(H\\) is the hessian of negative log-likelihood with respect to model parameters. This leads to slight different results compared with R implementation. run ( model , dataset , num_epochs = 10000 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 1000: Log-likelihood=-179.78282165527344 Epoch 2000: Log-likelihood=-178.6439666748047 Epoch 3000: Log-likelihood=-178.45376586914062 Epoch 4000: Log-likelihood=-178.30226135253906 Epoch 5000: Log-likelihood=-178.19009399414062 Epoch 6000: Log-likelihood=-178.1377716064453 Epoch 7000: Log-likelihood=-178.1256866455078 Epoch 8000: Log-likelihood=-178.124755859375 Epoch 9000: Log-likelihood=-178.12757873535156 Epoch 10000: Log-likelihood=-178.12527465820312 ==================== model results ==================== Training Epochs: 10000 Learning Rate: 0.01 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -178.12527465820312 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 0.585844 | 0.166706 | | item_price_obs[constant]_0 | -0.555026 | 0.144731 | | item_price_obs[constant]_1 | -0.858004 | 0.237756 | | item_price_obs[constant]_2 | -0.224923 | 0.110701 | | item_price_obs[constant]_3 | -1.08933 | 1.03791 | | item_price_obs[constant]_4 | -0.379122 | 0.100874 | | item_price_obs[constant]_5 | 0.249721 | 0.051977 | | item_price_obs[constant]_6 | -5.99982 | 4.83646 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) )","title":"Example 1"},{"location":"nested_logit_model_house_cooling/#r-output","text":"Here we provide the output from mlogit model in R for estimation reference. Coefficient names reported are slightly different in Python and R , please use the following table for comparison. Please note that the lambda_weight_0 in Python (at the top) corresponds to the iv (inclusive value) in R (at the bottom). Orderings of coefficients for observables should be the same in both languages. Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", ## \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 11 iterations, 0h:0m:0s ## g'(-H)^-1g = 7.26E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.554878 0.144205 -3.8478 0.0001192 *** ## och -0.857886 0.255313 -3.3601 0.0007791 *** ## icca -0.225079 0.144423 -1.5585 0.1191212 ## occa -1.089458 1.219821 -0.8931 0.3717882 ## inc.room -0.378971 0.099631 -3.8038 0.0001425 *** ## inc.cooling 0.249575 0.059213 4.2149 2.499e-05 *** ## int.cooling -6.000415 5.562423 -1.0787 0.2807030 ## iv 0.585922 0.179708 3.2604 0.0011125 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -178.12","title":"R Output"},{"location":"nested_logit_model_house_cooling/#example-2","text":"The second example is similar to the first one, but we change the way we group items into different nests. Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.) nest_to_item = { 0 : [ 'ec' , 'ecc' , 'gc' , 'gcc' , 'hpc' ], 1 : [ 'er' , 'erc' ]} for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) # these two initializations are equivalent. model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) print ( model ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) print ( model ) model = model . to ( DEVICE ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) run <function torch_choice.utils.run_helper.run(model, dataset, dataset_test=None, batch_size=-1, learning_rate=0.01, num_epochs=5000, report_frequency=None, compute_std=True, return_final_training_log_likelihood=False)> run ( model , dataset , num_epochs = 50000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 5000: Log-likelihood=-180.560791015625 Epoch 10000: Log-likelihood=-180.80062866210938 Epoch 15000: Log-likelihood=-181.21275329589844 Epoch 20000: Log-likelihood=-180.3982696533203 Epoch 25000: Log-likelihood=-180.29925537109375 Epoch 30000: Log-likelihood=-182.28366088867188 Epoch 35000: Log-likelihood=-180.1341552734375 Epoch 40000: Log-likelihood=-182.2633514404297 Epoch 45000: Log-likelihood=-180.19305419921875 Epoch 50000: Log-likelihood=-180.68240356445312 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -180.68240356445312 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 1.63154 | 0.678117 | | item_price_obs[constant]_0 | -1.34966 | 0.531558 | | item_price_obs[constant]_1 | -2.17924 | 0.894518 | | item_price_obs[constant]_2 | -0.412631 | 0.243317 | | item_price_obs[constant]_3 | -2.61227 | 2.06289 | | item_price_obs[constant]_4 | -0.885769 | 0.337734 | | item_price_obs[constant]_5 | 0.49301 | 0.199931 | | item_price_obs[constant]_6 | -16.0524 | 9.32373 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) )","title":"Example 2"},{"location":"nested_logit_model_house_cooling/#r-output_1","text":"You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", ## \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 10 iterations, 0h:0m:0s ## g'(-H)^-1g = 5.87E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -1.13818 0.54216 -2.0993 0.03579 * ## och -1.82532 0.93228 -1.9579 0.05024 . ## icca -0.33746 0.26934 -1.2529 0.21024 ## occa -2.06328 1.89726 -1.0875 0.27681 ## inc.room -0.75722 0.34292 -2.2081 0.02723 * ## inc.cooling 0.41689 0.20742 2.0099 0.04444 * ## int.cooling -13.82487 7.94031 -1.7411 0.08167 . ## iv 1.36201 0.65393 2.0828 0.03727 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.02","title":"R Output"},{"location":"nested_logit_model_house_cooling/#example-3","text":"For the third example, we now group items into three nests. Specifically, we have items gcc , ecc and erc in the first nest (nest 0 in the nest_to_item dictionary), hpc in a nest (nest 1 ) alone, and items gc , ec and er in the last nest (nest 2 ). nest_to_item = { 0 : [ 'gcc' , 'ecc' , 'erc' ], 1 : [ 'hpc' ], 2 : [ 'gc' , 'ec' , 'er' ]} for k , v in nest_to_item . items (): v = [ encoder [ item ] for item in v ] nest_to_item [ k ] = sorted ( v ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_coef_variation_dict = {}, nest_num_param_dict = {}, item_coef_variation_dict = { 'price_obs' : 'constant' }, item_num_param_dict = { 'price_obs' : 7 }, shared_lambda = True ) model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '' , item_formula = '(price_obs|constant)' , dataset = dataset , shared_lambda = True ) model = model . to ( DEVICE ) run ( model , dataset , num_epochs = 50000 , learning_rate = 0.3 ) ==================== received model ==================== NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) ) ==================== received dataset ==================== JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu) item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu) ) ==================== training the model ==================== Epoch 5000: Log-likelihood=-180.84153747558594 Epoch 10000: Log-likelihood=-182.17794799804688 Epoch 15000: Log-likelihood=-181.74029541015625 Epoch 20000: Log-likelihood=-182.3179931640625 Epoch 25000: Log-likelihood=-182.50352478027344 Epoch 30000: Log-likelihood=-181.481201171875 Epoch 35000: Log-likelihood=-181.8275604248047 Epoch 40000: Log-likelihood=-180.5753173828125 Epoch 45000: Log-likelihood=-182.4506072998047 Epoch 50000: Log-likelihood=-185.08358764648438 ==================== model results ==================== Training Epochs: 50000 Learning Rate: 0.3 Batch Size: 250 out of 250 observations in total Final Log-likelihood: -185.08358764648438 Coefficients: | Coefficient | Estimation | Std. Err. | |:---------------------------|-------------:|------------:| | lambda_weight_0 | 0.949264 | 0.19245 | | item_price_obs[constant]_0 | -0.852556 | 0.100724 | | item_price_obs[constant]_1 | -1.35082 | 0.188374 | | item_price_obs[constant]_2 | -0.248292 | 0.14014 | | item_price_obs[constant]_3 | -1.41068 | 1.2839 | | item_price_obs[constant]_4 | -0.581716 | 0.0771356 | | item_price_obs[constant]_5 | 0.336492 | 0.0656387 | | item_price_obs[constant]_6 | -10.5186 | 5.71641 | NestedLogitModel( (nest_coef_dict): ModuleDict() (item_coef_dict): ModuleDict( (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu). ) )","title":"Example 3"},{"location":"nested_logit_model_house_cooling/#r-output_2","text":"You can use the table for converting coefficient names reported by Python and R : Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling ## ## Call: ## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + ## inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", ## \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), ## un.nest.el = TRUE) ## ## Frequencies of alternatives:choice ## ec ecc er erc gc gcc hpc ## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 ## ## bfgs method ## 8 iterations, 0h:0m:0s ## g'(-H)^-1g = 3.71E-08 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(>|z|) ## ich -0.838394 0.100546 -8.3384 < 2.2e-16 *** ## och -1.331598 0.252069 -5.2827 1.273e-07 *** ## icca -0.256131 0.145564 -1.7596 0.07848 . ## occa -1.405656 1.207281 -1.1643 0.24430 ## inc.room -0.571352 0.077950 -7.3297 2.307e-13 *** ## inc.cooling 0.311355 0.056357 5.5247 3.301e-08 *** ## int.cooling -10.413384 5.612445 -1.8554 0.06354 . ## iv 0.956544 0.180722 5.2929 1.204e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -180.26","title":"R Output"},{"location":"post_estimation_demos/","text":"Tutorial: Post-Estimations Author: Tianyu Du (tianyudu@stanford.edu) This tutorial covers the toolkit in torch-choice for visualizing and analyzing models after model estimation. Note : models demonstrated in this tutorial are for demonstration purpose only, hence we don't estimate them in this tutorial. Instead, this tutorial focuses on APIs to visualize and analyze models. # import required dependencies. from time import time import numpy as np import pandas as pd import seaborn as sns import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model import ConditionalLogitModel , NestedLogitModel from torch_choice.utils.run_helper import run # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' ) Creating ChoiceDataset Object We first create a dummy ChoiceDataset object, please refer to the data management tutorial for more details. # Feel free to modify it as you want. num_users = 100 num_items = 25 num_sessions = 500 length_of_dataset = 10000 # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. itemsession_obs = torch . randn ( num_sessions , num_items , 12 ) item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool () # initialize a ChoiceDataset object. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: num_users = num_users , num_items = num_items , user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , itemsession_obs = itemsession_obs ) print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu) Conditional Logit Model Suppose that we are creating a very complicated dummy model as the following. Please note that model and dataset here are for demonstration purpose only, the model is unlikely to converge if one estimate it on this dataset. \\[ U_{uis} = \\alpha + \\beta_i + \\gamma_u + \\delta_i^\\top \\textbf{x}^{(user)}_u + \\eta^\\top \\textbf{y}^{(item)}_i + \\theta_u^\\top \\textbf{z}^{(session)}_{s} + \\kappa_i^\\top \\textbf{w}^{(itemsession)}_{is} + \\iota_u^\\top \\textbf{w}^{(itemsession)}_{is} + \\epsilon_{uis} \\] model = ConditionalLogitModel ( formula = '(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)' , dataset = dataset , num_users = num_users , num_items = num_items ) # estimate the model... omitted in this tutorial. model ConditionalLogitModel( (coef_dict): ModuleDict( (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu). (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu). (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu). (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu). (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu). (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[intercept[constant]] with 1 parameters, with constant level variation. X[intercept[item]] with 1 parameters, with item level variation. X[intercept[user]] with 1 parameters, with user level variation. X[user_obs[item]] with 128 parameters, with item level variation. X[item_obs[constant]] with 64 parameters, with constant level variation. X[session_obs[user]] with 10 parameters, with user level variation. X[itemsession_obs[item]] with 12 parameters, with item level variation. X[itemsession_obs[user]] with 12 parameters, with user level variation. device=cpu Retrieving Model Parameters with the get_coefficient() method. In the model representation above, we can see that the model has coefficients from intercept[constant] to itemsession_obs . The get_coefficient() method allows users to retrieve the coefficient values from the model using the general syntax model.get_coefficient(COEFFICIENT_NAME) . For example, model.get_coefficient('intercept[constant]') will return the value of \\(\\alpha\\) , which is a scalar. model . get_coefficient ( 'intercept[constant]' ) tensor([1.2126]) model.get_coefficient('intercept[user]') returns the array of \\(\\gamma_u\\) 's, which is a 1D array of length num_users . model . get_coefficient ( 'intercept[user]' ) . shape torch.Size([100, 1]) model.get_coefficient('session_obs[user]') returns the corresponding coefficient \\(\\theta_u\\) , which is a 2D array of shape (num_users, num_session_features) . Each row of the returned tensor corresponds to the coefficient vector of a user. model . get_coefficient ( 'session_obs[user]' ) . shape torch.Size([100, 10]) Lastly, the itemsession_obs (a 12-dimensional feature vector for each \\((i, s)\\) pairs) affects the utility through both \\(\\kappa_i\\) and \\(\\iota_u\\) . For each item (except for the first item indexed with 0 , all coefficients of it are 0 ), the get_coefficient() method returns a 2D array of shape (num_items-1, num_itemsession_features) . The first row of the returned tensor corresponds to the coefficient vector of the second item, and so on. model.get_coefficient('itemsession_obs[user]') provides the user-specific relationship between utility and item-session observables, \\(\\iota_u\\) , which is a 2D array of shape (num_users, num_itemsession_features) . Each row of the returned tensor corresponds to the coefficient vector of a user. model . get_coefficient ( 'itemsession_obs[item]' ) . shape torch.Size([24, 12]) model . get_coefficient ( 'itemsession_obs[user]' ) . shape torch.Size([100, 12]) Nested Logit Model The nested logit model has a very similar interface for coefficient extraction to the conditional logit model demonstrated above. Consider a nested logit model with the same item-level model but with nest-level model incorporating user-fixed effect, category-fixed effect (specified by (1|item) in the nest_formula ), and user-specific coefficient on a 64-dimensional nest-specific observable (specified by (item_obs|user) in the nest_formula ). The only difference is researcher would need to retrieve the coefficients of the nested logit model using the get_coefficient() method with the level argument. NestedLogitModel.get_coefficient() Method. nest_to_item = { 0 : [ 0 , 1 , 2 , 3 , 4 ], 1 : [ 5 , 6 , 7 , 8 , 9 ], 2 : [ 10 , 11 , 12 , 13 , 14 ], 3 : [ 15 , 16 , 17 , 18 , 19 ], 4 : [ 20 , 21 , 22 , 23 , 24 ] } nest_dataset = ChoiceDataset ( item_index = item_index , user_index = user_index , num_items = len ( nest_to_item ), num_users = num_users , item_obs = torch . randn ( len ( nest_to_item ), 64 )) joint_dataset = JointDataset ( nest = nest_dataset , item = dataset ) joint_dataset No `session_index` is provided, assume each choice instance is in its own session. JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[], item_obs=[5, 64], device=cpu) item: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu) ) nested_model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '(1|user) + (1|item) + (item_obs|user)' , item_formula = '(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)' , num_users = num_users , dataset = joint_dataset , shared_lambda = False ) nested_model NestedLogitModel( (nest_coef_dict): ModuleDict( (intercept[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=5, num_users=100, num_params=1, 4 trainable parameters in total, device=cpu). (item_obs[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=64, 6400 trainable parameters in total, device=cpu). ) (item_coef_dict): ModuleDict( (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu). (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu). (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu). (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu). (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu). (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu). ) ) # estimate the model... omitted in this tutorial. For example, you can use the following code snippet to retrieve the coefficient of the user-fixed effect in the nest level model, which is a vector with num_users elements. nested_model . get_coefficient ( 'intercept[user]' , level = 'nest' ) . shape torch.Size([100, 1]) Similarly, by changing to level='item' , the researcher can obtain the coefficient of user-specific fixed effect in the item level model, which is a also vector with num_users elements. nested_model . get_coefficient ( 'intercept[user]' , level = 'item' ) . shape torch.Size([100, 1]) This API generalizes to all other coefficients listed above such as itemsession_obs[item] and itemsession_obs[user] . One exception is the coefficients for inclusive values, (often denoted as \\(\\lambda\\) ). Researchers can retrieve the coefficient of the inclusive value by using get_coefficient('lambda') without specifying the level argument ( get_coefficient will disregard any level argument if the coefficient name is lambda ). The returned value is a scalar if shared_lambda is True , and a 1D array of length num_nests if shared_lambda is False . In our case, the returned value is an array of length five (we have five nests in this model). nested_model . get_coefficient ( 'lambda' ) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000])","title":"Post Estimation"},{"location":"post_estimation_demos/#tutorial-post-estimations","text":"Author: Tianyu Du (tianyudu@stanford.edu) This tutorial covers the toolkit in torch-choice for visualizing and analyzing models after model estimation. Note : models demonstrated in this tutorial are for demonstration purpose only, hence we don't estimate them in this tutorial. Instead, this tutorial focuses on APIs to visualize and analyze models. # import required dependencies. from time import time import numpy as np import pandas as pd import seaborn as sns import torch import torch.nn.functional as F from torch_choice.data import ChoiceDataset , JointDataset , utils from torch_choice.model import ConditionalLogitModel , NestedLogitModel from torch_choice.utils.run_helper import run # let's get a helper def print_dict_shape ( d ): for key , val in d . items (): if torch . is_tensor ( val ): print ( f 'dict. { key } .shape= { val . shape } ' )","title":"Tutorial: Post-Estimations"},{"location":"post_estimation_demos/#creating-choicedataset-object","text":"We first create a dummy ChoiceDataset object, please refer to the data management tutorial for more details. # Feel free to modify it as you want. num_users = 100 num_items = 25 num_sessions = 500 length_of_dataset = 10000 # create observables/features, the number of parameters are arbitrarily chosen. # generate 128 features for each user, e.g., race, gender. user_obs = torch . randn ( num_users , 128 ) # generate 64 features for each user, e.g., quality. item_obs = torch . randn ( num_items , 64 ) # generate 10 features for each session, e.g., weekday indicator. session_obs = torch . randn ( num_sessions , 10 ) # generate 12 features for each session user pair, e.g., the budget of that user at the shopping day. itemsession_obs = torch . randn ( num_sessions , num_items , 12 ) item_index = torch . LongTensor ( np . random . choice ( num_items , size = length_of_dataset )) user_index = torch . LongTensor ( np . random . choice ( num_users , size = length_of_dataset )) session_index = torch . LongTensor ( np . random . choice ( num_sessions , size = length_of_dataset )) # assume all items are available in all sessions. item_availability = torch . ones ( num_sessions , num_items ) . bool () # initialize a ChoiceDataset object. dataset = ChoiceDataset ( # pre-specified keywords of __init__ item_index = item_index , # required. # optional: num_users = num_users , num_items = num_items , user_index = user_index , session_index = session_index , item_availability = item_availability , # additional keywords of __init__ user_obs = user_obs , item_obs = item_obs , session_obs = session_obs , itemsession_obs = itemsession_obs ) print ( dataset ) ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu)","title":"Creating  ChoiceDataset Object"},{"location":"post_estimation_demos/#conditional-logit-model","text":"Suppose that we are creating a very complicated dummy model as the following. Please note that model and dataset here are for demonstration purpose only, the model is unlikely to converge if one estimate it on this dataset. \\[ U_{uis} = \\alpha + \\beta_i + \\gamma_u + \\delta_i^\\top \\textbf{x}^{(user)}_u + \\eta^\\top \\textbf{y}^{(item)}_i + \\theta_u^\\top \\textbf{z}^{(session)}_{s} + \\kappa_i^\\top \\textbf{w}^{(itemsession)}_{is} + \\iota_u^\\top \\textbf{w}^{(itemsession)}_{is} + \\epsilon_{uis} \\] model = ConditionalLogitModel ( formula = '(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)' , dataset = dataset , num_users = num_users , num_items = num_items ) # estimate the model... omitted in this tutorial. model ConditionalLogitModel( (coef_dict): ModuleDict( (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu). (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu). (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu). (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu). (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu). (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu). ) ) Conditional logistic discrete choice model, expects input features: X[intercept[constant]] with 1 parameters, with constant level variation. X[intercept[item]] with 1 parameters, with item level variation. X[intercept[user]] with 1 parameters, with user level variation. X[user_obs[item]] with 128 parameters, with item level variation. X[item_obs[constant]] with 64 parameters, with constant level variation. X[session_obs[user]] with 10 parameters, with user level variation. X[itemsession_obs[item]] with 12 parameters, with item level variation. X[itemsession_obs[user]] with 12 parameters, with user level variation. device=cpu","title":"Conditional Logit Model"},{"location":"post_estimation_demos/#retrieving-model-parameters-with-the-get_coefficient-method","text":"In the model representation above, we can see that the model has coefficients from intercept[constant] to itemsession_obs . The get_coefficient() method allows users to retrieve the coefficient values from the model using the general syntax model.get_coefficient(COEFFICIENT_NAME) . For example, model.get_coefficient('intercept[constant]') will return the value of \\(\\alpha\\) , which is a scalar. model . get_coefficient ( 'intercept[constant]' ) tensor([1.2126]) model.get_coefficient('intercept[user]') returns the array of \\(\\gamma_u\\) 's, which is a 1D array of length num_users . model . get_coefficient ( 'intercept[user]' ) . shape torch.Size([100, 1]) model.get_coefficient('session_obs[user]') returns the corresponding coefficient \\(\\theta_u\\) , which is a 2D array of shape (num_users, num_session_features) . Each row of the returned tensor corresponds to the coefficient vector of a user. model . get_coefficient ( 'session_obs[user]' ) . shape torch.Size([100, 10]) Lastly, the itemsession_obs (a 12-dimensional feature vector for each \\((i, s)\\) pairs) affects the utility through both \\(\\kappa_i\\) and \\(\\iota_u\\) . For each item (except for the first item indexed with 0 , all coefficients of it are 0 ), the get_coefficient() method returns a 2D array of shape (num_items-1, num_itemsession_features) . The first row of the returned tensor corresponds to the coefficient vector of the second item, and so on. model.get_coefficient('itemsession_obs[user]') provides the user-specific relationship between utility and item-session observables, \\(\\iota_u\\) , which is a 2D array of shape (num_users, num_itemsession_features) . Each row of the returned tensor corresponds to the coefficient vector of a user. model . get_coefficient ( 'itemsession_obs[item]' ) . shape torch.Size([24, 12]) model . get_coefficient ( 'itemsession_obs[user]' ) . shape torch.Size([100, 12])","title":"Retrieving Model Parameters with the get_coefficient() method."},{"location":"post_estimation_demos/#nested-logit-model","text":"The nested logit model has a very similar interface for coefficient extraction to the conditional logit model demonstrated above. Consider a nested logit model with the same item-level model but with nest-level model incorporating user-fixed effect, category-fixed effect (specified by (1|item) in the nest_formula ), and user-specific coefficient on a 64-dimensional nest-specific observable (specified by (item_obs|user) in the nest_formula ). The only difference is researcher would need to retrieve the coefficients of the nested logit model using the get_coefficient() method with the level argument.","title":"Nested Logit Model"},{"location":"post_estimation_demos/#nestedlogitmodelget_coefficient-method","text":"nest_to_item = { 0 : [ 0 , 1 , 2 , 3 , 4 ], 1 : [ 5 , 6 , 7 , 8 , 9 ], 2 : [ 10 , 11 , 12 , 13 , 14 ], 3 : [ 15 , 16 , 17 , 18 , 19 ], 4 : [ 20 , 21 , 22 , 23 , 24 ] } nest_dataset = ChoiceDataset ( item_index = item_index , user_index = user_index , num_items = len ( nest_to_item ), num_users = num_users , item_obs = torch . randn ( len ( nest_to_item ), 64 )) joint_dataset = JointDataset ( nest = nest_dataset , item = dataset ) joint_dataset No `session_index` is provided, assume each choice instance is in its own session. JointDataset with 2 sub-datasets: ( nest: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[], item_obs=[5, 64], device=cpu) item: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu) ) nested_model = NestedLogitModel ( nest_to_item = nest_to_item , nest_formula = '(1|user) + (1|item) + (item_obs|user)' , item_formula = '(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)' , num_users = num_users , dataset = joint_dataset , shared_lambda = False ) nested_model NestedLogitModel( (nest_coef_dict): ModuleDict( (intercept[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=5, num_users=100, num_params=1, 4 trainable parameters in total, device=cpu). (item_obs[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=64, 6400 trainable parameters in total, device=cpu). ) (item_coef_dict): ModuleDict( (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu). (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu). (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu). (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu). (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu). (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu). (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu). (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu). ) ) # estimate the model... omitted in this tutorial. For example, you can use the following code snippet to retrieve the coefficient of the user-fixed effect in the nest level model, which is a vector with num_users elements. nested_model . get_coefficient ( 'intercept[user]' , level = 'nest' ) . shape torch.Size([100, 1]) Similarly, by changing to level='item' , the researcher can obtain the coefficient of user-specific fixed effect in the item level model, which is a also vector with num_users elements. nested_model . get_coefficient ( 'intercept[user]' , level = 'item' ) . shape torch.Size([100, 1]) This API generalizes to all other coefficients listed above such as itemsession_obs[item] and itemsession_obs[user] . One exception is the coefficients for inclusive values, (often denoted as \\(\\lambda\\) ). Researchers can retrieve the coefficient of the inclusive value by using get_coefficient('lambda') without specifying the level argument ( get_coefficient will disregard any level argument if the coefficient name is lambda ). The returned value is a scalar if shared_lambda is True , and a 1D array of length num_nests if shared_lambda is False . In our case, the returned value is an array of length five (we have five nests in this model). nested_model . get_coefficient ( 'lambda' ) tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000])","title":"NestedLogitModel.get_coefficient() Method."},{"location":"projects/","text":"Research Projects using this Package Question-Answering Data for Educational Applications Tutorial on Educational Question-Answering","title":"Related Projects"},{"location":"projects/#research-projects-using-this-package","text":"","title":"Research Projects using this Package"},{"location":"projects/#question-answering-data-for-educational-applications","text":"Tutorial on Educational Question-Answering","title":"Question-Answering Data for Educational Applications"},{"location":"test/","text":"Compatibility Check List We have tested the tutorials using the following environments, please let us know if there is any issue with our packages on other systems. Tutorial Platform Versions CPU GPU Device Tested Data Management MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Data Management Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Conditional Logit Model MacOS 12.2 Python 3.9 PyTorch 1.10.0 M1 Max N/A cpu Conditional Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Nested Logit Model MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Nested Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda","title":"Compatibility Tests"},{"location":"test/#compatibility-check-list","text":"We have tested the tutorials using the following environments, please let us know if there is any issue with our packages on other systems. Tutorial Platform Versions CPU GPU Device Tested Data Management MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Data Management Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Conditional Logit Model MacOS 12.2 Python 3.9 PyTorch 1.10.0 M1 Max N/A cpu Conditional Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Nested Logit Model MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Nested Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda","title":"Compatibility Check List"}]}