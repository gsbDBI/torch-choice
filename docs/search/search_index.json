{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"torch-choice","text":"<p>Authors: Tianyu Du, Ayush Kanodia and Susan Athey; Contact: tianyudu@stanford.edu</p> <p>Acknowledgements: We would like to thank Erik Sverdrup, Charles Pebereau and Keshav Agrawal for their feedback.</p> <p><code>torch-choice</code> is a library for flexible, fast choice modeling with PyTorch: it has logit and nested logit models, designed for both estimation and prediction. See the complete documentation for more details. Unique features: 1. GPU support via torch for speed 2. Specify customized models 3. Specify availability sets 4. Maximum Likelihood Estimation (MLE) (optionally, reporting standard errors or MAP inference with Bayesian Priors on coefficients) 5. Estimation via minimization of Cross Entropy Loss (optionally with L1/L2 regularization)</p>"},{"location":"#introduction","title":"Introduction","text":""},{"location":"#logistic-regression-and-choice-models","title":"Logistic Regression and Choice Models","text":"<p>Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function</p> \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] <p>where, </p> \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] <p>here \\(X\\), \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\).</p> <p>When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\), as</p> \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] <p>where \\(\\epsilon\\) is an unobserved random error term.</p> <p>If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\), this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\), as shown by McFadden, and as often studied in Econometrics.</p>"},{"location":"#package","title":"Package","text":"<p>We implement a fully flexible setup, where we allow  1. coefficients (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\dots\\)) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\)), item-specific (i.e., \\(\\alpha=\\alpha_i\\)), session-specific (i.e., \\(\\alpha=\\alpha_t\\)), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\)). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables (\\(X\\), \\(Y\\), \\(\\dots\\)) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\)</p> <p>This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\), which enables modelling rich structures.</p> <p>As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below</p> <p>Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial)</p> \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] <p>This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S</p> <p>Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3)</p> <ol> <li>MNIST classification (Upcoming Detailed Tutorial)</li> </ol> \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] <p>We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1)</p> <p>This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression</p> <p>We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form.</p>"},{"location":"#installation","title":"Installation","text":"<ol> <li>Clone the repository to your local machine or server.</li> <li>Install required dependencies using: <code>pip3 install -r requirements.txt</code>.</li> <li>Run <code>pip3 install torch-choice</code>.</li> <li>Check installation by running <code>python3 -c 'import torch_choice; print(torch_choice.__version__)'</code>.</li> </ol> <p>The installation page provides more details on installation.</p>"},{"location":"#example-usage-transportation-choice-dataset","title":"Example Usage - Transportation Choice Dataset","text":"<p>In this demonstration, we setup a minimal example of fitting a conditional logit model using our package. We provide equivalent R code as well for reference, to aid replicating from R to this package.</p> <p>We are modelling people's choices on transportation modes using the publicly available <code>ModeCanada</code> dataset. More information about the ModeCanada: Mode Choice for the Montreal-Toronto Corridor.</p> <p>In this example, we are estimating the utility for user \\(u\\) to choose transport method \\(i\\) in session \\(s\\) as $$ U_{uis} = \\alpha_i + \\beta_i \\text{income}_s + \\gamma \\text{cost} + \\delta \\text{freq} + \\eta \\text{ovt} + \\iota_i \\text{ivt} + \\varepsilon $$ this is equivalent to the functional form described in the previous section</p>"},{"location":"#mode-canada-with-torch-choice","title":"Mode Canada with Torch-Choice","text":"<pre><code># load packages.\nimport pandas as pd\nimport torch_choice\n\n# load data.\ndf = pd.read_csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA').query('noalt == 4').reset_index(drop=True)\n\n# format data.\ndata = torch_choice.utils.easy_data_wrapper.EasyDatasetWrapper(\n    main_data=df,\n    purchase_record_column='case',\n    choice_column='choice',\n    item_name_column='alt',\n    user_index_column='case',\n    session_index_column='case',\n    session_observable_columns=['income'],\n    price_observable_columns=['cost', 'freq', 'ovt', 'ivt'])\n\n# define the conditional logit model.\nmodel = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'price_cost': 'constant',\n                         'price_freq': 'constant',\n                         'price_ovt': 'constant',\n                         'session_income': 'item',\n                         'price_ivt': 'item-full',\n                         'intercept': 'item'},\n    num_items=4)\n# fit the conditional logit model.\ntorch_choice.utils.run_helper.run(model, data.choice_dataset, num_epochs=5000, learning_rate=0.01, batch_size=-1)\n</code></pre>"},{"location":"#mode-canada-with-r","title":"Mode Canada with R","text":"<p>We include the R code for the ModeCanada example as well. <pre><code># load packages.\nlibrary(\"mlogit\")\n\n# load data.\nModeCanada &lt;- read.csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/ModeCanada.csv?token=GHSAT0AAAAAABRGHCCSNNQARRMU63W7P7F4YWYP5HA')\nModeCanada &lt;- select(ModeCanada, -X)\nModeCanada$alt &lt;- as.factor(ModeCanada$alt)\n\n# format data.\nMC &lt;- dfidx(ModeCanada, subset = noalt == 4)\n\n# fit the data.\nml.MC1 &lt;- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air')\nsummary(ml.MC1)\n</code></pre></p>"},{"location":"#whats-in-the-package","title":"What's in the package?","text":"<p>Overall, the <code>torch-choice</code> package offers the following features:</p> <ol> <li> <p>The package includes a data management module called <code>ChoiceDataset</code>, which is built upon PyTorch's dataset module. Our dataset implementation allows users to easily move data between CPU and GPU. Unlike traditional long or wide formats, the <code>ChoiceDataset</code> offers a memory-efficient way to manage observables.</p> </li> <li> <p>The package provides a (1) conditional logit model and (2) a nested logit model for consumer choice modeling.</p> </li> <li> <p>The package leverage GPU acceleration using PyTorch and easily scale to large dataset of millions of choice records. All models are trained using state-of-the-art optimizers by in PyTorch. These optimization algorithms are tested to be scalable by modern machine learning practitioners. However, you can rest assure that the package runs flawlessly when no GPU is used as well.</p> </li> <li> <p>Setting up the PyTorch training pipelines can be frustrating. We provide easy-to-use PyTorch lightning wrapper of models to free researchers from the hassle from setting up PyTorch optimizers and training loops.</p> </li> </ol> <pre><code>\n</code></pre>"},{"location":"all_model_specification/","title":"All model specification","text":"<pre><code># import required dependencies.\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch_choice.data import ChoiceDataset, JointDataset\n</code></pre> <pre><code>/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: &lt;B3E58761-2785-34C6-A89B-F37110C88A05&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;AE6DCE26-A528-35ED-BB3D-88890D27E6B9&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n  warn(f\"Failed to load image Python extension: {e}\")\n</code></pre> <pre><code># let's get a helper\ndef print_dict_shape(d):\n    for key, val in d.items():\n        if torch.is_tensor(val):\n            print(f'dict.{key}.shape={val.shape}')\n</code></pre>"},{"location":"all_model_specification/#creating-choicedataset-object","title":"Creating  <code>ChoiceDataset</code> Object","text":"<pre><code># Feel free to modify it as you want.\nnum_users = 10\nnum_items = 4\nnum_sessions = 500\n\nlength_of_dataset = 10000\n</code></pre> <pre><code># create observables/features, the number of parameters are arbitrarily chosen.\n# generate 128 features for each user, e.g., race, gender.\nuser_obs = torch.randn(num_users, 128)\n# generate 64 features for each user, e.g., quality.\nitem_obs = torch.randn(num_items, 64)\n# generate 32 features for each user item pair.\nuseritem_obs = torch.randn(num_users, num_items, 32)\n# generate 10 features for each session, e.g., weekday indicator. \nsession_obs = torch.randn(num_sessions, 10)\n# generate 12 features for each session user pair, e.g., the budget of that user at the shopping day.\nprice_obs = torch.randn(num_sessions, num_items, 12)\n# generate 16 useritemsession observable.\nusersessionitem_obs = torch.randn(num_users, num_sessions, num_items, 16)\n</code></pre> <p>We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen.</p> <p>Notes on Encodings Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well).</p> <pre><code>item_index = torch.LongTensor(np.random.choice(num_items, size=length_of_dataset))\nuser_index = torch.LongTensor(np.random.choice(num_users, size=length_of_dataset))\nsession_index = torch.LongTensor(np.random.choice(num_sessions, size=length_of_dataset))\n\n# assume all items are available in all sessions.\nitem_availability = torch.ones(num_sessions, num_items).bool()\n</code></pre>"},{"location":"all_model_specification/#step-2-initialize-the-choicedataset","title":"Step 2: Initialize the <code>ChoiceDataset</code>.","text":"<p>You can construct a choice set using the following code, which manage all information for you.</p> <pre><code>dataset = ChoiceDataset(\n    # pre-specified keywords of __init__\n    item_index=item_index,  # required.\n    # optional:\n    user_index=user_index,\n    session_index=session_index,\n    item_availability=item_availability,\n    user_obs=user_obs,\n    item_obs=item_obs,\n    useritem_obs=useritem_obs,\n    session_obs=session_obs,\n    price_obs=price_obs,\n    usersessionitem_obs=usersessionitem_obs)\n</code></pre>"},{"location":"all_model_specification/#what-you-can-do-with-the-choicedataset","title":"What you can do with the <code>ChoiceDataset</code>?","text":""},{"location":"all_model_specification/#printdataset-and-dataset__str__","title":"<code>print(dataset)</code> and <code>dataset.__str__</code>","text":"<p>The command <code>print(dataset)</code> will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory).</p> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], price_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 16], device=cpu)\n</code></pre>"},{"location":"all_model_specification/#confirm-correctness-using-the-x_dict-method","title":"Confirm Correctness using the <code>x_dict</code> Method","text":"<p>The port between <code>ChoiceDataset</code> and model classes is a method called <code>x_dict</code> in the dataset. The <code>x_dict</code> method compiles all information in the dataset into a dictionary, with observable names as keys, and all values of this dictionary are tensors with shape <code>(length_of_dataset, num_items, *. The</code>*` denotes the number of corresponding observables and could be different for different observable tensors.</p> <pre><code>print_dict_shape(dataset.x_dict)\n</code></pre> <pre><code>dict.user_obs.shape=torch.Size([10000, 4, 128])\ndict.item_obs.shape=torch.Size([10000, 4, 64])\ndict.useritem_obs.shape=torch.Size([10000, 4, 32])\ndict.session_obs.shape=torch.Size([10000, 4, 10])\ndict.price_obs.shape=torch.Size([10000, 4, 12])\ndict.usersessionitem_obs.shape=torch.Size([10000, 4, 16])\n</code></pre> <pre><code># check the `x_dict` indeed have what we are expecting, test 10 random records.\nfor n in tqdm(np.random.choice(length_of_dataset, 10)):\n    u = user_index[n]\n    s = session_index[n]\n\n    for i in range(num_items):\n        for k in range(128):\n            expected = user_obs[u, k]\n            got = dataset.x_dict[\"user_obs\"][n, i, k]\n            assert expected == got\n\n        for k in range(64):\n            expected = item_obs[i, k]\n            got = dataset.x_dict[\"item_obs\"][n, i, k]\n            assert expected == got\n\n        for k in range(32):\n            expected = useritem_obs[u, i, k]\n            got = dataset.x_dict[\"useritem_obs\"][n, i, k]\n            assert expected == got\n\n        for k in range(10):\n            expected = session_obs[s, k]\n            got = dataset.x_dict[\"session_obs\"][n, i, k]\n            assert expected == got\n\n\n        for k in range(12):\n            expected = price_obs[s, i, k]\n            got = dataset.x_dict[\"price_obs\"][n, i, k]\n            assert expected == got\n\n        for k in range(16):\n            expected = usersessionitem_obs[u, s, i, k]\n            got = dataset.x_dict[\"usersessionitem_obs\"][n, i, k]\n            assert expected == got\n\nprint(\"all good!\")\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:17&lt;00:00,  1.74s/it]\n\nall good!\n</code></pre>"},{"location":"all_model_specification/#build-a-clm-model","title":"Build a CLM Model","text":"<pre><code>from time import time\nfrom torch_choice.model import ConditionalLogitModel\nfrom torch_choice import run\n</code></pre> <pre><code>model = model = ConditionalLogitModel(\n    formula='(user_obs|item) + (item_obs|user) + (useritem_obs|constant) + (session_obs|item) + (price_obs|constant) + (usersessionitem_obs|constant) + (intercept|item)',\n    dataset=dataset,\n    num_users=num_users,\n    num_items=num_items)\n</code></pre> <pre><code>model\n</code></pre> <pre><code>ConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (user_obs[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=128, 384 trainable parameters in total, device=cpu).\n    (item_obs[user]): Coefficient(variation=user, num_items=4, num_users=10, num_params=64, 640 trainable parameters in total, device=cpu).\n    (useritem_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=32, 32 trainable parameters in total, device=cpu).\n    (session_obs[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=10, 30 trainable parameters in total, device=cpu).\n    (price_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=12, 12 trainable parameters in total, device=cpu).\n    (usersessionitem_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=16, 16 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[user_obs[item]] with 128 parameters, with item level variation.\nX[item_obs[user]] with 64 parameters, with user level variation.\nX[useritem_obs[constant]] with 32 parameters, with constant level variation.\nX[session_obs[item]] with 10 parameters, with item level variation.\nX[price_obs[constant]] with 12 parameters, with constant level variation.\nX[usersessionitem_obs[constant]] with 16 parameters, with constant level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre> <pre><code>start_time = time()\nrun(model, dataset, num_epochs=10, learning_rate=0.01, model_optimizer=\"Adam\", batch_size=-1)\nprint('Time taken:', time() - start_time)\n</code></pre> <pre><code>GPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n==================== model received ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (user_obs[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=128, 384 trainable parameters in total, device=cpu).\n    (item_obs[user]): Coefficient(variation=user, num_items=4, num_users=10, num_params=64, 640 trainable parameters in total, device=cpu).\n    (useritem_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=32, 32 trainable parameters in total, device=cpu).\n    (session_obs[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=10, 30 trainable parameters in total, device=cpu).\n    (price_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=12, 12 trainable parameters in total, device=cpu).\n    (usersessionitem_obs[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=16, 16 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[user_obs[item]] with 128 parameters, with item level variation.\nX[item_obs[user]] with 64 parameters, with user level variation.\nX[useritem_obs[constant]] with 32 parameters, with constant level variation.\nX[session_obs[item]] with 10 parameters, with item level variation.\nX[price_obs[constant]] with 12 parameters, with constant level variation.\nX[usersessionitem_obs[constant]] with 16 parameters, with constant level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n==================== data set received ====================\n[Train dataset] ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], price_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 16], device=cpu)\n[Validation dataset] None\n[Test dataset] None\n\n\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n  rank_zero_warn(\n\n  | Name  | Type                  | Params\n------------------------------------------------\n0 | model | ConditionalLogitModel | 1.1 K \n------------------------------------------------\n1.1 K     Trainable params\n0         Non-trainable params\n1.1 K     Total params\n0.004     Total estimated model params size (MB)\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\nEpoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.40it/s, loss=7.1e+04, v_num=44]\n\n`Trainer.fit` stopped: `max_epochs=10` reached.\n\n\nEpoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 39.46it/s, loss=7.1e+04, v_num=44]\nTime taken for training: 0.9634578227996826\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -56230.62890625, [Validation] N/A, [Test] N/A\n\n| Coefficient                      |   Estimation |   Std. Err. |       z-value |      Pr(&gt;|z|) | Significance   |\n|:---------------------------------|-------------:|------------:|--------------:|--------------:|:---------------|\n| user_obs[item]_0                 | -0.0889893   | 139.667     |  -0.000637154 |   0.999492    |                |\n| user_obs[item]_1                 | -0.0341076   | nan         | nan           | nan           |                |\n| user_obs[item]_2                 | -0.0982556   | nan         | nan           | nan           |                |\n| user_obs[item]_3                 |  0.0670713   |  18.9429    |   0.00354071  |   0.997175    |                |\n| user_obs[item]_4                 |  0.0836471   | nan         | nan           | nan           |                |\n| user_obs[item]_5                 |  0.0925889   | nan         | nan           | nan           |                |\n| user_obs[item]_6                 |  0.0884399   |  37.105     |   0.0023835   |   0.998098    |                |\n| user_obs[item]_7                 | -0.0993338   | nan         | nan           | nan           |                |\n| user_obs[item]_8                 |  0.0396195   | nan         | nan           | nan           |                |\n| user_obs[item]_9                 | -0.000340273 |  24.0647    |  -1.41399e-05 |   0.999989    |                |\n| user_obs[item]_10                |  0.0980912   |  65.7966    |   0.00149082  |   0.99881     |                |\n| user_obs[item]_11                |  0.0712994   |  25.9504    |   0.00274752  |   0.997808    |                |\n| user_obs[item]_12                | -0.0654005   | nan         | nan           | nan           |                |\n| user_obs[item]_13                |  0.0673973   | nan         | nan           | nan           |                |\n| user_obs[item]_14                |  0.0940802   | nan         | nan           | nan           |                |\n| user_obs[item]_15                | -0.0938857   |  30.5794    |  -0.00307023  |   0.99755     |                |\n| user_obs[item]_16                | -0.0425794   |  47.2045    |  -0.000902021 |   0.99928     |                |\n| user_obs[item]_17                |  0.0794171   |  44.3518    |   0.00179062  |   0.998571    |                |\n| user_obs[item]_18                | -0.0998146   | nan         | nan           | nan           |                |\n| user_obs[item]_19                |  0.0852862   |   6.97753   |   0.012223    |   0.990248    |                |\n| user_obs[item]_20                |  0.05898     | nan         | nan           | nan           |                |\n| user_obs[item]_21                | -0.0854324   | nan         | nan           | nan           |                |\n| user_obs[item]_22                | -0.0237377   | nan         | nan           | nan           |                |\n| user_obs[item]_23                |  0.10116     |  20.1079    |   0.00503089  |   0.995986    |                |\n| user_obs[item]_24                |  0.0741224   |  46.0753    |   0.00160872  |   0.998716    |                |\n| user_obs[item]_25                |  0.0865076   |  58.1918    |   0.0014866   |   0.998814    |                |\n| user_obs[item]_26                |  0.0521571   |  51.714     |   0.00100857  |   0.999195    |                |\n| user_obs[item]_27                |  0.0759137   | nan         | nan           | nan           |                |\n| user_obs[item]_28                | -0.0809748   | nan         | nan           | nan           |                |\n| user_obs[item]_29                | -0.054031    |  25.3736    |  -0.00212942  |   0.998301    |                |\n| user_obs[item]_30                | -0.0164601   |  18.0856    |  -0.000910124 |   0.999274    |                |\n| user_obs[item]_31                | -0.080897    |  57.7205    |  -0.00140153  |   0.998882    |                |\n| user_obs[item]_32                | -0.0909767   |  37.1174    |  -0.00245105  |   0.998044    |                |\n| user_obs[item]_33                |  0.0485426   | nan         | nan           | nan           |                |\n| user_obs[item]_34                | -0.0734832   |  31.7805    |  -0.00231221  |   0.998155    |                |\n| user_obs[item]_35                |  0.0980138   |  20.8498    |   0.00470096  |   0.996249    |                |\n| user_obs[item]_36                |  0.0955767   | nan         | nan           | nan           |                |\n| user_obs[item]_37                |  0.0555144   |  97.5976    |   0.000568809 |   0.999546    |                |\n| user_obs[item]_38                |  0.0946894   | nan         | nan           | nan           |                |\n| user_obs[item]_39                |  0.0555615   | nan         | nan           | nan           |                |\n| user_obs[item]_40                | -0.100283    | 108.641     |  -0.00092307  |   0.999263    |                |\n| user_obs[item]_41                | -0.0225418   | nan         | nan           | nan           |                |\n| user_obs[item]_42                |  0.0648984   | nan         | nan           | nan           |                |\n| user_obs[item]_43                | -0.100141    | nan         | nan           | nan           |                |\n| user_obs[item]_44                | -0.0892176   |  54.9148    |  -0.00162466  |   0.998704    |                |\n| user_obs[item]_45                |  0.0068979   |  34.171     |   0.000201864 |   0.999839    |                |\n| user_obs[item]_46                |  0.0980878   | 111.224     |   0.000881892 |   0.999296    |                |\n| user_obs[item]_47                |  0.100062    | nan         | nan           | nan           |                |\n| user_obs[item]_48                |  0.0889476   | 119.958     |   0.000741491 |   0.999408    |                |\n| user_obs[item]_49                | -0.0980555   |  87.2634    |  -0.00112367  |   0.999103    |                |\n| user_obs[item]_50                | -0.00933557  |  65.2444    |  -0.000143086 |   0.999886    |                |\n| user_obs[item]_51                |  0.0365757   |  19.2679    |   0.00189827  |   0.998485    |                |\n| user_obs[item]_52                | -0.00636163  |  51.2761    |  -0.000124066 |   0.999901    |                |\n| user_obs[item]_53                | -0.0921691   |  24.4548    |  -0.00376896  |   0.996993    |                |\n| user_obs[item]_54                |  0.0908732   |  57.6774    |   0.00157554  |   0.998743    |                |\n| user_obs[item]_55                |  0.0971482   |  20.9316    |   0.00464121  |   0.996297    |                |\n| user_obs[item]_56                |  0.0900242   |  49.1325    |   0.00183227  |   0.998538    |                |\n| user_obs[item]_57                | -0.0802871   | nan         | nan           | nan           |                |\n| user_obs[item]_58                |  0.070944    | nan         | nan           | nan           |                |\n| user_obs[item]_59                |  0.089703    | nan         | nan           | nan           |                |\n| user_obs[item]_60                |  0.0615047   | nan         | nan           | nan           |                |\n| user_obs[item]_61                |  0.101459    |  60.6506    |   0.00167284  |   0.998665    |                |\n| user_obs[item]_62                |  0.0843676   |  51.1154    |   0.00165053  |   0.998683    |                |\n| user_obs[item]_63                |  0.0744298   |  50.0475    |   0.00148718  |   0.998813    |                |\n| user_obs[item]_64                |  0.0484441   |  66.7117    |   0.000726171 |   0.999421    |                |\n| user_obs[item]_65                | -0.0932577   |  47.0875    |  -0.00198052  |   0.99842     |                |\n| user_obs[item]_66                |  0.0937148   | nan         | nan           | nan           |                |\n| user_obs[item]_67                |  0.0938114   | nan         | nan           | nan           |                |\n| user_obs[item]_68                |  0.0919321   | 144.463     |   0.000636369 |   0.999492    |                |\n| user_obs[item]_69                | -0.100955    | nan         | nan           | nan           |                |\n| user_obs[item]_70                | -0.0694414   |  28.964     |  -0.00239751  |   0.998087    |                |\n| user_obs[item]_71                |  0.0618674   | 141.564     |   0.000437029 |   0.999651    |                |\n| user_obs[item]_72                |  0.0887234   | nan         | nan           | nan           |                |\n| user_obs[item]_73                |  0.0547917   | nan         | nan           | nan           |                |\n| user_obs[item]_74                | -0.0225696   | nan         | nan           | nan           |                |\n| user_obs[item]_75                |  0.0974109   |  21.2039    |   0.004594    |   0.996335    |                |\n| user_obs[item]_76                | -0.0991129   | nan         | nan           | nan           |                |\n| user_obs[item]_77                |  0.100791    | nan         | nan           | nan           |                |\n| user_obs[item]_78                | -0.0773081   | nan         | nan           | nan           |                |\n| user_obs[item]_79                |  0.095945    |  21.3157    |   0.00450114  |   0.996409    |                |\n| user_obs[item]_80                | -0.0946032   |  42.3599    |  -0.00223332  |   0.998218    |                |\n| user_obs[item]_81                | -0.0911672   |   8.6325    |  -0.0105609   |   0.991574    |                |\n| user_obs[item]_82                |  0.0901775   |  43.12      |   0.00209132  |   0.998331    |                |\n| user_obs[item]_83                |  0.0354095   | nan         | nan           | nan           |                |\n| user_obs[item]_84                |  0.100448    |  28.0476    |   0.00358133  |   0.997143    |                |\n| user_obs[item]_85                | -0.0448048   | nan         | nan           | nan           |                |\n| user_obs[item]_86                |  0.0958236   |  45.4539    |   0.00210815  |   0.998318    |                |\n| user_obs[item]_87                |  0.0734553   |  43.357     |   0.00169419  |   0.998648    |                |\n| user_obs[item]_88                | -0.0863962   |  40.7907    |  -0.00211804  |   0.99831     |                |\n| user_obs[item]_89                |  0.0714083   |  39.8753    |   0.00179079  |   0.998571    |                |\n| user_obs[item]_90                | -0.0744745   |  82.7771    |  -0.000899699 |   0.999282    |                |\n| user_obs[item]_91                |  0.0926561   |  77.8328    |   0.00119045  |   0.99905     |                |\n| user_obs[item]_92                | -0.0641654   |  32.6775    |  -0.0019636   |   0.998433    |                |\n| user_obs[item]_93                | -0.0825138   |  62.9386    |  -0.00131102  |   0.998954    |                |\n| user_obs[item]_94                |  0.0997649   |  38.279     |   0.00260625  |   0.997921    |                |\n| user_obs[item]_95                |  0.0904881   | nan         | nan           | nan           |                |\n| user_obs[item]_96                |  0.0678373   | 151.095     |   0.00044897  |   0.999642    |                |\n| user_obs[item]_97                | -0.0679895   | nan         | nan           | nan           |                |\n| user_obs[item]_98                | -0.0945796   |  57.5709    |  -0.00164284  |   0.998689    |                |\n| user_obs[item]_99                | -0.0876268   | nan         | nan           | nan           |                |\n| user_obs[item]_100               |  0.0973275   | nan         | nan           | nan           |                |\n| user_obs[item]_101               |  0.0733176   | nan         | nan           | nan           |                |\n| user_obs[item]_102               | -0.0915377   | nan         | nan           | nan           |                |\n| user_obs[item]_103               |  0.0939341   |  36.7327    |   0.00255723  |   0.99796     |                |\n| user_obs[item]_104               | -0.0685851   |  57.6987    |  -0.00118868  |   0.999052    |                |\n| user_obs[item]_105               |  0.0894344   | nan         | nan           | nan           |                |\n| user_obs[item]_106               |  0.00271086  | nan         | nan           | nan           |                |\n| user_obs[item]_107               |  0.0411001   |  21.7561    |   0.00188913  |   0.998493    |                |\n| user_obs[item]_108               |  0.0987024   |  28.026     |   0.00352181  |   0.99719     |                |\n| user_obs[item]_109               |  0.0967847   |  26.5516    |   0.00364516  |   0.997092    |                |\n| user_obs[item]_110               |  0.025779    |  25.6526    |   0.00100493  |   0.999198    |                |\n| user_obs[item]_111               | -0.0944387   | nan         | nan           | nan           |                |\n| user_obs[item]_112               | -0.0997579   | nan         | nan           | nan           |                |\n| user_obs[item]_113               | -0.00786996  | nan         | nan           | nan           |                |\n| user_obs[item]_114               |  0.0873028   | nan         | nan           | nan           |                |\n| user_obs[item]_115               |  0.0969093   | nan         | nan           | nan           |                |\n| user_obs[item]_116               | -0.0933952   | nan         | nan           | nan           |                |\n| user_obs[item]_117               |  0.0953693   |  36.8322    |   0.00258929  |   0.997934    |                |\n| user_obs[item]_118               | -0.0711203   |  44.3295    |  -0.00160436  |   0.99872     |                |\n| user_obs[item]_119               |  0.0387712   | nan         | nan           | nan           |                |\n| user_obs[item]_120               | -0.0718235   |  72.4445    |  -0.000991428 |   0.999209    |                |\n| user_obs[item]_121               |  0.0965576   | nan         | nan           | nan           |                |\n| user_obs[item]_122               |  0.0932075   | nan         | nan           | nan           |                |\n| user_obs[item]_123               |  0.0960881   | nan         | nan           | nan           |                |\n| user_obs[item]_124               | -0.0057018   | nan         | nan           | nan           |                |\n| user_obs[item]_125               | -0.0240208   |  59.3318    |  -0.000404855 |   0.999677    |                |\n| user_obs[item]_126               | -0.0832291   | nan         | nan           | nan           |                |\n| user_obs[item]_127               | -0.0127958   |  31.8142    |  -0.000402203 |   0.999679    |                |\n| user_obs[item]_128               |  0.0946697   |  39.9644    |   0.00236885  |   0.99811     |                |\n| user_obs[item]_129               |  0.0955388   | nan         | nan           | nan           |                |\n| user_obs[item]_130               | -0.100848    |  29.1098    |  -0.00346438  |   0.997236    |                |\n| user_obs[item]_131               | -0.0897986   | nan         | nan           | nan           |                |\n| user_obs[item]_132               | -0.0893218   | nan         | nan           | nan           |                |\n| user_obs[item]_133               | -0.0987934   |  51.4924    |  -0.0019186   |   0.998469    |                |\n| user_obs[item]_134               | -0.0561973   |   8.48221   |  -0.00662531  |   0.994714    |                |\n| user_obs[item]_135               |  0.0591518   | nan         | nan           | nan           |                |\n| user_obs[item]_136               | -0.0981552   | nan         | nan           | nan           |                |\n| user_obs[item]_137               | -0.0920462   |  14.2351    |  -0.00646616  |   0.994841    |                |\n| user_obs[item]_138               | -0.088343    |  52.507     |  -0.0016825   |   0.998658    |                |\n| user_obs[item]_139               | -0.0847579   | nan         | nan           | nan           |                |\n| user_obs[item]_140               |  0.0964178   | nan         | nan           | nan           |                |\n| user_obs[item]_141               | -0.0954367   |  44.7388    |  -0.0021332   |   0.998298    |                |\n| user_obs[item]_142               |  0.0901083   |  59.9893    |   0.00150207  |   0.998802    |                |\n| user_obs[item]_143               | -0.0976047   | nan         | nan           | nan           |                |\n| user_obs[item]_144               |  0.0893714   |   9.06721   |   0.00985655  |   0.992136    |                |\n| user_obs[item]_145               |  0.0762919   | nan         | nan           | nan           |                |\n| user_obs[item]_146               |  0.100248    |  34.6165    |   0.00289595  |   0.997689    |                |\n| user_obs[item]_147               |  0.0919174   |  24.59      |   0.00373801  |   0.997018    |                |\n| user_obs[item]_148               |  0.0596661   |  15.9393    |   0.00374334  |   0.997013    |                |\n| user_obs[item]_149               |  0.0956791   |  30.0228    |   0.00318688  |   0.997457    |                |\n| user_obs[item]_150               |  0.0988285   | nan         | nan           | nan           |                |\n| user_obs[item]_151               | -0.0958375   | nan         | nan           | nan           |                |\n| user_obs[item]_152               | -0.0862783   | nan         | nan           | nan           |                |\n| user_obs[item]_153               | -0.096879    |   8.60707   |  -0.0112557   |   0.991019    |                |\n| user_obs[item]_154               |  0.0899647   |  26.4254    |   0.00340448  |   0.997284    |                |\n| user_obs[item]_155               | -0.0971653   |  17.2313    |  -0.00563889  |   0.995501    |                |\n| user_obs[item]_156               | -0.0347459   | nan         | nan           | nan           |                |\n| user_obs[item]_157               |  0.0946411   | nan         | nan           | nan           |                |\n| user_obs[item]_158               |  0.0965939   |  33.4381    |   0.00288874  |   0.997695    |                |\n| user_obs[item]_159               | -0.0987432   |  50.6435    |  -0.00194977  |   0.998444    |                |\n| user_obs[item]_160               |  0.0554879   |  54.8858    |   0.00101097  |   0.999193    |                |\n| user_obs[item]_161               |  0.0916695   | nan         | nan           | nan           |                |\n| user_obs[item]_162               |  0.0903534   |  21.761     |   0.00415209  |   0.996687    |                |\n| user_obs[item]_163               |  0.0925355   | nan         | nan           | nan           |                |\n| user_obs[item]_164               | -0.0157946   |  21.2459    |  -0.000743417 |   0.999407    |                |\n| user_obs[item]_165               | -0.0911579   | nan         | nan           | nan           |                |\n| user_obs[item]_166               |  0.0960882   |  22.7401    |   0.0042255   |   0.996629    |                |\n| user_obs[item]_167               | -0.0921166   |  33.1998    |  -0.00277461  |   0.997786    |                |\n| user_obs[item]_168               | -0.0952001   |  16.7478    |  -0.00568434  |   0.995465    |                |\n| user_obs[item]_169               |  0.0906606   | nan         | nan           | nan           |                |\n| user_obs[item]_170               | -0.0929105   |  34.9538    |  -0.0026581   |   0.997879    |                |\n| user_obs[item]_171               | -0.0856273   | nan         | nan           | nan           |                |\n| user_obs[item]_172               |  0.0977275   | nan         | nan           | nan           |                |\n| user_obs[item]_173               | -0.0964915   | nan         | nan           | nan           |                |\n| user_obs[item]_174               |  0.0961557   |  34.6991    |   0.00277113  |   0.997789    |                |\n| user_obs[item]_175               | -0.0868459   | nan         | nan           | nan           |                |\n| user_obs[item]_176               |  0.0997443   | nan         | nan           | nan           |                |\n| user_obs[item]_177               | -0.0936593   | nan         | nan           | nan           |                |\n| user_obs[item]_178               |  0.0896609   | nan         | nan           | nan           |                |\n| user_obs[item]_179               | -0.0921307   | nan         | nan           | nan           |                |\n| user_obs[item]_180               |  0.0989775   | nan         | nan           | nan           |                |\n| user_obs[item]_181               | -0.0934007   | nan         | nan           | nan           |                |\n| user_obs[item]_182               | -0.0954752   | nan         | nan           | nan           |                |\n| user_obs[item]_183               |  0.101086    | nan         | nan           | nan           |                |\n| user_obs[item]_184               | -0.042742    | nan         | nan           | nan           |                |\n| user_obs[item]_185               | -0.0904764   | nan         | nan           | nan           |                |\n| user_obs[item]_186               |  0.0580969   | nan         | nan           | nan           |                |\n| user_obs[item]_187               |  0.0472953   |  21.1506    |   0.00223613  |   0.998216    |                |\n| user_obs[item]_188               | -0.0861624   |  21.6303    |  -0.00398342  |   0.996822    |                |\n| user_obs[item]_189               | -0.0496282   |  30.8671    |  -0.0016078   |   0.998717    |                |\n| user_obs[item]_190               |  0.0980903   |  40.4903    |   0.00242256  |   0.998067    |                |\n| user_obs[item]_191               | -0.0999367   | nan         | nan           | nan           |                |\n| user_obs[item]_192               |  0.0937635   | nan         | nan           | nan           |                |\n| user_obs[item]_193               |  0.0932748   | nan         | nan           | nan           |                |\n| user_obs[item]_194               | -0.0912127   |  43.9686    |  -0.0020745   |   0.998345    |                |\n| user_obs[item]_195               | -0.0954091   |  56.3073    |  -0.00169444  |   0.998648    |                |\n| user_obs[item]_196               |  0.0946353   | nan         | nan           | nan           |                |\n| user_obs[item]_197               |  0.0775572   |  11.1232    |   0.00697254  |   0.994437    |                |\n| user_obs[item]_198               |  0.0917828   | nan         | nan           | nan           |                |\n| user_obs[item]_199               | -0.0971661   | nan         | nan           | nan           |                |\n| user_obs[item]_200               | -0.0954552   |  22.4761    |  -0.00424696  |   0.996611    |                |\n| user_obs[item]_201               | -0.0986228   | nan         | nan           | nan           |                |\n| user_obs[item]_202               | -0.0948749   |  50.355     |  -0.00188412  |   0.998497    |                |\n| user_obs[item]_203               |  0.0887569   | nan         | nan           | nan           |                |\n| user_obs[item]_204               | -0.0736365   | nan         | nan           | nan           |                |\n| user_obs[item]_205               |  0.0945059   |  21.2864    |   0.00443973  |   0.996458    |                |\n| user_obs[item]_206               |  0.0952938   | nan         | nan           | nan           |                |\n| user_obs[item]_207               |  0.0852773   |  31.1982    |   0.0027334   |   0.997819    |                |\n| user_obs[item]_208               |  0.0331864   | nan         | nan           | nan           |                |\n| user_obs[item]_209               |  0.0887002   | nan         | nan           | nan           |                |\n| user_obs[item]_210               | -0.0987527   | nan         | nan           | nan           |                |\n| user_obs[item]_211               |  0.0601945   |  14.0004    |   0.00429948  |   0.99657     |                |\n| user_obs[item]_212               | -0.100431    | nan         | nan           | nan           |                |\n| user_obs[item]_213               | -0.0950364   | nan         | nan           | nan           |                |\n| user_obs[item]_214               | -0.0846135   | nan         | nan           | nan           |                |\n| user_obs[item]_215               | -0.0983446   | nan         | nan           | nan           |                |\n| user_obs[item]_216               | -0.0899129   |  21.6253    |  -0.00415776  |   0.996683    |                |\n| user_obs[item]_217               | -0.0969675   | nan         | nan           | nan           |                |\n| user_obs[item]_218               | -0.0936005   |  19.6852    |  -0.00475488  |   0.996206    |                |\n| user_obs[item]_219               |  0.0243296   | nan         | nan           | nan           |                |\n| user_obs[item]_220               |  0.0801605   | nan         | nan           | nan           |                |\n| user_obs[item]_221               | -0.100749    | nan         | nan           | nan           |                |\n| user_obs[item]_222               | -0.0995452   |   7.18452   |  -0.0138555   |   0.988945    |                |\n| user_obs[item]_223               | -0.0914686   |   4.87038   |  -0.0187806   |   0.985016    |                |\n| user_obs[item]_224               | -0.0500416   | nan         | nan           | nan           |                |\n| user_obs[item]_225               |  0.00523383  | nan         | nan           | nan           |                |\n| user_obs[item]_226               | -0.00516245  |  35.3175    |  -0.000146172 |   0.999883    |                |\n| user_obs[item]_227               | -0.0835066   | nan         | nan           | nan           |                |\n| user_obs[item]_228               | -0.0900212   | nan         | nan           | nan           |                |\n| user_obs[item]_229               | -0.0921853   | nan         | nan           | nan           |                |\n| user_obs[item]_230               |  0.0933748   |  25.8102    |   0.00361775  |   0.997113    |                |\n| user_obs[item]_231               |  0.0884962   |   5.54233   |   0.0159673   |   0.98726     |                |\n| user_obs[item]_232               | -0.100834    |  31.0845    |  -0.00324386  |   0.997412    |                |\n| user_obs[item]_233               | -0.0706101   |  45.4504    |  -0.00155356  |   0.99876     |                |\n| user_obs[item]_234               | -0.100623    |   9.49371   |  -0.0105989   |   0.991543    |                |\n| user_obs[item]_235               | -0.0950226   |   9.10836   |  -0.0104325   |   0.991676    |                |\n| user_obs[item]_236               |  0.0787224   | nan         | nan           | nan           |                |\n| user_obs[item]_237               |  0.0516931   | nan         | nan           | nan           |                |\n| user_obs[item]_238               |  0.0795601   | nan         | nan           | nan           |                |\n| user_obs[item]_239               | -0.0991568   |  25.7028    |  -0.00385782  |   0.996922    |                |\n| user_obs[item]_240               | -0.100388    |  17.91      |  -0.00560511  |   0.995528    |                |\n| user_obs[item]_241               | -0.0956626   | nan         | nan           | nan           |                |\n| user_obs[item]_242               | -0.00400375  |  23.6861    |  -0.000169034 |   0.999865    |                |\n| user_obs[item]_243               |  0.0944553   | nan         | nan           | nan           |                |\n| user_obs[item]_244               |  0.0983843   |  48.3653    |   0.00203419  |   0.998377    |                |\n| user_obs[item]_245               | -0.100297    | nan         | nan           | nan           |                |\n| user_obs[item]_246               | -0.0124805   | nan         | nan           | nan           |                |\n| user_obs[item]_247               |  0.0965049   | nan         | nan           | nan           |                |\n| user_obs[item]_248               |  0.0854822   | nan         | nan           | nan           |                |\n| user_obs[item]_249               |  0.0931216   | nan         | nan           | nan           |                |\n| user_obs[item]_250               |  0.0158794   |  28.2352    |   0.000562395 |   0.999551    |                |\n| user_obs[item]_251               | -0.0952249   | nan         | nan           | nan           |                |\n| user_obs[item]_252               |  0.0919503   |  12.5916    |   0.00730253  |   0.994173    |                |\n| user_obs[item]_253               |  0.0828517   | nan         | nan           | nan           |                |\n| user_obs[item]_254               | -0.0932373   | nan         | nan           | nan           |                |\n| user_obs[item]_255               |  0.100438    |   9.87784   |   0.010168    |   0.991887    |                |\n| user_obs[item]_256               | -0.0985134   |  11.5526    |  -0.00852738  |   0.993196    |                |\n| user_obs[item]_257               | -0.095954    | nan         | nan           | nan           |                |\n| user_obs[item]_258               | -0.0860235   | nan         | nan           | nan           |                |\n| user_obs[item]_259               |  0.0939709   | nan         | nan           | nan           |                |\n| user_obs[item]_260               |  0.0654559   | nan         | nan           | nan           |                |\n| user_obs[item]_261               |  0.0999005   |  10.7345    |   0.00930653  |   0.992575    |                |\n| user_obs[item]_262               | -0.0964731   | nan         | nan           | nan           |                |\n| user_obs[item]_263               |  0.0928897   | nan         | nan           | nan           |                |\n| user_obs[item]_264               |  0.0999005   |  21.306     |   0.00468885  |   0.996259    |                |\n| user_obs[item]_265               | -0.0909176   |  11.2045    |  -0.00811436  |   0.993526    |                |\n| user_obs[item]_266               |  0.0922943   |  35.6997    |   0.0025853   |   0.997937    |                |\n| user_obs[item]_267               | -0.0804861   | nan         | nan           | nan           |                |\n| user_obs[item]_268               | -0.0950919   |  10.6069    |  -0.00896508  |   0.992847    |                |\n| user_obs[item]_269               | -0.0514438   | nan         | nan           | nan           |                |\n| user_obs[item]_270               | -0.0884027   |  45.7692    |  -0.00193149  |   0.998459    |                |\n| user_obs[item]_271               |  0.0939372   |   4.13603   |   0.0227119   |   0.98188     |                |\n| user_obs[item]_272               |  0.0456208   |  26.7976    |   0.00170242  |   0.998642    |                |\n| user_obs[item]_273               | -0.0987425   | nan         | nan           | nan           |                |\n| user_obs[item]_274               | -0.0907353   |  15.5872    |  -0.00582112  |   0.995355    |                |\n| user_obs[item]_275               | -0.0895816   | nan         | nan           | nan           |                |\n| user_obs[item]_276               |  0.0891188   | nan         | nan           | nan           |                |\n| user_obs[item]_277               | -0.0987827   | nan         | nan           | nan           |                |\n| user_obs[item]_278               | -0.0972348   | nan         | nan           | nan           |                |\n| user_obs[item]_279               |  0.0966328   | nan         | nan           | nan           |                |\n| user_obs[item]_280               |  0.0951051   | nan         | nan           | nan           |                |\n| user_obs[item]_281               |  0.0975423   |  25.7122    |   0.00379362  |   0.996973    |                |\n| user_obs[item]_282               | -0.0791925   | nan         | nan           | nan           |                |\n| user_obs[item]_283               |  0.0958204   | nan         | nan           | nan           |                |\n| user_obs[item]_284               | -0.03573     |   7.18889   |  -0.00497016  |   0.996034    |                |\n| user_obs[item]_285               |  0.0810994   | nan         | nan           | nan           |                |\n| user_obs[item]_286               | -0.0951336   | nan         | nan           | nan           |                |\n| user_obs[item]_287               |  0.0841253   |  30.7186    |   0.00273858  |   0.997815    |                |\n| user_obs[item]_288               | -0.089892    | nan         | nan           | nan           |                |\n| user_obs[item]_289               |  0.00667884  |  19.0526    |   0.000350547 |   0.99972     |                |\n| user_obs[item]_290               | -0.0998388   | nan         | nan           | nan           |                |\n| user_obs[item]_291               | -0.0946387   | nan         | nan           | nan           |                |\n| user_obs[item]_292               |  0.0621858   | nan         | nan           | nan           |                |\n| user_obs[item]_293               |  0.0741694   | nan         | nan           | nan           |                |\n| user_obs[item]_294               | -0.0965614   | nan         | nan           | nan           |                |\n| user_obs[item]_295               |  0.0154573   | nan         | nan           | nan           |                |\n| user_obs[item]_296               |  0.0974524   |  33.1169    |   0.00294268  |   0.997652    |                |\n| user_obs[item]_297               |  0.0196788   |  11.5686    |   0.00170105  |   0.998643    |                |\n| user_obs[item]_298               |  0.0849441   |  12.0542    |   0.00704682  |   0.994377    |                |\n| user_obs[item]_299               |  0.086225    | nan         | nan           | nan           |                |\n| user_obs[item]_300               | -0.0914505   | nan         | nan           | nan           |                |\n| user_obs[item]_301               |  0.0921167   |   6.46036   |   0.0142588   |   0.988624    |                |\n| user_obs[item]_302               | -0.0933694   | nan         | nan           | nan           |                |\n| user_obs[item]_303               | -0.0489442   |  29.5071    |  -0.00165873  |   0.998677    |                |\n| user_obs[item]_304               | -0.0915071   |  42.9878    |  -0.00212868  |   0.998302    |                |\n| user_obs[item]_305               | -0.0667167   | nan         | nan           | nan           |                |\n| user_obs[item]_306               | -0.0938267   | nan         | nan           | nan           |                |\n| user_obs[item]_307               |  0.0443927   |  20.2969    |   0.00218716  |   0.998255    |                |\n| user_obs[item]_308               |  0.0775837   |  13.0335    |   0.00595263  |   0.995251    |                |\n| user_obs[item]_309               |  0.0827553   |   9.05362   |   0.00914058  |   0.992707    |                |\n| user_obs[item]_310               |  0.098086    | nan         | nan           | nan           |                |\n| user_obs[item]_311               |  0.0934891   | nan         | nan           | nan           |                |\n| user_obs[item]_312               | -0.0943368   |  21.9152    |  -0.00430463  |   0.996565    |                |\n| user_obs[item]_313               | -0.0976427   |  22.4439    |  -0.00435051  |   0.996529    |                |\n| user_obs[item]_314               | -0.0774443   |   8.54967   |  -0.00905817  |   0.992773    |                |\n| user_obs[item]_315               |  0.0536131   |  25.3484    |   0.00211504  |   0.998312    |                |\n| user_obs[item]_316               |  0.0899316   |  13.7525    |   0.00653929  |   0.994782    |                |\n| user_obs[item]_317               |  0.0912012   | nan         | nan           | nan           |                |\n| user_obs[item]_318               | -0.0992509   | nan         | nan           | nan           |                |\n| user_obs[item]_319               | -0.0153901   |  19.0264    |  -0.000808883 |   0.999355    |                |\n| user_obs[item]_320               | -0.100451    |  18.9858    |  -0.00529086  |   0.995779    |                |\n| user_obs[item]_321               |  0.0648664   |   9.6448    |   0.00672553  |   0.994634    |                |\n| user_obs[item]_322               |  0.0956555   | nan         | nan           | nan           |                |\n| user_obs[item]_323               | -0.00337489  |  46.38      |  -7.2766e-05  |   0.999942    |                |\n| user_obs[item]_324               | -0.0889243   |  68.278     |  -0.00130239  |   0.998961    |                |\n| user_obs[item]_325               |  0.0320671   |  18.9087    |   0.00169589  |   0.998647    |                |\n| user_obs[item]_326               | -0.0959391   |  13.0347    |  -0.0073603   |   0.994127    |                |\n| user_obs[item]_327               |  0.00849687  | nan         | nan           | nan           |                |\n| user_obs[item]_328               |  0.0979957   |  17.4037    |   0.00563072  |   0.995507    |                |\n| user_obs[item]_329               |  0.0999434   |  13.6447    |   0.00732472  |   0.994156    |                |\n| user_obs[item]_330               |  0.0902393   |  18.4601    |   0.00488835  |   0.9961      |                |\n| user_obs[item]_331               | -0.0937392   | nan         | nan           | nan           |                |\n| user_obs[item]_332               | -0.0918172   | nan         | nan           | nan           |                |\n| user_obs[item]_333               |  0.077242    | nan         | nan           | nan           |                |\n| user_obs[item]_334               |  0.0490345   | nan         | nan           | nan           |                |\n| user_obs[item]_335               | -0.0348693   |  25.5131    |  -0.00136672  |   0.99891     |                |\n| user_obs[item]_336               | -0.0219657   | nan         | nan           | nan           |                |\n| user_obs[item]_337               | -0.0996667   |   5.13986   |  -0.0193909   |   0.984529    |                |\n| user_obs[item]_338               |  0.0939913   | nan         | nan           | nan           |                |\n| user_obs[item]_339               |  0.0801734   | nan         | nan           | nan           |                |\n| user_obs[item]_340               |  0.0905974   | nan         | nan           | nan           |                |\n| user_obs[item]_341               |  0.0943427   |  24.0998    |   0.00391467  |   0.996877    |                |\n| user_obs[item]_342               | -0.0936997   |  22.9254    |  -0.00408717  |   0.996739    |                |\n| user_obs[item]_343               |  0.0956675   | nan         | nan           | nan           |                |\n| user_obs[item]_344               |  0.0924433   | nan         | nan           | nan           |                |\n| user_obs[item]_345               |  0.101126    |  30.9882    |   0.00326336  |   0.997396    |                |\n| user_obs[item]_346               |  0.0918561   |  30.0324    |   0.00305856  |   0.99756     |                |\n| user_obs[item]_347               | -0.0245837   |  38.7188    |  -0.000634929 |   0.999493    |                |\n| user_obs[item]_348               | -0.0896538   | nan         | nan           | nan           |                |\n| user_obs[item]_349               |  0.0947577   | nan         | nan           | nan           |                |\n| user_obs[item]_350               |  0.0985861   |  13.4305    |   0.00734046  |   0.994143    |                |\n| user_obs[item]_351               |  0.0979897   | nan         | nan           | nan           |                |\n| user_obs[item]_352               |  0.0999614   |  21.67      |   0.00461289  |   0.996319    |                |\n| user_obs[item]_353               |  0.0822363   |  10.8624    |   0.00757076  |   0.993959    |                |\n| user_obs[item]_354               | -0.0967529   | nan         | nan           | nan           |                |\n| user_obs[item]_355               | -0.0241711   |  11.2837    |  -0.00214212  |   0.998291    |                |\n| user_obs[item]_356               |  0.0973434   | nan         | nan           | nan           |                |\n| user_obs[item]_357               | -0.0788176   |   5.33626   |  -0.0147702   |   0.988216    |                |\n| user_obs[item]_358               | -0.0964527   | nan         | nan           | nan           |                |\n| user_obs[item]_359               | -0.0906674   | nan         | nan           | nan           |                |\n| user_obs[item]_360               |  0.0743494   |  19.7904    |   0.00375685  |   0.997002    |                |\n| user_obs[item]_361               | -0.0918878   |  15.1822    |  -0.00605235  |   0.995171    |                |\n| user_obs[item]_362               |  0.0670612   | nan         | nan           | nan           |                |\n| user_obs[item]_363               |  0.0951579   |   7.74944   |   0.0122793   |   0.990203    |                |\n| user_obs[item]_364               |  0.0961641   |  10.0334    |   0.00958435  |   0.992353    |                |\n| user_obs[item]_365               | -0.0716678   | nan         | nan           | nan           |                |\n| user_obs[item]_366               |  0.000673009 | nan         | nan           | nan           |                |\n| user_obs[item]_367               |  0.0941488   | nan         | nan           | nan           |                |\n| user_obs[item]_368               |  0.100942    | nan         | nan           | nan           |                |\n| user_obs[item]_369               |  0.0993893   | nan         | nan           | nan           |                |\n| user_obs[item]_370               |  0.100108    |  11.1383    |   0.00898776  |   0.992829    |                |\n| user_obs[item]_371               |  0.0221737   | nan         | nan           | nan           |                |\n| user_obs[item]_372               | -0.098036    | nan         | nan           | nan           |                |\n| user_obs[item]_373               |  0.0977015   | nan         | nan           | nan           |                |\n| user_obs[item]_374               | -0.0474964   |   6.59998   |  -0.00719644  |   0.994258    |                |\n| user_obs[item]_375               | -0.0979622   |  18.7567    |  -0.00522279  |   0.995833    |                |\n| user_obs[item]_376               |  0.0885426   | nan         | nan           | nan           |                |\n| user_obs[item]_377               | -0.0919497   | nan         | nan           | nan           |                |\n| user_obs[item]_378               | -0.0564472   | nan         | nan           | nan           |                |\n| user_obs[item]_379               |  0.0964465   |   7.6299    |   0.0126406   |   0.989915    |                |\n| user_obs[item]_380               |  0.0248318   |  21.5719    |   0.00115112  |   0.999082    |                |\n| user_obs[item]_381               | -0.10122     | nan         | nan           | nan           |                |\n| user_obs[item]_382               |  0.0836685   |  30.0802    |   0.00278152  |   0.997781    |                |\n| user_obs[item]_383               | -0.00442575  | nan         | nan           | nan           |                |\n| item_obs[user]_0                 |  0.0953294   | nan         | nan           | nan           |                |\n| item_obs[user]_1                 |  0.0849606   | nan         | nan           | nan           |                |\n| item_obs[user]_2                 | -0.074826    |  11.5431    |  -0.00648234  |   0.994828    |                |\n| item_obs[user]_3                 | -0.0930095   | nan         | nan           | nan           |                |\n| item_obs[user]_4                 | -0.088183    | nan         | nan           | nan           |                |\n| item_obs[user]_5                 | -0.0985564   | nan         | nan           | nan           |                |\n| item_obs[user]_6                 |  0.082744    |  24.3046    |   0.00340446  |   0.997284    |                |\n| item_obs[user]_7                 | -0.0800593   |  27.0081    |  -0.00296427  |   0.997635    |                |\n| item_obs[user]_8                 | -0.0951243   | nan         | nan           | nan           |                |\n| item_obs[user]_9                 | -0.0879356   | nan         | nan           | nan           |                |\n| item_obs[user]_10                |  0.096148    |  34.1177    |   0.00281813  |   0.997751    |                |\n| item_obs[user]_11                | -0.0197937   | nan         | nan           | nan           |                |\n| item_obs[user]_12                |  0.00669822  |  37.6851    |   0.000177742 |   0.999858    |                |\n| item_obs[user]_13                | -0.0797179   | nan         | nan           | nan           |                |\n| item_obs[user]_14                |  0.036266    | nan         | nan           | nan           |                |\n| item_obs[user]_15                |  0.0701227   | nan         | nan           | nan           |                |\n| item_obs[user]_16                |  0.0723617   | nan         | nan           | nan           |                |\n| item_obs[user]_17                |  0.0411248   | nan         | nan           | nan           |                |\n| item_obs[user]_18                | -0.0554874   |  22.3587    |  -0.00248169  |   0.99802     |                |\n| item_obs[user]_19                | -0.0863106   | nan         | nan           | nan           |                |\n| item_obs[user]_20                |  0.0677762   | nan         | nan           | nan           |                |\n| item_obs[user]_21                | -0.0538812   | nan         | nan           | nan           |                |\n| item_obs[user]_22                |  0.0669267   | nan         | nan           | nan           |                |\n| item_obs[user]_23                | -0.064349    | nan         | nan           | nan           |                |\n| item_obs[user]_24                | -0.0746401   | nan         | nan           | nan           |                |\n| item_obs[user]_25                | -0.0784752   | nan         | nan           | nan           |                |\n| item_obs[user]_26                |  0.0728878   |  36.6919    |   0.00198648  |   0.998415    |                |\n| item_obs[user]_27                |  0.0765257   | nan         | nan           | nan           |                |\n| item_obs[user]_28                |  0.079921    | nan         | nan           | nan           |                |\n| item_obs[user]_29                |  0.0753488   |  44.7365    |   0.00168428  |   0.998656    |                |\n| item_obs[user]_30                |  0.0669301   |  14.2758    |   0.00468836  |   0.996259    |                |\n| item_obs[user]_31                | -0.0371062   | nan         | nan           | nan           |                |\n| item_obs[user]_32                |  0.0682146   | nan         | nan           | nan           |                |\n| item_obs[user]_33                |  0.0834677   | nan         | nan           | nan           |                |\n| item_obs[user]_34                |  0.0153546   | nan         | nan           | nan           |                |\n| item_obs[user]_35                |  0.0685816   | nan         | nan           | nan           |                |\n| item_obs[user]_36                | -0.0880266   |  29.742     |  -0.00295968  |   0.997639    |                |\n| item_obs[user]_37                | -0.0885725   |   5.39468   |  -0.0164185   |   0.986901    |                |\n| item_obs[user]_38                |  0.09194     | nan         | nan           | nan           |                |\n| item_obs[user]_39                |  0.0756242   | nan         | nan           | nan           |                |\n| item_obs[user]_40                | -0.0287947   | nan         | nan           | nan           |                |\n| item_obs[user]_41                |  0.0585076   |  13.8805    |   0.0042151   |   0.996637    |                |\n| item_obs[user]_42                |  0.0564173   |   6.45187   |   0.00874434  |   0.993023    |                |\n| item_obs[user]_43                |  0.0635485   |   8.21708   |   0.0077337   |   0.993829    |                |\n| item_obs[user]_44                | -0.0873843   |  48.6134    |  -0.00179754  |   0.998566    |                |\n| item_obs[user]_45                | -0.0834753   |  94.0016    |  -0.00088802  |   0.999291    |                |\n| item_obs[user]_46                |  0.0764609   |  14.8363    |   0.00515363  |   0.995888    |                |\n| item_obs[user]_47                |  0.0274219   |  43.366     |   0.000632336 |   0.999495    |                |\n| item_obs[user]_48                | -0.0873803   | nan         | nan           | nan           |                |\n| item_obs[user]_49                | -0.0816547   |   2.08968   |  -0.0390751   |   0.96883     |                |\n| item_obs[user]_50                | -0.0226442   | nan         | nan           | nan           |                |\n| item_obs[user]_51                | -0.0956599   | nan         | nan           | nan           |                |\n| item_obs[user]_52                |  0.0783386   | nan         | nan           | nan           |                |\n| item_obs[user]_53                | -0.0633794   |  13.9623    |  -0.00453934  |   0.996378    |                |\n| item_obs[user]_54                |  0.0953538   |  26.6147    |   0.00358275  |   0.997141    |                |\n| item_obs[user]_55                |  0.055353    | nan         | nan           | nan           |                |\n| item_obs[user]_56                |  0.0908716   | nan         | nan           | nan           |                |\n| item_obs[user]_57                |  0.0607073   |  41.2198    |   0.00147277  |   0.998825    |                |\n| item_obs[user]_58                | -0.0720462   |  19.1767    |  -0.00375698  |   0.997002    |                |\n| item_obs[user]_59                | -0.0603374   |  54.1613    |  -0.00111403  |   0.999111    |                |\n| item_obs[user]_60                | -0.0185613   | nan         | nan           | nan           |                |\n| item_obs[user]_61                |  0.0708304   |   9.38333   |   0.00754853  |   0.993977    |                |\n| item_obs[user]_62                |  0.0791017   |   7.72274   |   0.0102427   |   0.991828    |                |\n| item_obs[user]_63                | -0.0571787   | nan         | nan           | nan           |                |\n| item_obs[user]_64                |  0.0329169   | 121.471     |   0.000270985 |   0.999784    |                |\n| item_obs[user]_65                |  0.0226816   | nan         | nan           | nan           |                |\n| item_obs[user]_66                | -0.00637343  | nan         | nan           | nan           |                |\n| item_obs[user]_67                | -0.0545586   | nan         | nan           | nan           |                |\n| item_obs[user]_68                | -0.0317221   |  32.09      |  -0.000988538 |   0.999211    |                |\n| item_obs[user]_69                | -0.0943223   |  17.817     |  -0.00529395  |   0.995776    |                |\n| item_obs[user]_70                |  0.00471362  | nan         | nan           | nan           |                |\n| item_obs[user]_71                | -0.00793153  | nan         | nan           | nan           |                |\n| item_obs[user]_72                | -0.0335794   | nan         | nan           | nan           |                |\n| item_obs[user]_73                | -0.0309263   |   2.68062   |  -0.011537    |   0.990795    |                |\n| item_obs[user]_74                |  0.0303063   | nan         | nan           | nan           |                |\n| item_obs[user]_75                | -0.0732667   | nan         | nan           | nan           |                |\n| item_obs[user]_76                | -0.0098093   |  27.1032    |  -0.000361924 |   0.999711    |                |\n| item_obs[user]_77                | -0.0138997   | nan         | nan           | nan           |                |\n| item_obs[user]_78                | -0.0118171   |   7.5586    |  -0.0015634   |   0.998753    |                |\n| item_obs[user]_79                |  0.0233654   | nan         | nan           | nan           |                |\n| item_obs[user]_80                |  0.0030736   |  37.2507    |   8.25112e-05 |   0.999934    |                |\n| item_obs[user]_81                |  0.0177793   | nan         | nan           | nan           |                |\n| item_obs[user]_82                | -0.071604    |  15.0995    |  -0.00474214  |   0.996216    |                |\n| item_obs[user]_83                | -0.0261888   | nan         | nan           | nan           |                |\n| item_obs[user]_84                |  0.0011142   | nan         | nan           | nan           |                |\n| item_obs[user]_85                |  0.0014559   |  47.6001    |   3.0586e-05  |   0.999976    |                |\n| item_obs[user]_86                |  0.00310078  | nan         | nan           | nan           |                |\n| item_obs[user]_87                |  0.0195079   |   0.511697  |   0.038124    |   0.969589    |                |\n| item_obs[user]_88                | -0.00919021  |  19.8504    |  -0.000462973 |   0.999631    |                |\n| item_obs[user]_89                | -0.0123839   | nan         | nan           | nan           |                |\n| item_obs[user]_90                | -0.0103893   | nan         | nan           | nan           |                |\n| item_obs[user]_91                |  0.0105079   | nan         | nan           | nan           |                |\n| item_obs[user]_92                |  0.0145338   | nan         | nan           | nan           |                |\n| item_obs[user]_93                |  0.00834626  | nan         | nan           | nan           |                |\n| item_obs[user]_94                |  0.00379122  |  26.508     |   0.000143022 |   0.999886    |                |\n| item_obs[user]_95                |  0.0260624   | nan         | nan           | nan           |                |\n| item_obs[user]_96                | -0.00103211  |  30.4632    |  -3.38804e-05 |   0.999973    |                |\n| item_obs[user]_97                |  0.00749817  | nan         | nan           | nan           |                |\n| item_obs[user]_98                | -0.00386528  | nan         | nan           | nan           |                |\n| item_obs[user]_99                | -0.00175826  |  14.1111    |  -0.000124602 |   0.999901    |                |\n| item_obs[user]_100               | -0.0241035   |  25.1352    |  -0.000958957 |   0.999235    |                |\n| item_obs[user]_101               | -0.0250482   |  10.4112    |  -0.00240589  |   0.99808     |                |\n| item_obs[user]_102               |  0.055889    | nan         | nan           | nan           |                |\n| item_obs[user]_103               |  0.000421926 | nan         | nan           | nan           |                |\n| item_obs[user]_104               |  0.0110912   | nan         | nan           | nan           |                |\n| item_obs[user]_105               | -0.00341909  | nan         | nan           | nan           |                |\n| item_obs[user]_106               | -0.00352724  | nan         | nan           | nan           |                |\n| item_obs[user]_107               | -0.00643651  | nan         | nan           | nan           |                |\n| item_obs[user]_108               | -0.0288748   | 107.796     |  -0.000267866 |   0.999786    |                |\n| item_obs[user]_109               | -0.016833    | nan         | nan           | nan           |                |\n| item_obs[user]_110               |  0.0100099   | nan         | nan           | nan           |                |\n| item_obs[user]_111               | -0.0096791   |  34.0665    |  -0.000284123 |   0.999773    |                |\n| item_obs[user]_112               | -0.0285409   |  80.6287    |  -0.000353979 |   0.999718    |                |\n| item_obs[user]_113               | -0.0169635   |  17.9643    |  -0.000944288 |   0.999247    |                |\n| item_obs[user]_114               | -0.00225602  |  31.8136    |  -7.09138e-05 |   0.999943    |                |\n| item_obs[user]_115               | -0.0541435   |  60.7676    |  -0.000890993 |   0.999289    |                |\n| item_obs[user]_116               |  0.00810096  |  20.7395    |   0.000390605 |   0.999688    |                |\n| item_obs[user]_117               | -0.0162214   |  37.0959    |  -0.000437283 |   0.999651    |                |\n| item_obs[user]_118               |  0.0258092   |  10.9363    |   0.00235996  |   0.998117    |                |\n| item_obs[user]_119               | -2.26311e-05 |  40.026     |  -5.65411e-07 |   1           |                |\n| item_obs[user]_120               |  0.0415286   |  13.5548    |   0.00306376  |   0.997555    |                |\n| item_obs[user]_121               | -0.0207364   | nan         | nan           | nan           |                |\n| item_obs[user]_122               | -0.0065696   |  20.8918    |  -0.000314458 |   0.999749    |                |\n| item_obs[user]_123               |  0.0190524   |  14.869     |   0.00128136  |   0.998978    |                |\n| item_obs[user]_124               |  0.0316112   | nan         | nan           | nan           |                |\n| item_obs[user]_125               |  0.00614487  | nan         | nan           | nan           |                |\n| item_obs[user]_126               |  0.0134792   |  19.8742    |   0.000678223 |   0.999459    |                |\n| item_obs[user]_127               | -0.0351917   | nan         | nan           | nan           |                |\n| item_obs[user]_128               |  0.082355    | nan         | nan           | nan           |                |\n| item_obs[user]_129               |  0.095309    |  18.8028    |   0.00506887  |   0.995956    |                |\n| item_obs[user]_130               | -0.0879696   |  29.2859    |  -0.00300382  |   0.997603    |                |\n| item_obs[user]_131               | -0.0830043   |  37.5038    |  -0.00221323  |   0.998234    |                |\n| item_obs[user]_132               | -0.0896925   | nan         | nan           | nan           |                |\n| item_obs[user]_133               | -0.0930006   | nan         | nan           | nan           |                |\n| item_obs[user]_134               |  0.0713312   |  37.349     |   0.00190986  |   0.998476    |                |\n| item_obs[user]_135               | -0.0770028   |  15.9662    |  -0.00482285  |   0.996152    |                |\n| item_obs[user]_136               | -0.0784881   |  36.5061    |  -0.00215     |   0.998285    |                |\n| item_obs[user]_137               | -0.0899085   | nan         | nan           | nan           |                |\n| item_obs[user]_138               |  0.0795249   |  52.347     |   0.00151919  |   0.998788    |                |\n| item_obs[user]_139               | -0.100354    |  56.5839    |  -0.00177354  |   0.998585    |                |\n| item_obs[user]_140               | -0.0748308   | nan         | nan           | nan           |                |\n| item_obs[user]_141               | -0.0945848   | nan         | nan           | nan           |                |\n| item_obs[user]_142               | -0.0841109   | nan         | nan           | nan           |                |\n| item_obs[user]_143               |  0.0791375   | nan         | nan           | nan           |                |\n| item_obs[user]_144               |  0.075823    |  24.9554    |   0.00303834  |   0.997576    |                |\n| item_obs[user]_145               |  0.0782816   | nan         | nan           | nan           |                |\n| item_obs[user]_146               | -0.100106    |  33.423     |  -0.00299513  |   0.99761     |                |\n| item_obs[user]_147               | -0.0894256   |  43.4752    |  -0.00205694  |   0.998359    |                |\n| item_obs[user]_148               | -0.0748489   |   6.55435   |  -0.0114197   |   0.990889    |                |\n| item_obs[user]_149               |  0.0691177   | nan         | nan           | nan           |                |\n| item_obs[user]_150               | -0.0574347   |  16.2724    |  -0.00352958  |   0.997184    |                |\n| item_obs[user]_151               |  0.0294429   | nan         | nan           | nan           |                |\n| item_obs[user]_152               |  0.00563431  |  39.8466    |   0.0001414   |   0.999887    |                |\n| item_obs[user]_153               | -0.096552    |  17.4384    |  -0.00553676  |   0.995582    |                |\n| item_obs[user]_154               |  0.0450145   | nan         | nan           | nan           |                |\n| item_obs[user]_155               |  0.0976836   |  50.7942    |   0.00192313  |   0.998466    |                |\n| item_obs[user]_156               |  0.0980685   |  14.9795    |   0.00654686  |   0.994776    |                |\n| item_obs[user]_157               |  0.0968644   |  21.2062    |   0.00456774  |   0.996355    |                |\n| item_obs[user]_158               | -0.0524446   |  17.5892    |  -0.00298163  |   0.997621    |                |\n| item_obs[user]_159               |  0.0945053   |  54.6474    |   0.00172936  |   0.99862     |                |\n| item_obs[user]_160               | -0.0973398   | nan         | nan           | nan           |                |\n| item_obs[user]_161               |  0.0734301   |  26.8878    |   0.00273098  |   0.997821    |                |\n| item_obs[user]_162               | -0.0609306   |  22.0643    |  -0.0027615   |   0.997797    |                |\n| item_obs[user]_163               | -0.101368    | nan         | nan           | nan           |                |\n| item_obs[user]_164               | -0.0810076   |  16.8674    |  -0.00480262  |   0.996168    |                |\n| item_obs[user]_165               | -0.080903    | nan         | nan           | nan           |                |\n| item_obs[user]_166               |  0.0833991   | nan         | nan           | nan           |                |\n| item_obs[user]_167               |  0.0658189   |  27.7891    |   0.00236851  |   0.99811     |                |\n| item_obs[user]_168               |  0.0812031   |   7.48511   |   0.0108486   |   0.991344    |                |\n| item_obs[user]_169               | -0.0805171   | nan         | nan           | nan           |                |\n| item_obs[user]_170               | -0.0787244   | nan         | nan           | nan           |                |\n| item_obs[user]_171               | -0.0975392   |   8.79285   |  -0.011093    |   0.991149    |                |\n| item_obs[user]_172               | -0.0924256   |  31.3937    |  -0.00294408  |   0.997651    |                |\n| item_obs[user]_173               | -0.0832566   |  46.9572    |  -0.00177303  |   0.998585    |                |\n| item_obs[user]_174               |  0.0980777   |   6.70879   |   0.0146193   |   0.988336    |                |\n| item_obs[user]_175               | -0.078849    | nan         | nan           | nan           |                |\n| item_obs[user]_176               | -0.0933594   | nan         | nan           | nan           |                |\n| item_obs[user]_177               | -0.0952743   |  26.0786    |  -0.00365334  |   0.997085    |                |\n| item_obs[user]_178               | -0.0432346   |  44.2405    |  -0.000977262 |   0.99922     |                |\n| item_obs[user]_179               | -0.0840413   |  26.3496    |  -0.00318947  |   0.997455    |                |\n| item_obs[user]_180               |  0.0800613   |  11.4991    |   0.00696239  |   0.994445    |                |\n| item_obs[user]_181               | -0.0740749   |  24.541     |  -0.00301842  |   0.997592    |                |\n| item_obs[user]_182               |  0.0782861   |  40.5394    |   0.00193111  |   0.998459    |                |\n| item_obs[user]_183               | -0.0634375   |  34.7348    |  -0.00182634  |   0.998543    |                |\n| item_obs[user]_184               |  0.0908027   |  12.6804    |   0.0071609   |   0.994286    |                |\n| item_obs[user]_185               |  0.0148654   | nan         | nan           | nan           |                |\n| item_obs[user]_186               |  0.0153121   |   7.12636   |   0.00214866  |   0.998286    |                |\n| item_obs[user]_187               |  0.0999311   |  22.8033    |   0.0043823   |   0.996503    |                |\n| item_obs[user]_188               |  0.0924136   | nan         | nan           | nan           |                |\n| item_obs[user]_189               | -0.0313981   | nan         | nan           | nan           |                |\n| item_obs[user]_190               |  0.0985501   |  10.9833    |   0.0089727   |   0.992841    |                |\n| item_obs[user]_191               | -0.0859244   |  36.5846    |  -0.00234865  |   0.998126    |                |\n| item_obs[user]_192               |  0.0940064   | nan         | nan           | nan           |                |\n| item_obs[user]_193               |  0.0832584   |  32.0548    |   0.00259738  |   0.997928    |                |\n| item_obs[user]_194               |  0.100225    | nan         | nan           | nan           |                |\n| item_obs[user]_195               | -0.0909778   |   9.19876   |  -0.00989022  |   0.992109    |                |\n| item_obs[user]_196               | -0.0874979   | nan         | nan           | nan           |                |\n| item_obs[user]_197               | -0.0938457   |  29.175     |  -0.00321665  |   0.997433    |                |\n| item_obs[user]_198               | -0.0937929   |  15.6314    |  -0.00600031  |   0.995212    |                |\n| item_obs[user]_199               |  0.0990231   | nan         | nan           | nan           |                |\n| item_obs[user]_200               | -0.0921627   |  11.5725    |  -0.00796393  |   0.993646    |                |\n| item_obs[user]_201               | -0.0871917   |  23.6936    |  -0.00367996  |   0.997064    |                |\n| item_obs[user]_202               |  0.0933521   | nan         | nan           | nan           |                |\n| item_obs[user]_203               | -0.0949833   | nan         | nan           | nan           |                |\n| item_obs[user]_204               | -0.0959055   | nan         | nan           | nan           |                |\n| item_obs[user]_205               |  0.0117426   | nan         | nan           | nan           |                |\n| item_obs[user]_206               | -0.0963619   |   6.62996   |  -0.0145343   |   0.988404    |                |\n| item_obs[user]_207               |  0.0942454   |  16.5252    |   0.00570314  |   0.99545     |                |\n| item_obs[user]_208               | -0.0993581   | nan         | nan           | nan           |                |\n| item_obs[user]_209               |  0.0948967   |   4.04483   |   0.0234612   |   0.981282    |                |\n| item_obs[user]_210               | -0.0947503   | nan         | nan           | nan           |                |\n| item_obs[user]_211               | -0.0834834   | nan         | nan           | nan           |                |\n| item_obs[user]_212               | -0.0986454   | nan         | nan           | nan           |                |\n| item_obs[user]_213               |  0.0977547   |  17.5796    |   0.00556067  |   0.995563    |                |\n| item_obs[user]_214               | -0.0991265   |  25.6013    |  -0.00387194  |   0.996911    |                |\n| item_obs[user]_215               |  0.0972205   |   3.50911   |   0.0277052   |   0.977897    |                |\n| item_obs[user]_216               |  0.0301802   |  24.2897    |   0.00124251  |   0.999009    |                |\n| item_obs[user]_217               |  0.0561406   |  13.9829    |   0.00401496  |   0.996797    |                |\n| item_obs[user]_218               | -0.098299    |  25.9009    |  -0.0037952   |   0.996972    |                |\n| item_obs[user]_219               | -0.0992086   |  32.7797    |  -0.00302652  |   0.997585    |                |\n| item_obs[user]_220               |  0.0253464   | nan         | nan           | nan           |                |\n| item_obs[user]_221               | -0.100714    |  23.3067    |  -0.00432123  |   0.996552    |                |\n| item_obs[user]_222               | -0.0995028   |  19.086     |  -0.0052134   |   0.99584     |                |\n| item_obs[user]_223               |  0.0961196   | nan         | nan           | nan           |                |\n| item_obs[user]_224               | -0.0984098   | nan         | nan           | nan           |                |\n| item_obs[user]_225               | -0.039659    | nan         | nan           | nan           |                |\n| item_obs[user]_226               | -0.0970929   | nan         | nan           | nan           |                |\n| item_obs[user]_227               | -0.0983761   | nan         | nan           | nan           |                |\n| item_obs[user]_228               | -0.0828517   |  31.4185    |  -0.00263703  |   0.997896    |                |\n| item_obs[user]_229               | -0.0843641   |  14.2476    |  -0.00592128  |   0.995276    |                |\n| item_obs[user]_230               |  0.0902104   | nan         | nan           | nan           |                |\n| item_obs[user]_231               | -0.0994968   |  29.1198    |  -0.00341681  |   0.997274    |                |\n| item_obs[user]_232               |  0.0962286   | nan         | nan           | nan           |                |\n| item_obs[user]_233               | -0.0975643   |  35.3898    |  -0.00275685  |   0.9978      |                |\n| item_obs[user]_234               | -0.09747     | nan         | nan           | nan           |                |\n| item_obs[user]_235               | -0.0976395   | nan         | nan           | nan           |                |\n| item_obs[user]_236               | -0.0871684   | nan         | nan           | nan           |                |\n| item_obs[user]_237               | -0.0422475   | nan         | nan           | nan           |                |\n| item_obs[user]_238               | -0.100429    |  15.8252    |  -0.0063461   |   0.994937    |                |\n| item_obs[user]_239               | -0.0962664   |  44.1614    |  -0.00217988  |   0.998261    |                |\n| item_obs[user]_240               | -0.0874716   | nan         | nan           | nan           |                |\n| item_obs[user]_241               | -0.0550082   | nan         | nan           | nan           |                |\n| item_obs[user]_242               |  0.0942492   |  19.712     |   0.00478131  |   0.996185    |                |\n| item_obs[user]_243               | -0.0925776   | nan         | nan           | nan           |                |\n| item_obs[user]_244               | -0.100297    |  10.4662    |  -0.00958297  |   0.992354    |                |\n| item_obs[user]_245               | -0.0942868   | nan         | nan           | nan           |                |\n| item_obs[user]_246               |  0.0936042   | nan         | nan           | nan           |                |\n| item_obs[user]_247               | -0.0982692   |  41.5473    |  -0.00236524  |   0.998113    |                |\n| item_obs[user]_248               |  0.0904482   | nan         | nan           | nan           |                |\n| item_obs[user]_249               | -0.0960117   | nan         | nan           | nan           |                |\n| item_obs[user]_250               |  0.100448    |  29.5796    |   0.00339585  |   0.997291    |                |\n| item_obs[user]_251               |  0.0970124   | nan         | nan           | nan           |                |\n| item_obs[user]_252               |  0.0957162   | nan         | nan           | nan           |                |\n| item_obs[user]_253               | -0.100491    |   8.99075   |  -0.0111771   |   0.991082    |                |\n| item_obs[user]_254               | -0.000854391 | nan         | nan           | nan           |                |\n| item_obs[user]_255               | -0.0946775   |  16.1911    |  -0.00584749  |   0.995334    |                |\n| item_obs[user]_256               | -0.0970131   | nan         | nan           | nan           |                |\n| item_obs[user]_257               | -0.0529265   |  89.3827    |  -0.000592134 |   0.999528    |                |\n| item_obs[user]_258               |  0.0968852   |  23.193     |   0.00417735  |   0.996667    |                |\n| item_obs[user]_259               |  0.0969059   | nan         | nan           | nan           |                |\n| item_obs[user]_260               |  0.0915281   | nan         | nan           | nan           |                |\n| item_obs[user]_261               | -0.100793    | nan         | nan           | nan           |                |\n| item_obs[user]_262               | -0.0983033   |  51.1771    |  -0.00192084  |   0.998467    |                |\n| item_obs[user]_263               |  0.0977857   | nan         | nan           | nan           |                |\n| item_obs[user]_264               |  0.098056    |  12.0407    |   0.0081437   |   0.993502    |                |\n| item_obs[user]_265               |  0.0912735   | nan         | nan           | nan           |                |\n| item_obs[user]_266               | -0.0979466   |  26.0132    |  -0.00376527  |   0.996996    |                |\n| item_obs[user]_267               | -0.0996658   | nan         | nan           | nan           |                |\n| item_obs[user]_268               |  0.0991034   |  11.0936    |   0.00893336  |   0.992872    |                |\n| item_obs[user]_269               |  0.0929837   | nan         | nan           | nan           |                |\n| item_obs[user]_270               | -0.0987016   |  21.8965    |  -0.00450765  |   0.996403    |                |\n| item_obs[user]_271               | -0.0981278   |  21.354     |  -0.00459529  |   0.996334    |                |\n| item_obs[user]_272               | -0.0974374   | nan         | nan           | nan           |                |\n| item_obs[user]_273               | -0.0984176   | nan         | nan           | nan           |                |\n| item_obs[user]_274               | -0.0997197   |  15.2512    |  -0.00653848  |   0.994783    |                |\n| item_obs[user]_275               |  0.09348     |  35.9393    |   0.00260105  |   0.997925    |                |\n| item_obs[user]_276               | -0.0969642   | nan         | nan           | nan           |                |\n| item_obs[user]_277               | -0.100422    |  40.003     |  -0.00251035  |   0.997997    |                |\n| item_obs[user]_278               | -0.0785515   | nan         | nan           | nan           |                |\n| item_obs[user]_279               |  0.0988289   |  22.1784    |   0.0044561   |   0.996445    |                |\n| item_obs[user]_280               | -0.100634    |  30.8907    |  -0.00325774  |   0.997401    |                |\n| item_obs[user]_281               |  0.0922791   |  47.1989    |   0.00195511  |   0.99844     |                |\n| item_obs[user]_282               | -0.0985925   | nan         | nan           | nan           |                |\n| item_obs[user]_283               | -0.0855048   | 109.686     |  -0.00077954  |   0.999378    |                |\n| item_obs[user]_284               | -0.079503    |   7.65406   |  -0.010387    |   0.991712    |                |\n| item_obs[user]_285               | -0.0951967   | nan         | nan           | nan           |                |\n| item_obs[user]_286               |  0.064595    | nan         | nan           | nan           |                |\n| item_obs[user]_287               |  0.0992524   | nan         | nan           | nan           |                |\n| item_obs[user]_288               | -0.0978423   | nan         | nan           | nan           |                |\n| item_obs[user]_289               | -0.0981954   |   8.2897    |  -0.0118455   |   0.990549    |                |\n| item_obs[user]_290               |  0.0993745   | nan         | nan           | nan           |                |\n| item_obs[user]_291               | -0.0979952   | nan         | nan           | nan           |                |\n| item_obs[user]_292               |  0.0974685   |  37.5531    |   0.00259549  |   0.997929    |                |\n| item_obs[user]_293               |  0.0974931   |  20.8291    |   0.00468061  |   0.996265    |                |\n| item_obs[user]_294               | -0.0967849   |  62.2247    |  -0.00155541  |   0.998759    |                |\n| item_obs[user]_295               | -0.0981366   |   8.26488   |  -0.0118739   |   0.990526    |                |\n| item_obs[user]_296               | -0.00655038  |  12.5499    |  -0.000521946 |   0.999584    |                |\n| item_obs[user]_297               | -0.097014    | nan         | nan           | nan           |                |\n| item_obs[user]_298               | -0.0959909   | nan         | nan           | nan           |                |\n| item_obs[user]_299               | -0.0983391   |   3.06209   |  -0.0321151   |   0.97438     |                |\n| item_obs[user]_300               |  0.0789196   | nan         | nan           | nan           |                |\n| item_obs[user]_301               |  0.0970505   | nan         | nan           | nan           |                |\n| item_obs[user]_302               | -0.0930589   | nan         | nan           | nan           |                |\n| item_obs[user]_303               |  0.0994269   |  60.973     |   0.00163067  |   0.998699    |                |\n| item_obs[user]_304               |  0.0624469   |  30.8561    |   0.00202381  |   0.998385    |                |\n| item_obs[user]_305               |  0.0875897   | nan         | nan           | nan           |                |\n| item_obs[user]_306               |  0.0993594   |  15.1301    |   0.00656701  |   0.99476     |                |\n| item_obs[user]_307               |  0.096207    | nan         | nan           | nan           |                |\n| item_obs[user]_308               | -0.0974722   |  14.7545    |  -0.00660625  |   0.994729    |                |\n| item_obs[user]_309               |  0.0987371   | nan         | nan           | nan           |                |\n| item_obs[user]_310               | -0.098197    |  48.1707    |  -0.00203852  |   0.998373    |                |\n| item_obs[user]_311               |  0.100163    | nan         | nan           | nan           |                |\n| item_obs[user]_312               | -0.0765836   |  11.3691    |  -0.00673609  |   0.994625    |                |\n| item_obs[user]_313               | -0.0992518   |  23.0874    |  -0.00429897  |   0.99657     |                |\n| item_obs[user]_314               |  0.081712    |  20.5884    |   0.00396884  |   0.996833    |                |\n| item_obs[user]_315               |  0.0988516   | nan         | nan           | nan           |                |\n| item_obs[user]_316               |  0.0995762   | nan         | nan           | nan           |                |\n| item_obs[user]_317               |  0.0199967   |   5.69144   |   0.00351346  |   0.997197    |                |\n| item_obs[user]_318               | -0.0811777   |  17.7111    |  -0.00458345  |   0.996343    |                |\n| item_obs[user]_319               |  0.0594883   |  36.3683    |   0.00163572  |   0.998695    |                |\n| item_obs[user]_320               | -0.0706452   | 127.149     |  -0.000555611 |   0.999557    |                |\n| item_obs[user]_321               |  0.0932138   | nan         | nan           | nan           |                |\n| item_obs[user]_322               |  0.0957365   | nan         | nan           | nan           |                |\n| item_obs[user]_323               |  0.0888632   |  15.3586    |   0.0057859   |   0.995384    |                |\n| item_obs[user]_324               |  0.0148378   |  34.7338    |   0.000427185 |   0.999659    |                |\n| item_obs[user]_325               | -0.0959005   |   3.56834   |  -0.0268754   |   0.978559    |                |\n| item_obs[user]_326               | -0.0936275   |  31.0117    |  -0.00301911  |   0.997591    |                |\n| item_obs[user]_327               |  0.094207    |  46.1988    |   0.00203917  |   0.998373    |                |\n| item_obs[user]_328               |  0.0904799   | nan         | nan           | nan           |                |\n| item_obs[user]_329               |  0.00110466  |  21.3344    |   5.17785e-05 |   0.999959    |                |\n| item_obs[user]_330               | -0.088346    | nan         | nan           | nan           |                |\n| item_obs[user]_331               | -0.0946461   |  47.9157    |  -0.00197526  |   0.998424    |                |\n| item_obs[user]_332               |  0.0858891   |  31.448     |   0.00273115  |   0.997821    |                |\n| item_obs[user]_333               |  0.0966716   | nan         | nan           | nan           |                |\n| item_obs[user]_334               | -0.0983847   |  30.7631    |  -0.00319814  |   0.997448    |                |\n| item_obs[user]_335               | -0.0862027   | nan         | nan           | nan           |                |\n| item_obs[user]_336               | -0.0956795   | nan         | nan           | nan           |                |\n| item_obs[user]_337               | -0.084618    |  13.3537    |  -0.00633668  |   0.994944    |                |\n| item_obs[user]_338               | -0.0945352   | nan         | nan           | nan           |                |\n| item_obs[user]_339               |  0.0895445   |  24.7765    |   0.00361409  |   0.997116    |                |\n| item_obs[user]_340               | -0.0967957   |  14.8669    |  -0.0065108   |   0.994805    |                |\n| item_obs[user]_341               | -0.075029    |  36.2511    |  -0.0020697   |   0.998349    |                |\n| item_obs[user]_342               | -0.0974254   |  14.4251    |  -0.00675388  |   0.994611    |                |\n| item_obs[user]_343               |  0.0947823   | nan         | nan           | nan           |                |\n| item_obs[user]_344               | -0.091086    | nan         | nan           | nan           |                |\n| item_obs[user]_345               |  0.0975158   | nan         | nan           | nan           |                |\n| item_obs[user]_346               | -0.0944492   |  52.0876    |  -0.00181328  |   0.998553    |                |\n| item_obs[user]_347               | -0.0646427   |  97.4615    |  -0.000663264 |   0.999471    |                |\n| item_obs[user]_348               |  0.0679354   |  29.6162    |   0.00229386  |   0.99817     |                |\n| item_obs[user]_349               | -0.0968292   |  43.9476    |  -0.00220329  |   0.998242    |                |\n| item_obs[user]_350               |  0.0696103   | nan         | nan           | nan           |                |\n| item_obs[user]_351               |  0.0957686   | nan         | nan           | nan           |                |\n| item_obs[user]_352               | -0.0958262   |  40.836     |  -0.00234661  |   0.998128    |                |\n| item_obs[user]_353               | -0.0935495   |  35.8829    |  -0.00260708  |   0.99792     |                |\n| item_obs[user]_354               |  0.091956    | nan         | nan           | nan           |                |\n| item_obs[user]_355               | -0.0956021   |   4.51372   |  -0.0211803   |   0.983102    |                |\n| item_obs[user]_356               |  0.0924389   | nan         | nan           | nan           |                |\n| item_obs[user]_357               |  0.0922818   |  15.5246    |   0.00594424  |   0.995257    |                |\n| item_obs[user]_358               | -0.0894831   | nan         | nan           | nan           |                |\n| item_obs[user]_359               | -0.0946042   | nan         | nan           | nan           |                |\n| item_obs[user]_360               |  0.100855    | nan         | nan           | nan           |                |\n| item_obs[user]_361               | -0.0978983   |   7.57954   |  -0.0129161   |   0.989695    |                |\n| item_obs[user]_362               | -0.098857    |  10.7598    |  -0.0091876   |   0.992669    |                |\n| item_obs[user]_363               | -0.0956965   |  10.4204    |  -0.00918361  |   0.992673    |                |\n| item_obs[user]_364               | -0.0953318   |  31.3876    |  -0.00303724  |   0.997577    |                |\n| item_obs[user]_365               |  0.0936969   | 100.819     |   0.000929354 |   0.999258    |                |\n| item_obs[user]_366               | -0.0978077   | nan         | nan           | nan           |                |\n| item_obs[user]_367               | -0.030269    | nan         | nan           | nan           |                |\n| item_obs[user]_368               | -0.0949272   |  26.6484    |  -0.00356221  |   0.997158    |                |\n| item_obs[user]_369               |  0.0185981   | nan         | nan           | nan           |                |\n| item_obs[user]_370               |  0.0932242   |  20.08      |   0.00464263  |   0.996296    |                |\n| item_obs[user]_371               |  0.0784275   |  26.7891    |   0.00292758  |   0.997664    |                |\n| item_obs[user]_372               | -0.0946762   |  20.9825    |  -0.00451214  |   0.9964      |                |\n| item_obs[user]_373               |  0.0904542   | nan         | nan           | nan           |                |\n| item_obs[user]_374               | -0.0890081   | nan         | nan           | nan           |                |\n| item_obs[user]_375               |  0.0875345   |  37.3756    |   0.00234202  |   0.998131    |                |\n| item_obs[user]_376               |  0.0971968   |   9.30587   |   0.0104447   |   0.991667    |                |\n| item_obs[user]_377               | -0.0939917   | nan         | nan           | nan           |                |\n| item_obs[user]_378               |  0.0703475   |  19.195     |   0.00366488  |   0.997076    |                |\n| item_obs[user]_379               |  0.0950455   |  45.6899    |   0.00208023  |   0.99834     |                |\n| item_obs[user]_380               |  0.096078    | nan         | nan           | nan           |                |\n| item_obs[user]_381               |  0.071838    |   4.46025   |   0.0161063   |   0.98715     |                |\n| item_obs[user]_382               |  0.0437681   | nan         | nan           | nan           |                |\n| item_obs[user]_383               | -0.0996076   | nan         | nan           | nan           |                |\n| item_obs[user]_384               |  0.0925058   |  78.3725    |   0.00118034  |   0.999058    |                |\n| item_obs[user]_385               | -0.0997655   | nan         | nan           | nan           |                |\n| item_obs[user]_386               |  0.0996641   | nan         | nan           | nan           |                |\n| item_obs[user]_387               | -0.0885556   |   6.50647   |  -0.0136104   |   0.989141    |                |\n| item_obs[user]_388               | -0.0394394   |  18.6329    |  -0.00211665  |   0.998311    |                |\n| item_obs[user]_389               | -0.0888595   | nan         | nan           | nan           |                |\n| item_obs[user]_390               |  0.0815189   | nan         | nan           | nan           |                |\n| item_obs[user]_391               | -0.00675963  | nan         | nan           | nan           |                |\n| item_obs[user]_392               | -0.0914254   | nan         | nan           | nan           |                |\n| item_obs[user]_393               | -0.0311297   |  35.5267    |  -0.000876234 |   0.999301    |                |\n| item_obs[user]_394               |  0.0921635   | nan         | nan           | nan           |                |\n| item_obs[user]_395               |  0.0857765   |  18.9483    |   0.00452688  |   0.996388    |                |\n| item_obs[user]_396               | -0.0943922   |  11.9876    |  -0.00787413  |   0.993717    |                |\n| item_obs[user]_397               |  0.0998388   |  29.0521    |   0.00343655  |   0.997258    |                |\n| item_obs[user]_398               | -0.0955568   |  36.7198    |  -0.00260233  |   0.997924    |                |\n| item_obs[user]_399               |  0.0929876   | nan         | nan           | nan           |                |\n| item_obs[user]_400               | -0.0994204   | nan         | nan           | nan           |                |\n| item_obs[user]_401               |  0.0935985   |  22.8397    |   0.00409806  |   0.99673     |                |\n| item_obs[user]_402               |  0.0935873   |  17.6195    |   0.00531157  |   0.995762    |                |\n| item_obs[user]_403               | -0.00388969  |  39.7795    |  -9.77812e-05 |   0.999922    |                |\n| item_obs[user]_404               | -0.0979633   |   8.70624   |  -0.0112521   |   0.991022    |                |\n| item_obs[user]_405               |  0.0957639   |  28.2247    |   0.00339291  |   0.997293    |                |\n| item_obs[user]_406               | -0.0970069   | nan         | nan           | nan           |                |\n| item_obs[user]_407               |  0.100603    |  43.6991    |   0.00230217  |   0.998163    |                |\n| item_obs[user]_408               |  0.0968577   |  29.672     |   0.00326428  |   0.997395    |                |\n| item_obs[user]_409               |  0.0994124   |  44.7094    |   0.00222352  |   0.998226    |                |\n| item_obs[user]_410               |  0.00833281  | nan         | nan           | nan           |                |\n| item_obs[user]_411               | -0.098528    | nan         | nan           | nan           |                |\n| item_obs[user]_412               | -0.0992177   |  21.6188    |  -0.00458941  |   0.996338    |                |\n| item_obs[user]_413               | -0.0990562   |  15.2129    |  -0.00651134  |   0.994805    |                |\n| item_obs[user]_414               | -0.0967073   |  14.7606    |  -0.00655172  |   0.994773    |                |\n| item_obs[user]_415               |  0.0964948   | nan         | nan           | nan           |                |\n| item_obs[user]_416               | -0.09874     | nan         | nan           | nan           |                |\n| item_obs[user]_417               |  0.0809907   |  55.8453    |   0.00145027  |   0.998843    |                |\n| item_obs[user]_418               | -0.0942727   | nan         | nan           | nan           |                |\n| item_obs[user]_419               | -0.0990881   |  36.3365    |  -0.00272696  |   0.997824    |                |\n| item_obs[user]_420               | -0.0832632   |  13.1282    |  -0.00634233  |   0.99494     |                |\n| item_obs[user]_421               | -0.0844952   | nan         | nan           | nan           |                |\n| item_obs[user]_422               |  0.0872249   |  30.0539    |   0.00290228  |   0.997684    |                |\n| item_obs[user]_423               | -0.0682815   |  37.9265    |  -0.00180036  |   0.998564    |                |\n| item_obs[user]_424               |  0.0951386   |  31.3659    |   0.00303318  |   0.99758     |                |\n| item_obs[user]_425               | -0.09675     | nan         | nan           | nan           |                |\n| item_obs[user]_426               | -0.0964502   |  13.2103    |  -0.00730117  |   0.994175    |                |\n| item_obs[user]_427               | -0.0984184   |  13.0909    |  -0.00751807  |   0.994002    |                |\n| item_obs[user]_428               |  0.0355085   | nan         | nan           | nan           |                |\n| item_obs[user]_429               | -0.0303457   |  71.8596    |  -0.000422291 |   0.999663    |                |\n| item_obs[user]_430               | -0.0989598   | nan         | nan           | nan           |                |\n| item_obs[user]_431               | -0.0950014   |  10.5138    |  -0.00903586  |   0.992791    |                |\n| item_obs[user]_432               |  0.059162    |  19.8895    |   0.00297454  |   0.997627    |                |\n| item_obs[user]_433               |  0.099902    |  26.4907    |   0.00377121  |   0.996991    |                |\n| item_obs[user]_434               | -0.0936469   | nan         | nan           | nan           |                |\n| item_obs[user]_435               | -0.0902371   | nan         | nan           | nan           |                |\n| item_obs[user]_436               | -0.0969874   |  19.8906    |  -0.00487605  |   0.996109    |                |\n| item_obs[user]_437               | -0.0932415   | nan         | nan           | nan           |                |\n| item_obs[user]_438               |  0.0925134   | nan         | nan           | nan           |                |\n| item_obs[user]_439               | -0.0956111   | nan         | nan           | nan           |                |\n| item_obs[user]_440               |  0.0624112   | nan         | nan           | nan           |                |\n| item_obs[user]_441               |  0.0909541   |  19.3149    |   0.00470901  |   0.996243    |                |\n| item_obs[user]_442               |  0.0976485   |   5.79844   |   0.0168405   |   0.986564    |                |\n| item_obs[user]_443               |  0.0999445   | nan         | nan           | nan           |                |\n| item_obs[user]_444               |  0.0954517   |  24.2489    |   0.00393633  |   0.996859    |                |\n| item_obs[user]_445               | -0.0971528   | nan         | nan           | nan           |                |\n| item_obs[user]_446               | -0.0990373   |   5.69325   |  -0.0173956   |   0.986121    |                |\n| item_obs[user]_447               | -0.0932183   |  19.3147    |  -0.00482627  |   0.996149    |                |\n| item_obs[user]_448               | -0.0927753   | nan         | nan           | nan           |                |\n| item_obs[user]_449               |  0.0621279   |  30.3748    |   0.00204538  |   0.998368    |                |\n| item_obs[user]_450               | -0.0809301   | nan         | nan           | nan           |                |\n| item_obs[user]_451               |  0.0994425   |  20.6135    |   0.00482413  |   0.996151    |                |\n| item_obs[user]_452               | -0.00339906  |  44.1701    |  -7.69537e-05 |   0.999939    |                |\n| item_obs[user]_453               |  0.088211    |  19.9226    |   0.00442768  |   0.996467    |                |\n| item_obs[user]_454               |  0.0713015   | nan         | nan           | nan           |                |\n| item_obs[user]_455               | -0.0759135   | nan         | nan           | nan           |                |\n| item_obs[user]_456               |  0.0990266   |  27.8852    |   0.00355122  |   0.997167    |                |\n| item_obs[user]_457               | -0.0117378   | nan         | nan           | nan           |                |\n| item_obs[user]_458               | -0.0961448   | nan         | nan           | nan           |                |\n| item_obs[user]_459               |  0.0831571   | nan         | nan           | nan           |                |\n| item_obs[user]_460               |  0.0912292   |  51.1911    |   0.00178213  |   0.998578    |                |\n| item_obs[user]_461               | -0.0786143   | nan         | nan           | nan           |                |\n| item_obs[user]_462               |  0.086667    |  23.4063    |   0.00370272  |   0.997046    |                |\n| item_obs[user]_463               | -0.0937922   | nan         | nan           | nan           |                |\n| item_obs[user]_464               |  0.0816098   | nan         | nan           | nan           |                |\n| item_obs[user]_465               | -0.092377    | nan         | nan           | nan           |                |\n| item_obs[user]_466               |  0.0829483   | nan         | nan           | nan           |                |\n| item_obs[user]_467               | -0.0526504   |  60.7403    |  -0.000866812 |   0.999308    |                |\n| item_obs[user]_468               |  0.0834334   | nan         | nan           | nan           |                |\n| item_obs[user]_469               | -0.0878659   | nan         | nan           | nan           |                |\n| item_obs[user]_470               |  0.085108    |  11.3149    |   0.00752177  |   0.993999    |                |\n| item_obs[user]_471               | -0.0813613   | nan         | nan           | nan           |                |\n| item_obs[user]_472               | -0.0868949   |  19.8092    |  -0.00438659  |   0.9965      |                |\n| item_obs[user]_473               | -0.0799593   |  20.2317    |  -0.00395218  |   0.996847    |                |\n| item_obs[user]_474               |  0.0792365   |  14.9123    |   0.0053135   |   0.99576     |                |\n| item_obs[user]_475               |  0.0819139   |   9.91519   |   0.00826145  |   0.993408    |                |\n| item_obs[user]_476               |  0.079369    | nan         | nan           | nan           |                |\n| item_obs[user]_477               |  0.0814347   | nan         | nan           | nan           |                |\n| item_obs[user]_478               |  0.0859619   |  10.2074    |   0.00842151  |   0.993281    |                |\n| item_obs[user]_479               | -0.0847155   | nan         | nan           | nan           |                |\n| item_obs[user]_480               |  0.0825994   |  96.9404    |   0.000852064 |   0.99932     |                |\n| item_obs[user]_481               |  0.0700469   |  12.9685    |   0.0054013   |   0.99569     |                |\n| item_obs[user]_482               |  0.0979825   | nan         | nan           | nan           |                |\n| item_obs[user]_483               |  0.0822898   |  19.2019    |   0.00428551  |   0.996581    |                |\n| item_obs[user]_484               | -0.0447001   |  34.3641    |  -0.00130078  |   0.998962    |                |\n| item_obs[user]_485               | -0.0377576   |  20.5678    |  -0.00183576  |   0.998535    |                |\n| item_obs[user]_486               | -0.096818    |  42.0827    |  -0.00230066  |   0.998164    |                |\n| item_obs[user]_487               |  0.0789985   | nan         | nan           | nan           |                |\n| item_obs[user]_488               | -0.0877571   |  17.5541    |  -0.00499923  |   0.996011    |                |\n| item_obs[user]_489               |  0.0851457   | nan         | nan           | nan           |                |\n| item_obs[user]_490               |  0.0856856   |   8.13371   |   0.0105346   |   0.991595    |                |\n| item_obs[user]_491               |  0.0830748   | nan         | nan           | nan           |                |\n| item_obs[user]_492               | -0.0120651   |  75.8833    |  -0.000158995 |   0.999873    |                |\n| item_obs[user]_493               | -0.0706219   | nan         | nan           | nan           |                |\n| item_obs[user]_494               |  0.0812898   | nan         | nan           | nan           |                |\n| item_obs[user]_495               |  0.0884591   |  51.1858    |   0.0017282   |   0.998621    |                |\n| item_obs[user]_496               | -0.000764623 |  17.3882    |  -4.39738e-05 |   0.999965    |                |\n| item_obs[user]_497               | -0.0763722   | nan         | nan           | nan           |                |\n| item_obs[user]_498               | -0.0622482   |  57.0773    |  -0.00109059  |   0.99913     |                |\n| item_obs[user]_499               |  0.0959818   | nan         | nan           | nan           |                |\n| item_obs[user]_500               |  0.0780311   |  13.5996    |   0.00573776  |   0.995422    |                |\n| item_obs[user]_501               |  0.0965235   | nan         | nan           | nan           |                |\n| item_obs[user]_502               | -0.0960972   | nan         | nan           | nan           |                |\n| item_obs[user]_503               |  0.0890833   | nan         | nan           | nan           |                |\n| item_obs[user]_504               | -0.0988816   |  25.2671    |  -0.00391345  |   0.996878    |                |\n| item_obs[user]_505               |  0.0779958   |  43.1736    |   0.00180656  |   0.998559    |                |\n| item_obs[user]_506               | -0.0838996   | nan         | nan           | nan           |                |\n| item_obs[user]_507               | -0.0823245   | nan         | nan           | nan           |                |\n| item_obs[user]_508               | -0.0856427   | nan         | nan           | nan           |                |\n| item_obs[user]_509               |  0.0850381   |  10.2073    |   0.00833112  |   0.993353    |                |\n| item_obs[user]_510               |  0.0801802   |  15.8546    |   0.00505723  |   0.995965    |                |\n| item_obs[user]_511               |  0.0894666   | nan         | nan           | nan           |                |\n| item_obs[user]_512               |  0.09686     |  11.4213    |   0.00848068  |   0.993233    |                |\n| item_obs[user]_513               | -0.101392    | 112.293     |  -0.000902922 |   0.99928     |                |\n| item_obs[user]_514               |  0.101387    |  92.6231    |   0.00109462  |   0.999127    |                |\n| item_obs[user]_515               | -0.0944149   |  11.5269    |  -0.00819082  |   0.993465    |                |\n| item_obs[user]_516               | -0.0550151   | nan         | nan           | nan           |                |\n| item_obs[user]_517               | -0.094628    |  24.6297    |  -0.00384203  |   0.996935    |                |\n| item_obs[user]_518               |  0.0907541   |  21.556     |   0.00421017  |   0.996641    |                |\n| item_obs[user]_519               | -0.0512732   | nan         | nan           | nan           |                |\n| item_obs[user]_520               | -0.0961344   |  20.3319    |  -0.00472827  |   0.996227    |                |\n| item_obs[user]_521               | -0.046617    | nan         | nan           | nan           |                |\n| item_obs[user]_522               |  0.096604    | nan         | nan           | nan           |                |\n| item_obs[user]_523               |  0.0946415   | nan         | nan           | nan           |                |\n| item_obs[user]_524               | -0.0979703   | nan         | nan           | nan           |                |\n| item_obs[user]_525               |  0.101373    |  40.7102    |   0.00249012  |   0.998013    |                |\n| item_obs[user]_526               | -0.09886     | nan         | nan           | nan           |                |\n| item_obs[user]_527               |  0.0971182   |  12.7973    |   0.00758894  |   0.993945    |                |\n| item_obs[user]_528               | -0.10131     | nan         | nan           | nan           |                |\n| item_obs[user]_529               |  0.0974968   | nan         | nan           | nan           |                |\n| item_obs[user]_530               |  0.0966032   | nan         | nan           | nan           |                |\n| item_obs[user]_531               | -0.0222043   | nan         | nan           | nan           |                |\n| item_obs[user]_532               | -0.100382    |  28.1908    |  -0.00356081  |   0.997159    |                |\n| item_obs[user]_533               |  0.0988351   |  19.4841    |   0.00507261  |   0.995953    |                |\n| item_obs[user]_534               | -0.0996468   | nan         | nan           | nan           |                |\n| item_obs[user]_535               |  0.0220482   | nan         | nan           | nan           |                |\n| item_obs[user]_536               |  0.0993891   |  27.55      |   0.00360759  |   0.997122    |                |\n| item_obs[user]_537               |  0.101127    | nan         | nan           | nan           |                |\n| item_obs[user]_538               |  0.0659058   | nan         | nan           | nan           |                |\n| item_obs[user]_539               | -0.100557    |  53.8734    |  -0.00186654  |   0.998511    |                |\n| item_obs[user]_540               | -0.100945    |  27.2811    |  -0.00370018  |   0.997048    |                |\n| item_obs[user]_541               | -0.100976    |   6.00068   |  -0.0168274   |   0.986574    |                |\n| item_obs[user]_542               | -0.0994133   |  26.806     |  -0.00370863  |   0.997041    |                |\n| item_obs[user]_543               |  0.0998101   |  47.2879    |   0.00211069  |   0.998316    |                |\n| item_obs[user]_544               | -0.100968    | nan         | nan           | nan           |                |\n| item_obs[user]_545               |  0.0904308   |  26.676     |   0.00338997  |   0.997295    |                |\n| item_obs[user]_546               | -0.0977851   | nan         | nan           | nan           |                |\n| item_obs[user]_547               | -0.101204    | nan         | nan           | nan           |                |\n| item_obs[user]_548               | -0.091354    | nan         | nan           | nan           |                |\n| item_obs[user]_549               | -0.0920535   | nan         | nan           | nan           |                |\n| item_obs[user]_550               |  0.0936059   | nan         | nan           | nan           |                |\n| item_obs[user]_551               | -0.00462256  | nan         | nan           | nan           |                |\n| item_obs[user]_552               |  0.0985385   | nan         | nan           | nan           |                |\n| item_obs[user]_553               | -0.0995969   |  86.588     |  -0.00115024  |   0.999082    |                |\n| item_obs[user]_554               | -0.0993786   | nan         | nan           | nan           |                |\n| item_obs[user]_555               | -0.100863    |  21.6456    |  -0.00465977  |   0.996282    |                |\n| item_obs[user]_556               |  0.0455634   | nan         | nan           | nan           |                |\n| item_obs[user]_557               | -0.0602423   |  90.3012    |  -0.000667126 |   0.999468    |                |\n| item_obs[user]_558               | -0.100872    | nan         | nan           | nan           |                |\n| item_obs[user]_559               | -0.098416    | nan         | nan           | nan           |                |\n| item_obs[user]_560               |  0.0946484   | nan         | nan           | nan           |                |\n| item_obs[user]_561               |  0.101381    | nan         | nan           | nan           |                |\n| item_obs[user]_562               | -0.0972952   |  34.1918    |  -0.00284557  |   0.99773     |                |\n| item_obs[user]_563               | -0.0954583   | nan         | nan           | nan           |                |\n| item_obs[user]_564               | -0.0615614   |  23.7913    |  -0.00258756  |   0.997935    |                |\n| item_obs[user]_565               | -0.0972221   | nan         | nan           | nan           |                |\n| item_obs[user]_566               |  0.0968082   | nan         | nan           | nan           |                |\n| item_obs[user]_567               | -0.0986948   | nan         | nan           | nan           |                |\n| item_obs[user]_568               |  0.0729944   |  16.1102    |   0.00453095  |   0.996385    |                |\n| item_obs[user]_569               |  0.0956325   |  75.9917    |   0.00125846  |   0.998996    |                |\n| item_obs[user]_570               |  0.100028    |  18.0383    |   0.00554533  |   0.995575    |                |\n| item_obs[user]_571               |  0.101555    | nan         | nan           | nan           |                |\n| item_obs[user]_572               |  0.0990644   |  66.289     |   0.00149443  |   0.998808    |                |\n| item_obs[user]_573               | -0.0996809   |  16.4579    |  -0.00605673  |   0.995167    |                |\n| item_obs[user]_574               | -0.100842    | nan         | nan           | nan           |                |\n| item_obs[user]_575               | -0.0973879   |  14.9075    |  -0.00653282  |   0.994788    |                |\n| item_obs[user]_576               | -0.100981    | 207.749     |  -0.000486071 |   0.999612    |                |\n| item_obs[user]_577               |  0.0736564   | nan         | nan           | nan           |                |\n| item_obs[user]_578               |  0.0971491   | nan         | nan           | nan           |                |\n| item_obs[user]_579               |  0.100339    |   9.64719   |   0.0104009   |   0.991701    |                |\n| item_obs[user]_580               | -0.0286677   | nan         | nan           | nan           |                |\n| item_obs[user]_581               | -0.0804997   |  10.7311    |  -0.00750155  |   0.994015    |                |\n| item_obs[user]_582               | -0.0941832   | nan         | nan           | nan           |                |\n| item_obs[user]_583               |  0.0954553   | nan         | nan           | nan           |                |\n| item_obs[user]_584               |  0.0969549   |   7.95363   |   0.01219     |   0.990274    |                |\n| item_obs[user]_585               | -0.0316282   |   6.77455   |  -0.00466867  |   0.996275    |                |\n| item_obs[user]_586               | -0.0978867   | nan         | nan           | nan           |                |\n| item_obs[user]_587               | -0.0880501   | nan         | nan           | nan           |                |\n| item_obs[user]_588               |  0.0954066   |  38.8982    |   0.00245273  |   0.998043    |                |\n| item_obs[user]_589               | -0.0273292   |  18.6961    |  -0.00146176  |   0.998834    |                |\n| item_obs[user]_590               | -0.0663264   |  25.0619    |  -0.0026465   |   0.997888    |                |\n| item_obs[user]_591               | -0.0978446   |  13.0446    |  -0.0075008   |   0.994015    |                |\n| item_obs[user]_592               | -0.094629    | nan         | nan           | nan           |                |\n| item_obs[user]_593               | -0.0973843   | nan         | nan           | nan           |                |\n| item_obs[user]_594               | -0.08807     |  12.54      |  -0.00702312  |   0.994396    |                |\n| item_obs[user]_595               |  0.000126938 | nan         | nan           | nan           |                |\n| item_obs[user]_596               | -0.0937052   |   7.72741   |  -0.0121263   |   0.990325    |                |\n| item_obs[user]_597               | -0.093032    |  52.6126    |  -0.00176825  |   0.998589    |                |\n| item_obs[user]_598               |  0.087314    |   4.23278   |   0.020628    |   0.983542    |                |\n| item_obs[user]_599               |  0.0906963   |  12.3998    |   0.00731431  |   0.994164    |                |\n| item_obs[user]_600               | -0.0879108   | nan         | nan           | nan           |                |\n| item_obs[user]_601               | -0.0428856   | nan         | nan           | nan           |                |\n| item_obs[user]_602               | -0.0921108   | nan         | nan           | nan           |                |\n| item_obs[user]_603               |  0.073359    |  52.1726    |   0.00140608  |   0.998878    |                |\n| item_obs[user]_604               |  0.0728731   |   7.48711   |   0.00973315  |   0.992234    |                |\n| item_obs[user]_605               | -0.0972289   |  25.1176    |  -0.00387094  |   0.996911    |                |\n| item_obs[user]_606               |  0.089037    |  19.2986    |   0.00461366  |   0.996319    |                |\n| item_obs[user]_607               |  0.0868189   |  78.4711    |   0.00110638  |   0.999117    |                |\n| item_obs[user]_608               | -0.0923796   |   5.70752   |  -0.0161856   |   0.987086    |                |\n| item_obs[user]_609               | -0.0946419   | nan         | nan           | nan           |                |\n| item_obs[user]_610               |  0.0921649   |   7.65234   |   0.012044    |   0.990391    |                |\n| item_obs[user]_611               | -0.0922693   | nan         | nan           | nan           |                |\n| item_obs[user]_612               |  0.0979111   | nan         | nan           | nan           |                |\n| item_obs[user]_613               |  0.0979093   | nan         | nan           | nan           |                |\n| item_obs[user]_614               | -0.100505    | nan         | nan           | nan           |                |\n| item_obs[user]_615               | -0.0935866   | nan         | nan           | nan           |                |\n| item_obs[user]_616               | -0.101036    |  11.2001    |  -0.00902098  |   0.992802    |                |\n| item_obs[user]_617               | -0.0834687   |  16.6885    |  -0.00500157  |   0.996009    |                |\n| item_obs[user]_618               | -0.0551828   | nan         | nan           | nan           |                |\n| item_obs[user]_619               | -0.0905586   | nan         | nan           | nan           |                |\n| item_obs[user]_620               | -0.0637796   | nan         | nan           | nan           |                |\n| item_obs[user]_621               |  0.0985965   |  56.6959    |   0.00173904  |   0.998612    |                |\n| item_obs[user]_622               |  0.0359899   | nan         | nan           | nan           |                |\n| item_obs[user]_623               |  0.0991734   | nan         | nan           | nan           |                |\n| item_obs[user]_624               | -0.0697729   |  25.4716    |  -0.00273925  |   0.997814    |                |\n| item_obs[user]_625               | -0.0617766   |  24.8065    |  -0.00249034  |   0.998013    |                |\n| item_obs[user]_626               |  0.0910757   |  27.072     |   0.0033642   |   0.997316    |                |\n| item_obs[user]_627               |  0.101418    |  58.0109    |   0.00174825  |   0.998605    |                |\n| item_obs[user]_628               | -0.0961346   | nan         | nan           | nan           |                |\n| item_obs[user]_629               |  0.094964    |   7.48973   |   0.0126792   |   0.989884    |                |\n| item_obs[user]_630               | -0.0970363   | nan         | nan           | nan           |                |\n| item_obs[user]_631               |  0.0919417   | nan         | nan           | nan           |                |\n| item_obs[user]_632               |  0.0605234   |  18.7139    |   0.00323414  |   0.99742     |                |\n| item_obs[user]_633               | -0.0904889   |  44.1136    |  -0.00205127  |   0.998363    |                |\n| item_obs[user]_634               | -0.0814091   | nan         | nan           | nan           |                |\n| item_obs[user]_635               |  0.0901025   |  40.7631    |   0.00221039  |   0.998236    |                |\n| item_obs[user]_636               |  0.0846401   | nan         | nan           | nan           |                |\n| item_obs[user]_637               |  0.0866962   |  20.7103    |   0.00418613  |   0.99666     |                |\n| item_obs[user]_638               |  0.0730155   | nan         | nan           | nan           |                |\n| item_obs[user]_639               | -0.0374388   |  76.4471    |  -0.000489735 |   0.999609    |                |\n| useritem_obs[constant]_0         | -2.19513     | nan         | nan           | nan           |                |\n| useritem_obs[constant]_1         | -0.0450221   |   5.51083   |  -0.00816975  |   0.993482    |                |\n| useritem_obs[constant]_2         |  0.335328    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_3         |  0.222887    |   8.65038   |   0.0257662   |   0.979444    |                |\n| useritem_obs[constant]_4         |  0.749399    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_5         |  0.113332    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_6         | -0.19438     | nan         | nan           | nan           |                |\n| useritem_obs[constant]_7         | -0.608253    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_8         | -0.938155    |   4.00518   |  -0.234236    |   0.814802    |                |\n| useritem_obs[constant]_9         |  0.442151    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_10        |  0.941669    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_11        |  0.92923     |  12.0349    |   0.0772115   |   0.938455    |                |\n| useritem_obs[constant]_12        | -1.79719     |  14.5266    |  -0.123717    |   0.901539    |                |\n| useritem_obs[constant]_13        | -2.44433     | nan         | nan           | nan           |                |\n| useritem_obs[constant]_14        | -1.80492     |   1.72967   |  -1.04351     |   0.296713    |                |\n| useritem_obs[constant]_15        |  0.294402    |  10.9838    |   0.0268033   |   0.978617    |                |\n| useritem_obs[constant]_16        |  0.325979    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_17        | -0.927703    |  14.3181    |  -0.0647922   |   0.948339    |                |\n| useritem_obs[constant]_18        |  0.644161    |   4.20224   |   0.15329     |   0.87817     |                |\n| useritem_obs[constant]_19        |  0.566723    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_20        | -0.0449253   | nan         | nan           | nan           |                |\n| useritem_obs[constant]_21        |  0.515094    |  15.1087    |   0.0340925   |   0.972803    |                |\n| useritem_obs[constant]_22        | -0.73947     |   7.51922   |  -0.0983439   |   0.921659    |                |\n| useritem_obs[constant]_23        |  1.77285     |   6.26404   |   0.283019    |   0.777162    |                |\n| useritem_obs[constant]_24        |  1.48855     |  13.3312    |   0.111659    |   0.911094    |                |\n| useritem_obs[constant]_25        | -0.589749    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_26        | -0.189601    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_27        | -1.58579     |  22.8557    |  -0.0693824   |   0.944685    |                |\n| useritem_obs[constant]_28        |  1.63166     | nan         | nan           | nan           |                |\n| useritem_obs[constant]_29        |  0.834129    | nan         | nan           | nan           |                |\n| useritem_obs[constant]_30        |  0.245165    |  14.8609    |   0.0164973   |   0.986838    |                |\n| useritem_obs[constant]_31        | -1.21305     |  13.5542    |  -0.0894957   |   0.928688    |                |\n| session_obs[item]_0              |  0.068609    |   0.0575214 |   1.19276     |   0.232965    |                |\n| session_obs[item]_1              |  0.0943611   |   0.0569426 |   1.65713     |   0.0974938   |                |\n| session_obs[item]_2              |  0.10083     |   0.0597371 |   1.68789     |   0.0914315   |                |\n| session_obs[item]_3              | -0.0830204   |   0.0582709 |  -1.42473     |   0.154235    |                |\n| session_obs[item]_4              | -0.0681213   |   0.0617401 |  -1.10336     |   0.269873    |                |\n| session_obs[item]_5              | -0.0285107   |   0.0595629 |  -0.478665    |   0.632177    |                |\n| session_obs[item]_6              | -0.0703343   |   0.0618407 |  -1.13735     |   0.255393    |                |\n| session_obs[item]_7              | -0.091474    |   0.0561316 |  -1.62964     |   0.103178    |                |\n| session_obs[item]_8              | -0.00161236  |   0.0584364 |  -0.0275917   |   0.977988    |                |\n| session_obs[item]_9              | -0.101113    |   0.0557252 |  -1.8145      |   0.0696007   |                |\n| session_obs[item]_10             |  0.0998974   |   0.059756  |   1.67176     |   0.0945726   |                |\n| session_obs[item]_11             |  0.0992974   |   0.0580802 |   1.70966     |   0.0873287   |                |\n| session_obs[item]_12             |  0.0687278   |   0.0615169 |   1.11722     |   0.263901    |                |\n| session_obs[item]_13             |  0.0993844   |   0.0609478 |   1.63065     |   0.102964    |                |\n| session_obs[item]_14             |  0.0950583   |   0.0647747 |   1.46752     |   0.142234    |                |\n| session_obs[item]_15             | -0.0906975   |   0.0621541 |  -1.45924     |   0.1445      |                |\n| session_obs[item]_16             |  0.100648    |   0.0630845 |   1.59544     |   0.110613    |                |\n| session_obs[item]_17             |  0.0970964   |   0.0571602 |   1.69867     |   0.0893813   |                |\n| session_obs[item]_18             | -0.0139554   |   0.0603362 |  -0.231293    |   0.817087    |                |\n| session_obs[item]_19             |  0.099739    |   0.0575703 |   1.73247     |   0.0831891   |                |\n| session_obs[item]_20             | -0.0737882   |   0.0598032 |  -1.23385     |   0.217258    |                |\n| session_obs[item]_21             | -0.0996387   |   0.0595788 |  -1.67238     |   0.0944485   |                |\n| session_obs[item]_22             | -0.0899075   |   0.0595985 |  -1.50855     |   0.131413    |                |\n| session_obs[item]_23             |  0.0723293   |   0.0592888 |   1.21995     |   0.222484    |                |\n| session_obs[item]_24             | -0.0981705   |   0.0633841 |  -1.54882     |   0.121425    |                |\n| session_obs[item]_25             |  0.0222866   |   0.0617213 |   0.361084    |   0.718037    |                |\n| session_obs[item]_26             |  0.0727327   |   0.064012  |   1.13624     |   0.255858    |                |\n| session_obs[item]_27             |  0.0981871   |   0.0596296 |   1.64662     |   0.0996369   |                |\n| session_obs[item]_28             | -0.0962339   |   0.061448  |  -1.5661      |   0.117324    |                |\n| session_obs[item]_29             | -0.0938598   |   0.0576089 |  -1.62926     |   0.103259    |                |\n| price_obs[constant]_0            | -1.3209      |   0.032758  | -40.3231      |   0           | ***            |\n| price_obs[constant]_1            | -0.0466729   |   0.024418  |  -1.91141     |   0.0559515   |                |\n| price_obs[constant]_2            |  1.6281      |   0.0371453 |  43.8305      |   0           | ***            |\n| price_obs[constant]_3            |  0.328977    |   0.0241234 |  13.6373      |   0           | ***            |\n| price_obs[constant]_4            | -0.437104    |   0.0250135 | -17.4747      |   0           | ***            |\n| price_obs[constant]_5            |  0.3052      |   0.0235386 |  12.9659      |   0           | ***            |\n| price_obs[constant]_6            |  1.03763     |   0.0299154 |  34.6855      |   0           | ***            |\n| price_obs[constant]_7            |  1.00561     |   0.0297657 |  33.784       |   0           | ***            |\n| price_obs[constant]_8            | -0.507221    |   0.0256846 | -19.748       |   0           | ***            |\n| price_obs[constant]_9            |  0.215656    |   0.0228565 |   9.43519     |   0           | ***            |\n| price_obs[constant]_10           | -1.17904     |   0.0317257 | -37.1636      |   0           | ***            |\n| price_obs[constant]_11           |  0.871396    |   0.0287952 |  30.2619      |   0           | ***            |\n| usersessionitem_obs[constant]_0  | -0.0973257   |   0.0230575 |  -4.221       |   2.43223e-05 | ***            |\n| usersessionitem_obs[constant]_1  | -1.25702     |   0.0319555 | -39.3367      |   0           | ***            |\n| usersessionitem_obs[constant]_2  |  1.17756     |   0.03094   |  38.0595      |   0           | ***            |\n| usersessionitem_obs[constant]_3  | -0.6465      |   0.0256232 | -25.231       |   0           | ***            |\n| usersessionitem_obs[constant]_4  |  0.203393    |   0.023352  |   8.70989     |   0           | ***            |\n| usersessionitem_obs[constant]_5  | -0.314032    |   0.0235116 | -13.3565      |   0           | ***            |\n| usersessionitem_obs[constant]_6  | -0.427146    |   0.0243446 | -17.5458      |   0           | ***            |\n| usersessionitem_obs[constant]_7  | -1.22026     |   0.0317813 | -38.3956      |   0           | ***            |\n| usersessionitem_obs[constant]_8  | -0.356615    |   0.0239962 | -14.8613      |   0           | ***            |\n| usersessionitem_obs[constant]_9  | -1.5531      |   0.0353216 | -43.9703      |   0           | ***            |\n| usersessionitem_obs[constant]_10 | -0.994266    |   0.0288841 | -34.4226      |   0           | ***            |\n| usersessionitem_obs[constant]_11 |  1.0114      |   0.0291006 |  34.7554      |   0           | ***            |\n| usersessionitem_obs[constant]_12 |  1.65349     |   0.0375474 |  44.0373      |   0           | ***            |\n| usersessionitem_obs[constant]_13 | -0.331221    |   0.0243139 | -13.6227      |   0           | ***            |\n| usersessionitem_obs[constant]_14 |  2.49692     |   0.0490014 |  50.9562      |   0           | ***            |\n| usersessionitem_obs[constant]_15 |  0.121984    |   0.0237004 |   5.14692     |   2.64804e-07 | ***            |\n| intercept[item]_0                |  0.0222749   |   8.51919   |   0.00261468  |   0.997914    |                |\n| intercept[item]_1                | -0.0889155   |  13.2526    |  -0.0067093   |   0.994647    |                |\n| intercept[item]_2                | -0.0932965   | nan         | nan           | nan           |                |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTime taken: 12.200577974319458\n</code></pre> <pre><code>\n</code></pre>"},{"location":"api_torch_choice/","title":"API Reference: Torch Choice","text":"Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>class ChoiceDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 item_index: torch.LongTensor,\n                 num_items: int = None,\n                 num_users: int = None,\n                 num_sessions: int = None,\n                 label: Optional[torch.LongTensor] = None,\n                 user_index: Optional[torch.LongTensor] = None,\n                 session_index: Optional[torch.LongTensor] = None,\n                 item_availability: Optional[torch.BoolTensor] = None,\n                 **kwargs) -&gt; None:\n        \"\"\"\n        Initialization methods for the dataset object, researchers should supply all information about the dataset\n        using this initialization method.\n\n        The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the\n        file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention\n        in machine learning literature.\n        A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`.\n\n        The dataset consists of:\n        (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance.\n        (2) a collection of `observables` associated with item, user, session, etc.\n\n        Args:\n            item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row\n                of the dataset, the relevant item can be:\n                (1) the item bought in this choice instance,\n                (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score.\n                NOTE: The support for second case is under-development, currently, we are only supporting binary label.\n\n            num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`.\n\n            num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`.\n\n            num_sessions (Optional[int]): the number of sessions in the dataset. If `None` is provided (default), the number of sessions will be inferred from the number of unique numbers in `session_index`.\n\n            label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in\n                each choice instance. While you want to predict the item bought, you can leave the `label` argument\n                as `None` in the initialization method, and the model will use `item_index` as the object to be predicted.\n                But if you are, for example, predicting the rating an user gave an item, label must be provided.\n                Defaults to None.\n\n            user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating\n                the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed\n                that the choice instances are from the same user.\n                `user_index` is required if and only if there are multiple users in the dataset, for example:\n                    (1) user-observables is involved in the utility form,\n                    (2) and/or the coefficient is user-specific.\n                This tensor is used to select the corresponding user observables and coefficients assigned to the\n                user (like theta_user) for making prediction for that purchase.\n                Defaults to None.\n\n            session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating\n                the ID of the session when that choice instance occurred. This tensor is used to select the correct\n                session observables or price observables for making prediction for that choice instance. Therefore, if\n                there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset`\n                object will assume each choice instance to be in its own session.\n                Defaults to None.\n\n            item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items)\n                indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite,\n                and hence these unavailable items will be set to 0 while making prediction.\n                We assume all items are available if set to None.\n                Defaults to None.\n\n        Other Kwargs (Observables):\n            One can specify the following types of observables, where * in shape denotes any positive\n                integer. Typically * represents the number of observables.\n            Please refer to the documentation for a detailed guide to use observables.\n            1. user observables must start with 'user_' and have shape (num_users, *)\n            2. item observables must start with 'item_' and have shape (num_items, *)\n            3. session observables must start with 'session_' and have shape (num_sessions, *)\n            4. taste observables (those vary by user and item) must start with `taste_` and have shape\n                (num_users, num_items, *).\n            NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large.\n            5. price observables (those vary by session and item) must start with `price_` and have\n                shape (num_sessions, num_items, *)\n            6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price\n                observable.\n        \"\"\"\n        # ENHANCEMENT(Tianyu): add item_names for summary.\n        super(ChoiceDataset, self).__init__()\n        self.label = label\n        self.item_index = item_index\n        self._num_items = num_items\n        self._num_users = num_users\n        self._num_sessions = num_sessions\n\n        self.user_index = user_index\n        self.session_index = session_index\n\n        if self.session_index is None:\n            # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]):\n            # if any session sensitive observable is provided, but session index is not,\n            # infer each row in the dataset to be a session.\n            # TODO: (design choice) should we assign unique session index to each choice instance or the same session index.\n            print('No `session_index` is provided, assume each choice instance is in its own session.')\n            self.session_index = torch.arange(len(self.item_index)).long()\n\n        self.item_availability = item_availability\n\n        for key, item in kwargs.items():\n            if self._is_attribute(key):\n                # all observable should be float.\n                item = item.float()\n            setattr(self, key, item)\n\n        # TODO: add a validation procedure to check the consistency of the dataset.\n\n    def __getitem__(self, indices: Union[int, torch.LongTensor]) -&gt; \"ChoiceDataset\":\n        \"\"\"Retrieves samples corresponding to the provided index or list of indices.\n\n        Args:\n            indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices.\n\n        Returns:\n            ChoiceDataset: a subset of the dataset.\n        \"\"\"\n        if isinstance(indices, int):\n            # convert single integer index to an array of indices.\n            indices = torch.LongTensor([indices])\n        new_dict = dict()\n        new_dict['item_index'] = self.item_index[indices].clone()\n\n        # copy optional attributes.\n        new_dict['label'] = self.label[indices].clone() if self.label is not None else None\n        new_dict['user_index'] = self.user_index[indices].clone() if self.user_index is not None else None\n        new_dict['session_index'] = self.session_index[indices].clone() if self.session_index is not None else None\n        # item_availability has shape (num_sessions, num_items), no need to re-index it.\n        new_dict['item_availability'] = self.item_availability\n\n        # copy other attributes.\n        for key, val in self.__dict__.items():\n            if key not in new_dict.keys():\n                if torch.is_tensor(val):\n                    new_dict[key] = val.clone()\n                else:\n                    new_dict[key] = copy.deepcopy(val)\n\n        subset = self._from_dict(new_dict)\n        # make sure the new dataset inherits the num_sessions, num_items, and num_users from parent.\n        subset._num_users = self.num_users\n        subset._num_items = self.num_items\n        subset._num_sessions = self.num_sessions\n        return subset\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns number of samples in this dataset.\n\n        Returns:\n            int: length of the dataset.\n        \"\"\"\n        return len(self.item_index)\n\n    def __contains__(self, key: str) -&gt; bool:\n        return key in self.keys\n\n    def __eq__(self, other: \"ChoiceDataset\") -&gt; bool:\n        \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\"\n        if not isinstance(other, ChoiceDataset):\n            raise TypeError('You can only compare with ChoiceDataset objects.')\n        else:\n            flag = True\n            for key, val in self.__dict__.items():\n                if torch.is_tensor(val):\n                    # ignore NaNs while comparing.\n                    if not torch.equal(torch.nan_to_num(val), torch.nan_to_num(other.__dict__[key])):\n                        print('Attribute {} is not equal.'.format(key))\n                        flag = False\n            return flag\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"Returns the device of the dataset.\n\n        Returns:\n            str: the device of the dataset.\n        \"\"\"\n        for attr in self.__dict__.values():\n            if torch.is_tensor(attr):\n                return attr.device\n\n    @property\n    def num_users(self) -&gt; int:\n        \"\"\"Returns number of users involved in this dataset, returns 1 if there is no user identity.\n\n        Returns:\n            int: the number of users involved in this dataset.\n        \"\"\"\n        if self._num_users is not None:\n            return self._num_users\n        elif self.user_index is not None:\n            num_unique = len(torch.unique(self.user_index))\n            expected_num_users = int(self.user_index.max()) + 1\n            if num_unique != expected_num_users:\n                warnings.warn(f\"The number of users is inferred from the number of unique users in the user_index tensor. The user_index tensor in the ChoiceDataset ranges from {int(self.user_index.min())} to {int(self.user_index.max())}. The ChoiceDataset assumes user_index to be 0-indexed and encoded using consecutive integers. There are {expected_num_users} users expected given max(user_index). However, there are {num_unique} unique values in the user_index . This could be caused by missing users in the dataset (i.e., some users are not in user_index at all). If this is not expected, please check the user_index tensor. For a safer behavior, please provide the number of users explicitly by using the num_users keyword while initializing the ChoiceDataset class.\")\n            else:\n                warnings.warn(f\"The number of users is inferred from the number of unique users in the user_index tensor. This might lead to unexpected behaviors if some users never appeared in the user_index tensor. For a safer behavior, please provide the number of users explicitly by using the num_users keyword while initializing the ChoiceDataset class.\")\n\n            # infer from the number of unique users using the user_index.\n            return len(torch.unique(self.user_index))\n        else:\n            return 1\n\n    @property\n    def num_items(self) -&gt; int:\n        \"\"\"Returns the number of items involved in this dataset.\n\n        Returns:\n            int: the number of items involved in this dataset.\n        \"\"\"\n        if self._num_items is not None:\n            # return the _num_items provided in the constructor.\n            return self._num_items\n        else:\n            # infer the number of items from item_index.\n            # the -1 is an optional special symbol for outside option, do not count it towards the number of items.\n            num_unique = len(torch.unique(self.item_index[self.item_index != -1]))\n            expected_num_items = int(self.item_index[self.item_index != -1].max()) + 1\n            if num_unique != expected_num_items:\n                warnings.warn(f\"The number of items is inferred from the number of unique items, excluding -1's denoting outside options, in the item_index tensor. The item_index tensor in the ChoiceDataset ranges from {int(self.item_index[self.item_index != -1].min())} to {int(self.item_index[self.item_index != -1].max())}, excluding -1's. The ChoiceDataset assumes item_index to be 0-indexed and encoded using consecutive integers. There are {expected_num_items} items expected given max(item_index). However, there are {num_unique} unique values in item_index. This could be caused by missing items in the dataset (i.e., some items are not in item_index at all). If this is not expected, please check the item_index tensor. For a safer behavior, please provide the number of items explicitly by using the num_items keyword while initializing the ChoiceDataset class.\")\n            else:\n                warnings.warn(f\"The number of items is inferred from the number of unique items, excluding -1's denoting outside options, in the item_index tensor. This might lead to unexpected behaviors if some items never appeared in the item_index tensor. For a safer behavior, please provide the number of items explicitly by using the num_items keyword while initializing the ChoiceDataset class.\")\n\n            return len(torch.unique(self.item_index[self.item_index != -1]))\n\n    @property\n    def num_sessions(self) -&gt; int:\n        \"\"\"Returns the number of sessions involved in this dataset.\n\n        Returns:\n            int: the number of sessions involved in this dataset.\n        \"\"\"\n        if self._num_sessions is not None:\n            # return the _num_sessions provided in the constructor.\n            return self._num_sessions\n        else:\n            num_unique = len(torch.unique(self.session_index))\n            expected_num_sessions = int(self.session_index.max()) + 1\n            if num_unique != expected_num_sessions:\n                warnings.warn(f\"The number of sessions is inferred from the number of unique sessions in the session_index tensor. The session_index tensor in the ChoiceDataset ranges from {int(self.session_index.min())} to {int(self.session_index.max())}. The ChoiceDataset assumes session_index to be 0-indexed and encoded using consecutive integers. There are {expected_num_sessions} sessions expected given max(session_index). However, there are {num_unique} unique values in the session_index . This could be caused by missing sessions in the dataset (i.e., some sessions are not in session_index at all). If this is not expected, please check the session_index tensor. For a safer behavior, please provide the number of sessions explicitly by using the num_sessions keyword while initializing the ChoiceDataset class.\")\n            else:\n                warnings.warn(f\"The number of sessions is inferred from the number of unique sessions in the session_index tensor. This might lead to unexpected behaviors if some sessions never appeared in the session_index tensor. For a safer behavior, please provide the number of sessions explicitly by using the num_sessions keyword while initializing the ChoiceDataset class.\")\n            # infer the number of sessions from session_index.\n            return len(torch.unique(self.session_index))\n\n    @property\n    def x_dict(self) -&gt; Dict[object, torch.Tensor]:\n        \"\"\"Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format.\n        Models in this package are expecting this dictionary based data format.\n\n        Returns:\n            Dict[object, torch.Tensor]: a dictionary with attribute names in the dataset as keys, and reshaped attribute\n                tensors as values.\n        \"\"\"\n        out = dict()\n        for key, val in self.__dict__.items():\n            if self._is_attribute(key):  # only include attributes.\n                out[key] = self._expand_tensor(key, val)  # reshape to (num_sessions, num_items, num_params).\n        return out\n\n    @classmethod\n    def _from_dict(cls, dictionary: Dict[str, torch.tensor]) -&gt; \"ChoiceDataset\":\n        \"\"\"Creates an instance of ChoiceDataset from a dictionary of arguments.\n\n        Args:\n            dictionary (Dict[str, torch.tensor]): a dictionary with keys as argument names and values as arguments.\n\n        Returns:\n            ChoiceDataset: the created copy of dataset.\n        \"\"\"\n        dataset = cls(**dictionary)\n        for key, item in dictionary.items():\n            setattr(dataset, key, item)\n        return dataset\n\n    def apply_tensor(self, func: callable) -&gt; \"ChoiceDataset\":\n        \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries.\n\n        Args:\n            func (callable): a callable function to be applied on tensors and tensor-values of dictionaries.\n\n        Returns:\n            ChoiceDataset: the modified dataset.\n        \"\"\"\n        for key, item in self.__dict__.items():\n            if torch.is_tensor(item):\n                setattr(self, key, func(item))\n            # boardcast func to dictionary of tensors as well.\n            elif isinstance(getattr(self, key), dict):\n                for obj_key, obj_item in getattr(self, key).items():\n                    if torch.is_tensor(obj_item):\n                        setattr(getattr(self, key), obj_key, func(obj_item))\n        return self\n\n    def to(self, device: Union[str, torch.device]) -&gt; \"ChoiceDataset\":\n        \"\"\"Moves all tensors in this dataset to the specified PyTorch device.\n\n        Args:\n            device (Union[str, torch.device]): the destination device.\n\n        Returns:\n            ChoiceDataset: the modified dataset on the new device.\n        \"\"\"\n        return self.apply_tensor(lambda x: x.to(device))\n\n    def clone(self) -&gt; \"ChoiceDataset\":\n        \"\"\"Creates a copy of self.\n\n        Returns:\n            ChoiceDataset: a copy of self.\n        \"\"\"\n        dictionary = {}\n        for k, v in self.__dict__.items():\n            if torch.is_tensor(v):\n                dictionary[k] = v.clone()\n            else:\n                dictionary[k] = copy.deepcopy(v)\n        new = self.__class__._from_dict(dictionary)\n        new._num_users = self.num_users\n        new._num_items = self.num_items\n        new._num_sessions = self.num_sessions\n        return new\n\n    def _check_device_consistency(self) -&gt; None:\n        \"\"\"Checks if all tensors in this dataset are on the same device.\n\n        Raises:\n            Exception: an exception is raised if not all tensors are on the same device.\n        \"\"\"\n        # assert all tensors are on the same device.\n        devices = list()\n        for val in self.__dict__.values():\n            if torch.is_tensor(val):\n                devices.append(val.device)\n        if len(set(devices)) &gt; 1:\n            raise Exception(f'Found tensors on different devices: {set(devices)}.',\n                            'Use dataset.to() method to align devices.')\n\n    def _size_repr(self, value: object) -&gt; List[int]:\n        \"\"\"A helper method to get the string-representation of object sizes, this is helpful while constructing the\n        string representation of the dataset.\n\n        Args:\n            value (object): an object to examine its size.\n\n        Returns:\n            List[int]: list of integers representing the size of the object, length of the list is equal to dimension of `value`.\n        \"\"\"\n        if torch.is_tensor(value):\n            return list(value.size())\n        elif isinstance(value, int) or isinstance(value, float):\n            return [1]\n        elif isinstance(value, list) or isinstance(value, tuple):\n            return [len(value)]\n        else:\n            return []\n\n    def __repr__(self) -&gt; str:\n        \"\"\"A method to get a string representation of the dataset.\n\n        Returns:\n            str: the string representation of the dataset.\n        \"\"\"\n        # don't print shapes of internal attributes like _num_users and _num_items.\n        info = [f'{key}={self._size_repr(item)}' for key, item in self.__dict__.items() if not key.startswith('_')]\n        return f\"{self.__class__.__name__}(num_items={self.num_items}, num_users={self.num_users}, num_sessions={self.num_sessions}, {', '.join(info)}, device={self.device})\"\n\n    # ==================================================================================================================\n    # methods for checking attribute categories.\n    # ==================================================================================================================\n    @staticmethod\n    def _is_item_attribute(key: str) -&gt; bool:\n        return key.startswith('item_') and (key != 'item_availability') and (key != 'item_index')\n\n    @staticmethod\n    def _is_user_attribute(key: str) -&gt; bool:\n        return key.startswith('user_') and (key != 'user_index')\n\n    @staticmethod\n    def _is_session_attribute(key: str) -&gt; bool:\n        return key.startswith('session_') and (key != 'session_index')\n\n    @staticmethod\n    def _is_useritem_attribute(key: str) -&gt; bool:\n        return key.startswith('useritem_') or key.startswith('itemuser_')\n\n    @staticmethod\n    def _is_price_attribute(key: str) -&gt; bool:\n        return key.startswith('price_') or key.startswith('itemsession_') or key.startswith('sessionitem_')\n\n    @staticmethod\n    def _is_usersession_attribute(key: str) -&gt; bool:\n        return key.startswith('usersession_') or key.startswith('sessionuser_')\n\n    @staticmethod\n    def _is_usersessionitem_attribute(key: str) -&gt; bool:\n        return key.startswith('usersessionitem_') or key.startswith('useritemsession_') \\\n            or key.startswith('itemusersession_') or key.startswith('itemsessionuser_') \\\n            or key.startswith('sessionuseritem_') or key.startswith('sessionitemuser_')\n\n    def _is_attribute(self, key: str) -&gt; bool:\n        return self._is_item_attribute(key) \\\n            or self._is_user_attribute(key) \\\n            or self._is_session_attribute(key) \\\n            or self._is_useritem_attribute(key) \\\n            or self._is_price_attribute(key) \\\n            or self._is_usersession_attribute(key) \\\n            or self._is_usersessionitem_attribute(key)\n\n    def _expand_tensor(self, key: str, val: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Expands attribute tensor to (len(self), num_items, num_params) shape for prediction tasks, this method\n        won't reshape the tensor at all if the `key` (i.e., name of the tensor) suggests its not an attribute of any kind.\n\n        Args:\n            key (str): name of the attribute used to determine the raw shape of the tensor. For example, 'item_obs' means\n                the raw tensor is in shape (num_items, num_params).\n            val (torch.Tensor): the attribute tensor to be reshaped.\n\n        Returns:\n            torch.Tensor: the reshaped tensor with shape (num_sessions, num_items, num_params).\n        \"\"\"\n        if not self._is_attribute(key):\n            # this is a sanity check.\n            raise ValueError(f'Warning: the input key {key} is not an attribute of the dataset, will NOT modify the provided tensor.')\n\n        num_params = val.shape[-1]  # the number of parameters/coefficients/observables.\n\n        # convert attribute tensors to (len(self), num_items, num_params) shape.\n        if self._is_user_attribute(key):\n            # user_attribute (num_users, *)\n            out = val[self.user_index, :].view(\n                len(self), 1, num_params).expand(-1, self.num_items, -1)\n        elif self._is_item_attribute(key):\n            # item_attribute (num_items, *)\n            out = val.view(1, self.num_items, num_params).expand(\n                len(self), -1, -1)\n        elif self._is_useritem_attribute(key):\n            # useritem_attribute (num_users, num_items, *)\n            out = val[self.user_index, :, :]\n        elif self._is_session_attribute(key):\n            # session_attribute (num_sessions, *)\n            out = val[self.session_index, :].view(\n                len(self), 1, num_params).expand(-1, self.num_items, -1)\n        elif self._is_price_attribute(key):\n            # price_attribute (num_sessions, num_items, *)\n            out = val[self.session_index, :, :]\n        elif self._is_usersession_attribute(key):\n            # user-session (num_users, num_sessions, *)\n            out = val[self.user_index, self.session_index, :]  # (len(self), *)\n            out = out.view(len(self), 1, num_params).expand(-1, self.num_items, -1)  # (len(self), num_items, *)\n        elif self._is_usersessionitem_attribute(key):\n            # usersessionitem_attribute has shape (num_users, num_sessions, num_items, *)\n            out = val[self.user_index, self.session_index, :, :]  # (len(self), num_items, *)\n\n        else:\n            raise ValueError(f'Warning: the input key {key} is not an attribute of the dataset, will NOT modify the provided tensor.')\n\n        assert out.shape == (len(self), self.num_items, num_params), f'Error: the output shape {out.shape} is not correct, expected: {(len(self), self.num_items, num_params)}.'\n        return out\n\n    @staticmethod\n    def unique(tensor: torch.Tensor) -&gt; Tuple[np.ndarray]:\n        arr = tensor.cpu().numpy()\n        unique, counts = np.unique(arr, return_counts=True)\n        count_sort_ind = np.argsort(-counts)\n        unique = unique[count_sort_ind]\n        counts = counts[count_sort_ind]\n        return unique, counts\n\n    def summary(self) -&gt; None:\n        \"\"\"A method to summarize the dataset.\n\n        Returns:\n            str: the string representation of the dataset.\n        \"\"\"\n        summary = ['ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .'.format(\n            self.num_sessions, self.num_items, self.num_users if self.user_index is not None else 'single', len(self))]\n\n        # summarize users.\n        if self.user_index is not None:\n            unique, counts = self.unique(self.user_index)\n            summary.append(f\"The most frequent user is {unique[0]} with {counts[0]} observations; the least frequent user is {unique[-1]} with {counts[-1]} observations; on average, there are {counts.astype(float).mean():.2f} observations per user.\")\n\n            N = len(unique)\n            K = min(5, N)\n            string = f'{K} most frequent users are: ' + ', '.join([f'{unique[i]}({counts[i]} times)' for i in range(K)]) + '.'\n            summary.append(string)\n            string = f'{K} least frequent users are: ' + ', '.join([f'{unique[N-i]}({counts[N-i]} times)' for i in range(1, K+1)]) + '.'\n            summary.append(string)\n\n        # summarize items.\n        unique, counts = self.unique(self.item_index)\n        N = len(unique)\n        K = min(5, N)\n        summary.append(f\"The most frequent item is {unique[0]}, it was chosen {counts[0]} times; the least frequent item is {unique[-1]} it was {counts[-1]} times; on average, each item was purchased {counts.astype(float).mean():.2f} times.\")\n\n        string = f'{K} most frequent items are: ' + ', '.join([f'{unique[i]}({counts[i]} times)' for i in range(K)]) + '.'\n        summary.append(string)\n        string = f'{K} least frequent items are: ' + ', '.join([f'{unique[N-i]}({counts[N-i]} times)' for i in range(1, K+1)]) + '.'\n        summary.append(string)\n\n        summary.append('Attribute Summaries:')\n        for key, item in self.__dict__.items():\n            if self._is_attribute(key) and torch.is_tensor(item):\n                summary.append(\"Observable Tensor '{}' with shape {}\".format(key, item.shape))\n                # price attributes are 3-dimensional tensors, ignore  for cleanness here.\n                if (not self._is_price_attribute(key)) and (not self._is_usersessionitem_attribute(key)) and (not self._is_useritem_attribute(key)) and (not self._is_usersession_attribute(key)):\n                    summary.append(str(pd.DataFrame(item.to('cpu').float().numpy()).describe()))\n        print('\\n'.join(summary) + f\"\\ndevice={self.device}\")\n        return None\n</code></pre> <p>A helper class for joining several pytorch datasets, using JointDataset and pytorch data loader allows for sampling the same batch index from several datasets.</p> <p>The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer purchased. You can do this by using the JointDataset class.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>class JointDataset(torch.utils.data.Dataset):\n    \"\"\"A helper class for joining several pytorch datasets, using JointDataset\n    and pytorch data loader allows for sampling the same batch index from several\n    datasets.\n\n    The JointDataset class is a wrapper for the torch.utils.data.ChoiceDataset class, it is particularly useful when we\n    need to make prediction from multiple datasets. For example, you have data on consumer purchase records in a fast food\n    store, and suppose every customer will purchase exactly a single main food and a single drink. In this case, you have\n    two separate datasets: FoodDataset and DrinkDataset. You may want to use PyTorch sampler to sample them in a dependent\n    manner: you want to take the i-th sample from both datasets, so that you know what (food, drink) combo the i-th customer\n    purchased. You can do this by using the JointDataset class.\n    \"\"\"\n    def __init__(self, **datasets) -&gt; None:\n        \"\"\"The initialize methods.\n\n        Args:\n            Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct\n            ```\n            dataset = JointDataset(food=FoodDataset, drink=DrinkDataset)\n            ```\n            All datasets should have the same length.\n\n        \"\"\"\n        super(JointDataset, self).__init__()\n        self.datasets = datasets\n        # check the length of sub-datasets are the same.\n        assert len(set([len(d) for d in self.datasets.values()])) == 1\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of samples in the joint dataset.\n\n        Returns:\n            int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained.\n        \"\"\"\n        for d in self.datasets.values():\n            return len(d)\n\n    def __getitem__(self, indices: Union[int, torch.LongTensor]) -&gt; Dict[str, ChoiceDataset]:\n        \"\"\"Queries samples from the dataset by index.\n\n        Args:\n            indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices.\n\n        Returns:\n            Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset\n                contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets\n                of contained datasets, sliced using the provided indices.\n        \"\"\"\n        return dict((name, d[indices]) for (name, d) in self.datasets.items())\n\n    def __repr__(self) -&gt; str:\n        \"\"\"A method to get a string representation of the dataset.\n\n        Returns:\n            str: the string representation of the dataset.\n        \"\"\"\n        out = [f'JointDataset with {len(self.datasets)} sub-datasets: (']\n        for name, dataset in self.datasets.items():\n            out.append(f'\\t{name}: {str(dataset)}')\n        out.append(')')\n        return '\\n'.join(out)\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"Returns the device of datasets contained in the joint dataset.\n\n        Returns:\n            str: the device of the dataset.\n        \"\"\"\n        for d in self.datasets.values():\n            return d.device\n\n    def to(self, device: Union[str, torch.device]) -&gt; \"JointDataset\":\n        \"\"\"Moves all datasets in this dataset to the specified PyTorch device.\n\n        Args:\n            device (Union[str, torch.device]): the destination device.\n\n        Returns:\n            ChoiceDataset: the modified dataset on the new device.\n        \"\"\"\n        for d in self.datasets.values():\n            d = d.to(device)\n        return self\n\n    def clone(self) -&gt; \"JointDataset\":\n        \"\"\"Returns a copy of the dataset.\n\n        Returns:\n            JointDataset: a copy of the dataset.\n        \"\"\"\n        return JointDataset(**{name: d.clone() for (name, d) in self.datasets.items()})\n\n    @property\n    def item_index(self) -&gt; torch.LongTensor:\n        \"\"\"Returns the current index of each dataset.\n\n        Returns:\n            torch.LongTensor: the indices of items chosen.\n        \"\"\"\n        return self.datasets[\"item\"].item_index\n</code></pre> <p>The more generalized version of conditional logit model, the model allows for research specific variable types(groups) and different levels of variations for coefficient.</p> <p>The model allows for the following levels for variable variations: !!! note \"unless the <code>-full</code> flag is specified (which means we want to explicitly model coefficients\"     for all items), for all variation levels related to item (item specific and user-item specific),     the model force coefficients for the first item to be zero. This design follows standard     econometric practice.</p> <ul> <li> <p>constant: constant over all users and items,</p> </li> <li> <p>user: user-specific parameters but constant across all items,</p> </li> <li> <p>item: item-specific parameters but constant across all users, parameters for the first item are     forced to be zero.</p> </li> <li> <p>item-full: item-specific parameters but constant across all users, explicitly model for all items.</p> </li> <li> <p>user-item: parameters that are specific to both user and item, parameter for the first item     for all users are forced to be zero.</p> </li> <li>user-item-full: parameters that are specific to both user and item, explicitly model for all items.</li> </ul> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>class ConditionalLogitModel(nn.Module):\n    \"\"\"The more generalized version of conditional logit model, the model allows for research specific\n    variable types(groups) and different levels of variations for coefficient.\n\n    The model allows for the following levels for variable variations:\n    NOTE: unless the `-full` flag is specified (which means we want to explicitly model coefficients\n        for all items), for all variation levels related to item (item specific and user-item specific),\n        the model force coefficients for the first item to be zero. This design follows standard\n        econometric practice.\n\n    - constant: constant over all users and items,\n\n    - user: user-specific parameters but constant across all items,\n\n    - item: item-specific parameters but constant across all users, parameters for the first item are\n        forced to be zero.\n    - item-full: item-specific parameters but constant across all users, explicitly model for all items.\n\n    - user-item: parameters that are specific to both user and item, parameter for the first item\n        for all users are forced to be zero.\n    - user-item-full: parameters that are specific to both user and item, explicitly model for all items.\n    \"\"\"\n\n    def __init__(self,\n                 formula: Optional[str]=None,\n                 dataset: Optional[ChoiceDataset]=None,\n                 coef_variation_dict: Optional[Dict[str, str]]=None,\n                 num_param_dict: Optional[Dict[str, int]]=None,\n                 num_items: Optional[int]=None,\n                 num_users: Optional[int]=None,\n                 regularization: Optional[str]=None,\n                 regularization_weight: Optional[float]=None,\n                 weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n                 model_outside_option: Optional[bool]=False\n                 ) -&gt; None:\n        \"\"\"\n        Args:\n            formula (str): a string representing the utility formula.\n                The formula consists of '(variable_name|variation)'s separated by '+', for example:\n                \"(var1|item) + (var2|user) + (var3|constant)\"\n                where the first part of each term is the name of the variable\n                and the second part is the variation of the coefficient.\n                The variation can be one of the following:\n                'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'.\n                All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.\n            data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables\n                and sizes of coefficients from the ChoiceDataset.\n            coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary\n                should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept`\n                if the researcher requires an intercept term.\n                For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should\n                be one of the following values, this value specifies the \"level of variation\" of the coefficient.\n\n                - `constant`: the coefficient constant over all users and items: $X \\beta$.\n\n                - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$.\n\n                - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$.\n                    Note that the coefficients for the first item are forced to be zero following the standard practice\n                    in econometrics.\n\n                - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to\n                    be zeros.\n\n                The following configurations are supported by the package, but we don't recommend using them due to the\n                    large number of parameters.\n                - `user-item`: parameters that are specific to both user and item, parameter for the first item\n                    for all users are forced to be zero.\n\n                - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items.\n            num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same\n                as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable.\n                If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary\n                and values of all ones. Default to be None.\n            num_items (int): number of items in the dataset.\n            num_users (int): number of users in the dataset.\n            regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of\n                regularization added to the log-likelihood.\n                - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood.\n                - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood.\n                - None does not modify the log-likelihood.\n                Defaults to None.\n            regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood.\n                This term controls the strength of regularization. This argument is required if and only if regularization\n                is not None.\n                Defaults to None.\n            weight_initialization (Optional[Union[str, Dict[str, str]]]): controls for how coefficients are initialized;\n                users can pass a string from {'normal', 'uniform', 'zero'} to initialize all coefficients in the same way.\n                Alternatively, users can pass a dictionary with keys exactly the same as the `coef_variation_dict` dictionary,\n                and values from {'normal', 'uniform', 'zero'} to initialize coefficients of different types of variables differently.\n                By default, all coefficients are initialized following a standard normal distribution.\n            model_outside_option (Optional[bool]): whether to explicitly model the outside option (i.e., the consumer did not buy anything).\n                To enable modeling outside option, the outside option is indicated by `item_index[n] == -1` in the item-index-tensor.\n                In this case, the item-index-tensor can contain values in `{-1, 0, 1, ..., num_items-1}`.\n                Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in `{0, 1, ..., num_items-1}`.\n                The utility of the outside option is always set to 0 while computing the probability.\n                By default, model_outside_option is set to False and the model does not model the outside option.\n        \"\"\"\n        # ==============================================================================================================\n        # Check that the model received a valid combination of inputs so that it can be initialized.\n        # ==============================================================================================================\n        if coef_variation_dict is None and formula is None:\n            raise ValueError(\"Either coef_variation_dict or formula should be provided to specify the model.\")\n\n        if (coef_variation_dict is not None) and (formula is not None):\n            raise ValueError(\"Only one of coef_variation_dict or formula should be provided to specify the model.\")\n\n        if (formula is not None) and (dataset is None):\n            raise ValueError(\"If formula is provided, data should be provided to specify the model.\")\n\n\n        # ==============================================================================================================\n        # Build necessary dictionaries for model initialization.\n        # ==============================================================================================================\n        if formula is None:\n            # Use dictionaries to initialize the model.\n            if num_param_dict is None:\n                warnings.warn(\"`num_param_dict` is not provided, all variables will be treated as having one parameter.\")\n                num_param_dict = {key:1 for key in coef_variation_dict.keys()}\n\n            assert coef_variation_dict.keys() == num_param_dict.keys()\n\n            # variable `var` with variation `spec` to variable `var[spec]`.\n            rename = dict()  # old variable name --&gt; new variable name.\n            for variable, specificity in coef_variation_dict.items():\n                rename[variable] = f\"{variable}[{specificity}]\"\n\n            for old_name, new_name in rename.items():\n                coef_variation_dict[new_name] = coef_variation_dict.pop(old_name)\n                num_param_dict[new_name] = num_param_dict.pop(old_name)\n        else:\n            # Use the formula to infer model.\n            coef_variation_dict, num_param_dict = parse_formula(formula, dataset)\n\n        # ==============================================================================================================\n        # Model Initialization.\n        # ==============================================================================================================\n        super(ConditionalLogitModel, self).__init__()\n\n        self.coef_variation_dict = deepcopy(coef_variation_dict)\n        self.num_param_dict = deepcopy(num_param_dict)\n\n        self.num_items = num_items\n        self.num_users = num_users\n\n        self.regularization = deepcopy(regularization)\n        assert self.regularization in ['L1', 'L2', None], f\"Provided regularization={self.regularization} is not allowed, allowed values are ['L1', 'L2', None].\"\n        self.regularization_weight = regularization_weight\n        if (self.regularization is not None) and (self.regularization_weight is None):\n            raise ValueError(f'You specified regularization type {self.regularization} without providing regularization_weight.')\n        if (self.regularization is None) and (self.regularization_weight is not None):\n            raise ValueError(f'You specified no regularization but you provide regularization_weight={self.regularization_weight}, you should leave regularization_weight as None if you do not want to regularize the model.')\n\n        # check number of parameters specified are all positive.\n        for var_type, num_params in self.num_param_dict.items():\n            assert num_params &gt; 0, f'num_params needs to be positive, got: {num_params}.'\n\n        # infer the number of parameters for intercept if the researcher forgets.\n        for variable in self.coef_variation_dict.keys():\n            if self.is_intercept_term(variable) and variable not in self.num_param_dict.keys():\n                warnings.warn(f\"`{variable}` key found in coef_variation_dict but not in num_param_dict, num_param_dict['{variable}'] has been set to 1.\")\n                self.num_param_dict[variable] = 1\n\n        # inform coefficients their ways of being initialized.\n        self.weight_initialization = deepcopy(weight_initialization)\n\n        # construct trainable parameters.\n        coef_dict = dict()\n        for var_type, variation in self.coef_variation_dict.items():\n            if isinstance(self.weight_initialization, dict):\n                if var_type.split('[')[0] in self.weight_initialization.keys():\n                    # use the variable-specific initialization if provided.\n                    init = self.weight_initialization[var_type.split('[')[0]]\n                else:\n                    # use default initialization.\n                    init = None\n            else:\n                # initialize all coefficients in the same way.\n                init = self.weight_initialization\n\n            coef_dict[var_type] = Coefficient(variation=variation,\n                                              num_items=self.num_items,\n                                              num_users=self.num_users,\n                                              num_params=self.num_param_dict[var_type],\n                                              init=init)\n        # A ModuleDict is required to properly register all trainable parameters.\n        # self.parameter() will fail if a python dictionary is used instead.\n        self.coef_dict = nn.ModuleDict(coef_dict)\n        self.model_outside_option = model_outside_option\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the model.\n\n        Returns:\n            str: the string representation of the model.\n        \"\"\"\n        out_str_lst = ['Conditional logistic discrete choice model, expects input features:\\n']\n        for var_type, num_params in self.num_param_dict.items():\n            out_str_lst.append(f'X[{var_type}] with {num_params} parameters, with {self.coef_variation_dict[var_type]} level variation.')\n        return super().__repr__() + '\\n' + '\\n'.join(out_str_lst) + '\\n' + f'device={self.device}'\n\n    @property\n    def num_params(self) -&gt; int:\n        \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied\n        with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no\n        intercept is involved.\n\n        Returns:\n            int: the total number of learnable parameters.\n        \"\"\"\n        return sum(w.numel() for w in self.parameters())\n\n    def summary(self):\n        \"\"\"Print out the current model parameter.\"\"\"\n        for var_type, coefficient in self.coef_dict.items():\n            if coefficient is not None:\n                print('Variable Type: ', var_type)\n                print(coefficient.coef)\n\n    def forward(self,\n                batch: ChoiceDataset,\n                manual_coef_value_dict: Optional[Dict[str, torch.Tensor]] = None\n                ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            batch: a `ChoiceDataset` object.\n\n            manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with\n                keys in {'u', 'i'} etc and tensors as values. If provided, the model will force\n                coefficient to be the provided values and compute utility conditioned on the provided\n                coefficient values. This feature is useful when the research wishes to plug in particular\n                values of coefficients and examine the utility values. If not provided, the model will\n                use the learned coefficient values in self.coef_dict.\n                Defaults to None.\n\n        Returns:\n            torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents\n                the utility from item i in trip t for the user involved in that trip.\n        \"\"\"\n        x_dict = batch.x_dict\n\n        for variable in self.coef_variation_dict.keys():\n            if self.is_intercept_term(variable):\n                # intercept term has no input tensor from the ChoiceDataset data structure.\n                # the tensor for intercept has only 1 feature, every entry is 1.\n                x_dict['intercept'] = torch.ones((len(batch), self.num_items, 1), device=batch.device)\n                break\n\n        # compute the utility from each item in each choice session.\n        total_utility = torch.zeros((len(batch), self.num_items), device=batch.device)\n        # for each type of variables, apply the corresponding coefficient to input x.\n\n        for var_type, coef in self.coef_dict.items():\n            # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name.\n            corresponding_observable = var_type.split(\"[\")[0]\n            total_utility += coef(\n                x_dict[corresponding_observable],\n                batch.user_index,\n                manual_coef_value=None if manual_coef_value_dict is None else manual_coef_value_dict[var_type])\n\n        assert total_utility.shape == (len(batch), self.num_items)\n\n        if batch.item_availability is not None:\n            # mask out unavailable items.\n            total_utility[~batch.item_availability[batch.session_index, :]] = torch.finfo(total_utility.dtype).min / 2\n\n        # accommodate the outside option.\n        if self.model_outside_option:\n            # the outside option has zero utility.\n            util_zero = torch.zeros(total_utility.size(0), 1, device=batch.device)  # (len(batch), 1)  zero tensor.\n            # outside option is indicated by item_index == -1, we put it at the end.\n            total_utility = torch.cat((total_utility, util_zero), dim=1)  # (len(batch), num_items+1)\n        return total_utility\n\n\n    def negative_log_likelihood(self, batch: ChoiceDataset, y: torch.Tensor, is_train: bool=True) -&gt; torch.Tensor:\n        \"\"\"Computes the log-likelihood for the batch and label.\n        TODO: consider remove y, change to label.\n        TODO: consider move this method outside the model, the role of the model is to compute the utility.\n\n        Args:\n            batch (ChoiceDataset): a ChoiceDataset object containing the data.\n            y (torch.Tensor): the label.\n            is_train (bool, optional): whether to trace the gradient. Defaults to True.\n\n        Returns:\n            torch.Tensor: the negative log-likelihood.\n        \"\"\"\n        if is_train:\n            self.train()\n        else:\n            self.eval()\n        # (num_trips, num_items)\n        total_utility = self.forward(batch)\n        # check shapes.\n        if self.model_outside_option:\n            assert total_utility.shape == (len(batch), self.num_items+1)\n            assert torch.all(total_utility[:, -1] == 0), \"The last column of total_utility should be all zeros, which corresponds to the outside option.\"\n        else:\n            assert total_utility.shape == (len(batch), self.num_items)\n        logP = torch.log_softmax(total_utility, dim=1)\n        # since y == -1 indicates the outside option and the last column of total_utility is the outside option, the following\n        # indexing should correctly retrieve the log-likelihood even for outside options.\n        nll = - logP[torch.arange(len(y)), y].sum()\n        return nll\n\n    def loss(self, *args, **kwargs):\n        \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\"\n        nll = self.negative_log_likelihood(*args, **kwargs)\n        if self.regularization is not None:\n            L = {'L1': 1, 'L2': 2}[self.regularization]\n            for param in self.parameters():\n                nll += self.regularization_weight * torch.norm(param, p=L)\n        return nll\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Returns the device of the coefficient.\n\n        Returns:\n            torch.device: the device of the model.\n        \"\"\"\n        return next(iter(self.coef_dict.values())).device\n\n    @staticmethod\n    def is_intercept_term(variable: str):\n        # check if the given variable is an intercept (fixed effect) term.\n        # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc.\n        return (variable.startswith('intercept[') and variable.endswith(']'))\n\n    def get_coefficient(self, variable: str) -&gt; torch.Tensor:\n        \"\"\"Retrieve the coefficient tensor for the given variable.\n\n        Args:\n            variable (str): the variable name.\n\n        Returns:\n            torch.Tensor: the corresponding coefficient tensor of the requested variable.\n        \"\"\"\n        return self.state_dict()[f\"coef_dict.{variable}.coef\"].detach().clone()\n</code></pre> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>class NestedLogitModel(nn.Module):\n    def __init__(self,\n                 nest_to_item: Dict[object, List[int]],\n                 # method 1: specify variation and num param. dictionary.\n                 nest_coef_variation_dict: Optional[Dict[str, str]]=None,\n                 nest_num_param_dict: Optional[Dict[str, int]]=None,\n                 item_coef_variation_dict: Optional[Dict[str, str]]=None,\n                 item_num_param_dict: Optional[Dict[str, int]]=None,\n                 # method 2: specify formula and dataset.\n                 item_formula: Optional[str]=None,\n                 nest_formula: Optional[str]=None,\n                 dataset: Optional[JointDataset]=None,\n                 num_users: Optional[int]=None,\n                 shared_lambda: bool=False,\n                 regularization: Optional[str]=None,\n                 regularization_weight: Optional[float]=None,\n                 nest_weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n                 item_weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n                 model_outside_option: Optional[bool]=False\n                 ) -&gt; None:\n        \"\"\"Initialization method of the nested logit model.\n\n        Args:\n            nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list\n                of items IDs of the queried nest.\n\n            nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type\n                (i.e., variable group) to the level of variation for the coefficient of this type\n                of variables.\n            nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to\n                the number of parameters in this variable group.\n\n            item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but\n                for item features.\n            item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item\n                features.\n\n            {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model.\n                The formula consists of '(variable_name|variation)'s separated by '+', for example:\n                \"(var1|item) + (var2|user) + (var3|constant)\"\n                where the first part of each term is the name of the variable\n                and the second part is the variation of the coefficient.\n                The variation can be one of the following:\n                'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'.\n                All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.\n            dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables\n                and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item'].\n\n            num_users (Optional[int], optional): number of users to be modelled, this is only\n                required if any of variable type requires user-specific variations.\n                Defaults to None.\n\n            shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which\n                is the coefficient for inclusive values, to be constant for all nests.\n                The lambda enters the nest-level selection as the following\n                Utility of choosing nest k = lambda * inclusive value of nest k\n                                               + linear combination of some other nest level features\n                If set to True, a single lambda will be learned for all nests, otherwise, the\n                model learns an individual lambda for each nest.\n                Defaults to False.\n\n            regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of\n                regularization added to the log-likelihood.\n                - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood.\n                - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood.\n                - None does not modify the log-likelihood.\n                Defaults to None.\n\n            regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood.\n                This term controls the strength of regularization. This argument is required if and only if regularization\n                is not None.\n                Defaults to None.\n\n            {nest, item}_weight_initialization (Optional[Union[str, Dict[str, str]]]): methods to initialize the weights of\n                coefficients for {nest, item} level model. Please refer to the `weight_initialization` keyword in ConditionalLogitModel's documentation for more details.\n\n            model_outside_option (Optional[bool]): whether to explicitly model the outside option (i.e., the consumer did not buy anything).\n                To enable modeling outside option, the outside option is indicated by `item_index[n] == -1` in the item-index-tensor.\n                In this case, the item-index-tensor can contain values in `{-1, 0, 1, ..., num_items-1}`.\n                Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in `{0, 1, ..., num_items-1}`.\n                The utility of the outside option is always set to 0 while computing the probability.\n                By default, model_outside_option is set to False and the model does not model the outside option.\n        \"\"\"\n        # handle nest level model.\n        using_formula_to_initiate = (item_formula is not None) and (nest_formula is not None)\n        if using_formula_to_initiate:\n            # make sure that the research does not specify duplicated information, which might cause conflict.\n            if (nest_coef_variation_dict is not None) or (item_coef_variation_dict is not None):\n                raise ValueError('You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.')\n            if (nest_num_param_dict is not None) or (item_num_param_dict is not None):\n                raise ValueError('You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.')\n            if dataset is None:\n                raise ValueError('Dataset is required if {item, nest}_formula is specified to initiate the model.')\n\n            nest_coef_variation_dict, nest_num_param_dict = parse_formula(nest_formula, dataset.datasets['nest'])\n            item_coef_variation_dict, item_num_param_dict = parse_formula(item_formula, dataset.datasets['item'])\n\n        else:\n            # check for conflicting information.\n            if (nest_formula is not None) or (item_formula is not None):\n                raise ValueError('You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.')\n            # make sure that the research specifies all the required information.\n            if (nest_coef_variation_dict is None) or (item_coef_variation_dict is None):\n                raise ValueError('You should specify the {item, nest}_coef_variation_dict to initiate the model.')\n            if (nest_num_param_dict is None) or (item_num_param_dict is None):\n                raise ValueError('You should specify the {item, nest}_num_param_dict to initiate the model.')\n\n        super(NestedLogitModel, self).__init__()\n        self.nest_to_item = nest_to_item\n        self.nest_coef_variation_dict = nest_coef_variation_dict\n        self.nest_num_param_dict = nest_num_param_dict\n        self.item_coef_variation_dict = item_coef_variation_dict\n        self.item_num_param_dict = item_num_param_dict\n        self.num_users = num_users\n\n        self.nests = list(nest_to_item.keys())\n        self.num_nests = len(self.nests)\n        self.num_items = sum(len(items) for items in nest_to_item.values())\n\n        # nest coefficients.\n        self.nest_coef_dict = self._build_coef_dict(self.nest_coef_variation_dict,\n                                                    self.nest_num_param_dict,\n                                                    self.num_nests,\n                                                    weight_initialization=deepcopy(nest_weight_initialization))\n\n        # item coefficients.\n        self.item_coef_dict = self._build_coef_dict(self.item_coef_variation_dict,\n                                                    self.item_num_param_dict,\n                                                    self.num_items,\n                                                    weight_initialization=deepcopy(item_weight_initialization))\n\n        self.shared_lambda = shared_lambda\n        if self.shared_lambda:\n            self.lambda_weight = nn.Parameter(torch.ones(1), requires_grad=True)\n        else:\n            self.lambda_weight = nn.Parameter(torch.ones(self.num_nests) / 2, requires_grad=True)\n        # breakpoint()\n        # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True)\n        # used to warn users if forgot to call clamp.\n        self._clamp_called_flag = True\n\n        self.regularization = regularization\n        assert self.regularization in ['L1', 'L2', None], f\"Provided regularization={self.regularization} is not allowed, allowed values are ['L1', 'L2', None].\"\n        self.regularization_weight = regularization_weight\n        if (self.regularization is not None) and (self.regularization_weight is None):\n            raise ValueError(f'You specified regularization type {self.regularization} without providing regularization_weight.')\n        if (self.regularization is None) and (self.regularization_weight is not None):\n            raise ValueError(f'You specified no regularization but you provide regularization_weight={self.regularization_weight}, you should leave regularization_weight as None if you do not want to regularize the model.')\n\n        self.model_outside_option = model_outside_option\n\n    @property\n    def num_params(self) -&gt; int:\n        \"\"\"Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied\n        with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no\n        intercept is involved.\n\n        Returns:\n            int: the total number of learnable parameters.\n        \"\"\"\n        return sum(w.numel() for w in self.parameters())\n\n    def _build_coef_dict(self,\n                         coef_variation_dict: Dict[str, str],\n                         num_param_dict: Dict[str, int],\n                         num_items: int,\n                         weight_initialization: Optional[Union[str, Dict[str, str]]]=None\n                         ) -&gt; nn.ModuleDict:\n        \"\"\"Builds a coefficient dictionary containing all trainable components of the model, mapping coefficient names\n            to the corresponding Coefficient Module.\n            num_items could be the actual number of items or the number of nests depends on the use case.\n            NOTE: torch-choice users don't directly interact with this method.\n\n        Args:\n            coef_variation_dict (Dict[str, str]): a dictionary mapping coefficient names (e.g., theta_user) to the level\n                of variation (e.g., 'user').\n            num_param_dict (Dict[str, int]): a dictionary mapping coefficient names to the number of parameters in this\n                coefficient. Be aware that, for example, if there is one K-dimensional coefficient for every user, then\n                the `num_param` should be K instead of K x number of users.\n            num_items (int): the total number of items in the prediction problem. `num_items` should be the number of nests if _build_coef_dict() is used for nest-level prediction.\n\n        Returns:\n            nn.ModuleDict: a PyTorch ModuleDict object mapping from coefficient names to training Coefficient.\n        \"\"\"\n        coef_dict = dict()\n        for var_type, variation in coef_variation_dict.items():\n            num_params = num_param_dict[var_type]\n\n            if isinstance(weight_initialization, dict):\n                if var_type.split('[')[0] in weight_initialization.keys():\n                    # use the variable-specific initialization if provided.\n                    init = weight_initialization[var_type.split('[')[0]]\n                else:\n                    # use default initialization.\n                    init = None\n            else:\n                # initialize all coefficients in the same way.\n                init = weight_initialization\n\n            coef_dict[var_type] = Coefficient(variation=variation,\n                                              num_items=num_items,\n                                              num_users=self.num_users,\n                                              num_params=num_params,\n                                              init=init)\n        return nn.ModuleDict(coef_dict)\n\n\n    def forward(self, batch: ChoiceDataset) -&gt; torch.Tensor:\n        \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the\n            predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide\n            this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument.\n            For more details about the forward passing, please refer to the _forward() method.\n\n        # TODO: the ConditionalLogitModel returns predicted utility, the NestedLogitModel behaves the same?\n\n        Args:\n            batch (ChoiceDataset): a ChoiceDataset object containing the data batch.\n\n        Returns:\n            torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability\n            of choosing item i in trip t.\n        \"\"\"\n        return self._forward(batch['nest'].x_dict,\n                             batch['item'].x_dict,\n                             batch['item'].user_index,\n                             batch['item'].item_availability)\n\n    def _forward(self,\n                 nest_x_dict: Dict[str, torch.Tensor],\n                 item_x_dict: Dict[str, torch.Tensor],\n                 user_index: Optional[torch.LongTensor] = None,\n                 item_availability: Optional[torch.BoolTensor] = None\n                 ) -&gt; torch.Tensor:\n        \"\"\"\"Computes log P[t, i] = the log probability for the user involved in trip t to choose item i.\n        Let n denote the ID of the user involved in trip t, then P[t, i] = P_{ni} on page 86 of the\n        book \"discrete choice methods with simulation\" by Train.\n\n        The `_forward` method is an internal API, users should refer to the `forward` method.\n\n        Args:\n            nest_x_dict (torch.Tensor): a dictionary mapping from nest-level feature names to the corresponding feature tensor.\n\n            item_x_dict (torch.Tensor): a dictionary mapping from item-level feature names to the corresponding feature tensor.\n\n                More details on the shape of the tensors can be found in the docstring of the `x_dict` method of `ChoiceDataset`.\n\n            user_index (torch.LongTensor): a tensor of shape (num_trips,) indicating which user is\n                making decision in each trip. Setting user_index = None assumes the same user is\n                making decisions in all trips.\n            item_availability (torch.BoolTensor): a boolean tensor with shape (num_trips, num_items)\n                indicating the aviliability of items in each trip. If item_availability[t, i] = False,\n                the utility of choosing item i in trip t, V[t, i], will be set to -inf.\n                Given the decomposition V[t, i] = W[t, k(i)] + Y[t, i] + eps, V[t, i] is set to -inf\n                by setting Y[t, i] = -inf for unavilable items.\n\n        Returns:\n            torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability\n            of choosing item i in trip t.\n        \"\"\"\n        if self.shared_lambda:\n            self.lambdas = self.lambda_weight.expand(self.num_nests)\n        else:\n            self.lambdas = self.lambda_weight\n\n        # if not self._clamp_called_flag:\n        #     warnings.warn('Did you forget to call clamp_lambdas() after optimizer.step()?')\n\n        # The overall utility of item can be decomposed into V[item] = W[nest] + Y[item] + eps.\n        T = list(item_x_dict.values())[0].shape[0]\n        device = list(item_x_dict.values())[0].device\n        # compute nest-specific utility with shape (T, num_nests).\n        W = torch.zeros(T, self.num_nests).to(device)\n\n        for variable in self.nest_coef_variation_dict.keys():\n            if self.is_intercept_term(variable):\n                nest_x_dict['intercept'] = torch.ones((T, self.num_nests, 1)).to(device)\n                break\n\n        for variable in self.item_coef_variation_dict.keys():\n            if self.is_intercept_term(variable):\n                item_x_dict['intercept'] = torch.ones((T, self.num_items, 1)).to(device)\n                break\n\n        for var_type, coef in self.nest_coef_dict.items():\n            corresponding_observable = var_type.split(\"[\")[0]\n            W += coef(nest_x_dict[corresponding_observable], user_index)\n\n        # compute item-specific utility (T, num_items).\n        Y = torch.zeros(T, self.num_items).to(device)\n        for var_type, coef in self.item_coef_dict.items():\n            corresponding_observable = var_type.split(\"[\")[0]\n            Y += coef(item_x_dict[corresponding_observable], user_index)\n\n        if item_availability is not None:\n            Y[~item_availability] = torch.finfo(Y.dtype).min / 2\n\n        # =============================================================================\n        # compute the inclusive value of each nest.\n        inclusive_value = dict()\n        for k, Bk in self.nest_to_item.items():\n            # for nest k, divide the Y of all items in Bk by lambda_k.\n            Y[:, Bk] /= self.lambdas[k]\n            # compute inclusive value for nest k.\n            # mask out unavilable items.\n            inclusive_value[k] = torch.logsumexp(Y[:, Bk], dim=1, keepdim=False)  # (T,)\n        # boardcast inclusive value from (T, num_nests) to (T, num_items).\n        # for trip t, I[t, i] is the inclusive value of the nest item i belongs to.\n        I = torch.zeros(T, self.num_items).to(device)\n        for k, Bk in self.nest_to_item.items():\n            I[:, Bk] = inclusive_value[k].view(-1, 1)  # (T, |Bk|)\n\n        # logP_item[t, i] = log P(ni|Bk), where Bk is the nest item i is in, n is the user in trip t.\n        logP_item = Y - I  # (T, num_items)\n\n        if self.model_outside_option:\n            # if the model explicitly models the outside option, we need to add a column of zeros to logP_item.\n            # log P(ni|Bk) = 0 for the outside option since Y = 0 and the outside option has its own nest.\n            logP_item = torch.cat((logP_item, torch.zeros(T, 1).to(device)), dim=1)\n            assert logP_item.shape == (T, self.num_items+1)\n            assert torch.all(logP_item[:, -1] == 0)\n\n        # =============================================================================\n        # logP_nest[t, i] = log P(Bk), for item i in trip t, the probability of choosing the nest/bucket\n        # item i belongs to. logP_nest has shape (T, num_items)\n        # logit[t, i] = W[n, k] + lambda[k] I[n, k], where n is the user involved in trip t, k is\n        # the nest item i belongs to.\n        logit = torch.zeros(T, self.num_items).to(device)\n        for k, Bk in self.nest_to_item.items():\n            logit[:, Bk] = (W[:, k] + self.lambdas[k] * inclusive_value[k]).view(-1, 1)  # (T, |Bk|)\n        # only count each nest once in the logsumexp within the nest level model.\n        cols = [x[0] for x in self.nest_to_item.values()]\n        if self.model_outside_option:\n            # the last column corresponds to the outside option, which has W+lambda*I = 0 since W = I = Y = 0 for the outside option.\n            logit = torch.cat((logit, torch.zeros(T, 1).to(device)), dim=1)\n            assert logit.shape == (T, self.num_items+1)\n            # we have already added W+lambda*I for each \"actual\" nest, now we add the \"fake\" nest for the outside option.\n            cols.append(-1)\n        logP_nest = logit - torch.logsumexp(logit[:, cols], dim=1, keepdim=True)\n\n        # =============================================================================\n        # compute the joint log P_{ni} as in the textbook.\n        logP = logP_item + logP_nest\n        self._clamp_called_flag = False\n        return logP\n\n    def log_likelihood(self, *args):\n        \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method.\n\n        Returns:\n            _type_: the log likelihood of the model.\n        \"\"\"\n        return - self.negative_log_likelihood(*args)\n\n    def negative_log_likelihood(self,\n                                batch: ChoiceDataset,\n                                y: torch.LongTensor,\n                                is_train: bool=True) -&gt; torch.scalar_tensor:\n        \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples\n            in batch instead of the average.\n\n        Args:\n            batch (ChoiceDataset): the ChoiceDataset object containing the data.\n            y (torch.LongTensor): the label.\n            is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian\n                of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric,\n                then `is_train` can be set to False for better performance.\n                Defaults to True.\n\n        Returns:\n            torch.scalar_tensor: the negative log likelihood of the model.\n        \"\"\"\n        # compute the negative log-likelihood loss directly.\n        if is_train:\n            self.train()\n        else:\n            self.eval()\n        # (num_trips, num_items)\n        logP = self.forward(batch)\n        # check shapes\n        if self.model_outside_option:\n            assert logP.shape == (len(batch['item']), self.num_items+1)\n        else:\n            assert logP.shape == (len(batch['item']), self.num_items)\n        # since y == -1 indicates the outside option and the last column of total_utility is the outside option, the following\n        # indexing should correctly retrieve the log-likelihood even for outside options.\n        nll = - logP[torch.arange(len(y)), y].sum()\n        return nll\n\n    def loss(self, *args, **kwargs):\n        \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\"\n        nll = self.negative_log_likelihood(*args, **kwargs)\n        if self.regularization is not None:\n            L = {'L1': 1, 'L2': 2}[self.regularization]\n            for name, param in self.named_parameters():\n                if name == 'lambda_weight':\n                    # we don't regularize the lambda term, we only regularize coefficients.\n                    continue\n                nll += self.regularization_weight * torch.norm(param, p=L)\n        return nll\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Returns the device of the coefficient.\n\n        Returns:\n            torch.device: the device of the model.\n        \"\"\"\n        return next(iter(self.item_coef_dict.values())).device\n\n    @staticmethod\n    def is_intercept_term(variable: str):\n        # check if the given variable is an intercept (fixed effect) term.\n        # intercept (fixed effect) terms are defined as 'intercept[*]' and looks like 'intercept[user]', 'intercept[item]', etc.\n        return (variable.startswith('intercept[') and variable.endswith(']'))\n\n    def get_coefficient(self, variable: str, level: Optional[str] = None) -&gt; torch.Tensor:\n        \"\"\"Retrieve the coefficient tensor for the given variable.\n\n        Args:\n            variable (str): the variable name.\n            level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`.\n\n        Returns:\n            torch.Tensor: the corresponding coefficient tensor of the requested variable.\n        \"\"\"\n        if variable == 'lambda':\n            return self.lambda_weight.detach().clone()\n\n        if level not in ['item', 'nest']:\n            raise ValueError(f\"Level should be either 'item' or 'nest', got {level}.\")\n\n        return self.state_dict()[f'{level}_coef_dict.{variable}.coef'].detach().clone()\n\n    # def clamp_lambdas(self):\n    #     \"\"\"\n    #     Restrict values of lambdas to 0 &lt; lambda &lt;= 1 to guarantee the utility maximization property\n    #     of the model.\n    #     This method should be called everytime after optimizer.step().\n    #     We add a self_clamp_called_flag to remind researchers if this method is not called.\n    #     \"\"\"\n    #     for k in range(len(self.lambdas)):\n    #         self.lambdas[k] = torch.clamp(self.lambdas[k], 1e-5, 1)\n    #     self._clam_called_flag = True\n\n    # @staticmethod\n    # def add_constant(x: torch.Tensor, where: str='prepend') -&gt; torch.Tensor:\n    #     \"\"\"A helper function used to add constant to feature tensor,\n    #     x has shape (batch_size, num_classes, num_parameters),\n    #     returns a tensor of shape (*, num_parameters+1).\n    #     \"\"\"\n    #     batch_size, num_classes, num_parameters = x.shape\n    #     ones = torch.ones((batch_size, num_classes, 1))\n    #     if where == 'prepend':\n    #         new = torch.cat((ones, x), dim=-1)\n    #     elif where == 'append':\n    #         new = torch.cat((x, ones), dim=-1)\n    #     else:\n    #         raise Exception\n    #     return new\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.device","title":"<code>device: str</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the device of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>the device of the dataset.</p>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_items","title":"<code>num_items: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the number of items involved in this dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of items involved in this dataset.</p>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_sessions","title":"<code>num_sessions: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the number of sessions involved in this dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of sessions involved in this dataset.</p>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.num_users","title":"<code>num_users: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns number of users involved in this dataset, returns 1 if there is no user identity.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of users involved in this dataset.</p>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.x_dict","title":"<code>x_dict: Dict[object, torch.Tensor]</code>  <code>property</code> <code>readonly</code>","text":"<p>Formats attributes of in this dataset into shape (num_sessions, num_items, num_params) and returns in a dictionary format. Models in this package are expecting this dictionary based data format.</p> <p>Returns:</p> Type Description <code>Dict[object, torch.Tensor]</code> <p>a dictionary with attribute names in the dataset as keys, and reshaped attribute     tensors as values.</p>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__eq__","title":"<code>__eq__(self, other)</code>  <code>special</code>","text":"<p>Returns whether all tensor attributes of both ChoiceDatasets are equal.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def __eq__(self, other: \"ChoiceDataset\") -&gt; bool:\n    \"\"\"Returns whether all tensor attributes of both ChoiceDatasets are equal.\"\"\"\n    if not isinstance(other, ChoiceDataset):\n        raise TypeError('You can only compare with ChoiceDataset objects.')\n    else:\n        flag = True\n        for key, val in self.__dict__.items():\n            if torch.is_tensor(val):\n                # ignore NaNs while comparing.\n                if not torch.equal(torch.nan_to_num(val), torch.nan_to_num(other.__dict__[key])):\n                    print('Attribute {} is not equal.'.format(key))\n                    flag = False\n        return flag\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__getitem__","title":"<code>__getitem__(self, indices)</code>  <code>special</code>","text":"<p>Retrieves samples corresponding to the provided index or list of indices.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Union[int, torch.LongTensor]</code> <p>a single integer index or a tensor of indices.</p> required <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>a subset of the dataset.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def __getitem__(self, indices: Union[int, torch.LongTensor]) -&gt; \"ChoiceDataset\":\n    \"\"\"Retrieves samples corresponding to the provided index or list of indices.\n\n    Args:\n        indices (Union[int, torch.LongTensor]): a single integer index or a tensor of indices.\n\n    Returns:\n        ChoiceDataset: a subset of the dataset.\n    \"\"\"\n    if isinstance(indices, int):\n        # convert single integer index to an array of indices.\n        indices = torch.LongTensor([indices])\n    new_dict = dict()\n    new_dict['item_index'] = self.item_index[indices].clone()\n\n    # copy optional attributes.\n    new_dict['label'] = self.label[indices].clone() if self.label is not None else None\n    new_dict['user_index'] = self.user_index[indices].clone() if self.user_index is not None else None\n    new_dict['session_index'] = self.session_index[indices].clone() if self.session_index is not None else None\n    # item_availability has shape (num_sessions, num_items), no need to re-index it.\n    new_dict['item_availability'] = self.item_availability\n\n    # copy other attributes.\n    for key, val in self.__dict__.items():\n        if key not in new_dict.keys():\n            if torch.is_tensor(val):\n                new_dict[key] = val.clone()\n            else:\n                new_dict[key] = copy.deepcopy(val)\n\n    subset = self._from_dict(new_dict)\n    # make sure the new dataset inherits the num_sessions, num_items, and num_users from parent.\n    subset._num_users = self.num_users\n    subset._num_items = self.num_items\n    subset._num_sessions = self.num_sessions\n    return subset\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__init__","title":"<code>__init__(self, item_index, num_items=None, num_users=None, num_sessions=None, label=None, user_index=None, session_index=None, item_availability=None, **kwargs)</code>  <code>special</code>","text":"<p>Initialization methods for the dataset object, researchers should supply all information about the dataset using this initialization method.</p> <p>The number of choice instances are called <code>batch_size</code> in the documentation. The <code>batch_size</code> corresponds to the file length in wide-format dataset, and often denoted using <code>N</code>. We call it <code>batch_size</code> to follow the convention in machine learning literature. A <code>choice instance</code> is a row of the dataset, so there are <code>batch_size</code> choice instances in each <code>ChoiceDataset</code>.</p> <p>The dataset consists of: (1) a collection of <code>batch_size</code> tuples (item_id, user_id, session_id, label), where each tuple is a choice instance. (2) a collection of <code>observables</code> associated with item, user, session, etc.</p> <p>Parameters:</p> Name Type Description Default <code>item_index</code> <code>torch.LongTensor</code> <p>a tensor of shape (batch_size) indicating the relevant item in each row of the dataset, the relevant item can be: (1) the item bought in this choice instance, (2) or the item reviewed by the user. In the later case, we need the <code>label</code> tensor to specify the rating score. NOTE: The support for second case is under-development, currently, we are only supporting binary label.</p> required <code>num_items</code> <code>Optional[int]</code> <p>the number of items in the dataset. If <code>None</code> is provided (default), the number of items will be inferred from the number of unique numbers in <code>item_index</code>.</p> <code>None</code> <code>num_users</code> <code>Optional[int]</code> <p>the number of users in the dataset. If <code>None</code> is provided (default), the number of users will be inferred from the number of unique numbers in <code>user_index</code>.</p> <code>None</code> <code>num_sessions</code> <code>Optional[int]</code> <p>the number of sessions in the dataset. If <code>None</code> is provided (default), the number of sessions will be inferred from the number of unique numbers in <code>session_index</code>.</p> <code>None</code> <code>label</code> <code>Optional[torch.LongTensor]</code> <p>a tensor of shape (batch_size) indicating the label for prediction in each choice instance. While you want to predict the item bought, you can leave the <code>label</code> argument as <code>None</code> in the initialization method, and the model will use <code>item_index</code> as the object to be predicted. But if you are, for example, predicting the rating an user gave an item, label must be provided. Defaults to None.</p> <code>None</code> <code>user_index</code> <code>Optional[torch.LongTensor]</code> <p>a tensor of shape num_purchases (batch_size) indicating the ID of the user who was involved in each choice instance. If <code>None</code> user index is provided, it's assumed that the choice instances are from the same user. <code>user_index</code> is required if and only if there are multiple users in the dataset, for example:     (1) user-observables is involved in the utility form,     (2) and/or the coefficient is user-specific. This tensor is used to select the corresponding user observables and coefficients assigned to the user (like theta_user) for making prediction for that purchase. Defaults to None.</p> <code>None</code> <code>session_index</code> <code>Optional[torch.LongTensor]</code> <p>a tensor of shape num_purchases (batch_size) indicating the ID of the session when that choice instance occurred. This tensor is used to select the correct session observables or price observables for making prediction for that choice instance. Therefore, if there is no session/price observables, you can leave this argument as <code>None</code>. In this case, the <code>ChoiceDataset</code> object will assume each choice instance to be in its own session. Defaults to None.</p> <code>None</code> <code>item_availability</code> <code>Optional[torch.BoolTensor]</code> <p>A boolean tensor of shape (num_sessions, num_items) indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite, and hence these unavailable items will be set to 0 while making prediction. We assume all items are available if set to None. Defaults to None.</p> <code>None</code> <p>Other Kwargs (Observables):     One can specify the following types of observables, where * in shape denotes any positive         integer. Typically * represents the number of observables.     Please refer to the documentation for a detailed guide to use observables.     1. user observables must start with 'user_' and have shape (num_users, )     2. item observables must start with 'item_' and have shape (num_items, )     3. session observables must start with 'session_' and have shape (num_sessions, )     4. taste observables (those vary by user and item) must start with <code>taste_</code> and have shape         (num_users, num_items, ).     NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large.     5. price observables (those vary by session and item) must start with <code>price_</code> and have         shape (num_sessions, num_items, *)     6. itemsession observables starting with <code>itemsession_</code>, this is a more intuitive alias to the price         observable.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def __init__(self,\n             item_index: torch.LongTensor,\n             num_items: int = None,\n             num_users: int = None,\n             num_sessions: int = None,\n             label: Optional[torch.LongTensor] = None,\n             user_index: Optional[torch.LongTensor] = None,\n             session_index: Optional[torch.LongTensor] = None,\n             item_availability: Optional[torch.BoolTensor] = None,\n             **kwargs) -&gt; None:\n    \"\"\"\n    Initialization methods for the dataset object, researchers should supply all information about the dataset\n    using this initialization method.\n\n    The number of choice instances are called `batch_size` in the documentation. The `batch_size` corresponds to the\n    file length in wide-format dataset, and often denoted using `N`. We call it `batch_size` to follow the convention\n    in machine learning literature.\n    A `choice instance` is a row of the dataset, so there are `batch_size` choice instances in each `ChoiceDataset`.\n\n    The dataset consists of:\n    (1) a collection of `batch_size` tuples (item_id, user_id, session_id, label), where each tuple is a choice instance.\n    (2) a collection of `observables` associated with item, user, session, etc.\n\n    Args:\n        item_index (torch.LongTensor): a tensor of shape (batch_size) indicating the relevant item in each row\n            of the dataset, the relevant item can be:\n            (1) the item bought in this choice instance,\n            (2) or the item reviewed by the user. In the later case, we need the `label` tensor to specify the rating score.\n            NOTE: The support for second case is under-development, currently, we are only supporting binary label.\n\n        num_items (Optional[int]): the number of items in the dataset. If `None` is provided (default), the number of items will be inferred from the number of unique numbers in `item_index`.\n\n        num_users (Optional[int]): the number of users in the dataset. If `None` is provided (default), the number of users will be inferred from the number of unique numbers in `user_index`.\n\n        num_sessions (Optional[int]): the number of sessions in the dataset. If `None` is provided (default), the number of sessions will be inferred from the number of unique numbers in `session_index`.\n\n        label (Optional[torch.LongTensor], optional): a tensor of shape (batch_size) indicating the label for prediction in\n            each choice instance. While you want to predict the item bought, you can leave the `label` argument\n            as `None` in the initialization method, and the model will use `item_index` as the object to be predicted.\n            But if you are, for example, predicting the rating an user gave an item, label must be provided.\n            Defaults to None.\n\n        user_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating\n            the ID of the user who was involved in each choice instance. If `None` user index is provided, it's assumed\n            that the choice instances are from the same user.\n            `user_index` is required if and only if there are multiple users in the dataset, for example:\n                (1) user-observables is involved in the utility form,\n                (2) and/or the coefficient is user-specific.\n            This tensor is used to select the corresponding user observables and coefficients assigned to the\n            user (like theta_user) for making prediction for that purchase.\n            Defaults to None.\n\n        session_index (Optional[torch.LongTensor], optional): a tensor of shape num_purchases (batch_size) indicating\n            the ID of the session when that choice instance occurred. This tensor is used to select the correct\n            session observables or price observables for making prediction for that choice instance. Therefore, if\n            there is no session/price observables, you can leave this argument as `None`. In this case, the `ChoiceDataset`\n            object will assume each choice instance to be in its own session.\n            Defaults to None.\n\n        item_availability (Optional[torch.BoolTensor], optional): A boolean tensor of shape (num_sessions, num_items)\n            indicating the availability of each item in each session. Utilities of unavailable items would be set to -infinite,\n            and hence these unavailable items will be set to 0 while making prediction.\n            We assume all items are available if set to None.\n            Defaults to None.\n\n    Other Kwargs (Observables):\n        One can specify the following types of observables, where * in shape denotes any positive\n            integer. Typically * represents the number of observables.\n        Please refer to the documentation for a detailed guide to use observables.\n        1. user observables must start with 'user_' and have shape (num_users, *)\n        2. item observables must start with 'item_' and have shape (num_items, *)\n        3. session observables must start with 'session_' and have shape (num_sessions, *)\n        4. taste observables (those vary by user and item) must start with `taste_` and have shape\n            (num_users, num_items, *).\n        NOTE: we don't recommend using taste observables, because num_users * num_items is potentially large.\n        5. price observables (those vary by session and item) must start with `price_` and have\n            shape (num_sessions, num_items, *)\n        6. itemsession observables starting with `itemsession_`, this is a more intuitive alias to the price\n            observable.\n    \"\"\"\n    # ENHANCEMENT(Tianyu): add item_names for summary.\n    super(ChoiceDataset, self).__init__()\n    self.label = label\n    self.item_index = item_index\n    self._num_items = num_items\n    self._num_users = num_users\n    self._num_sessions = num_sessions\n\n    self.user_index = user_index\n    self.session_index = session_index\n\n    if self.session_index is None:\n        # if any([x.startswith('session_') or x.startswith('price_') for x in kwargs.keys()]):\n        # if any session sensitive observable is provided, but session index is not,\n        # infer each row in the dataset to be a session.\n        # TODO: (design choice) should we assign unique session index to each choice instance or the same session index.\n        print('No `session_index` is provided, assume each choice instance is in its own session.')\n        self.session_index = torch.arange(len(self.item_index)).long()\n\n    self.item_availability = item_availability\n\n    for key, item in kwargs.items():\n        if self._is_attribute(key):\n            # all observable should be float.\n            item = item.float()\n        setattr(self, key, item)\n\n    # TODO: add a validation procedure to check the consistency of the dataset.\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Returns number of samples in this dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>length of the dataset.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns number of samples in this dataset.\n\n    Returns:\n        int: length of the dataset.\n    \"\"\"\n    return len(self.item_index)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.__repr__","title":"<code>__repr__(self)</code>  <code>special</code>","text":"<p>A method to get a string representation of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the dataset.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"A method to get a string representation of the dataset.\n\n    Returns:\n        str: the string representation of the dataset.\n    \"\"\"\n    # don't print shapes of internal attributes like _num_users and _num_items.\n    info = [f'{key}={self._size_repr(item)}' for key, item in self.__dict__.items() if not key.startswith('_')]\n    return f\"{self.__class__.__name__}(num_items={self.num_items}, num_users={self.num_users}, num_sessions={self.num_sessions}, {', '.join(info)}, device={self.device})\"\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.apply_tensor","title":"<code>apply_tensor(self, func)</code>","text":"<p>This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>a callable function to be applied on tensors and tensor-values of dictionaries.</p> required <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>the modified dataset.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def apply_tensor(self, func: callable) -&gt; \"ChoiceDataset\":\n    \"\"\"This s a helper method to apply the provided function to all tensors and tensor values of all dictionaries.\n\n    Args:\n        func (callable): a callable function to be applied on tensors and tensor-values of dictionaries.\n\n    Returns:\n        ChoiceDataset: the modified dataset.\n    \"\"\"\n    for key, item in self.__dict__.items():\n        if torch.is_tensor(item):\n            setattr(self, key, func(item))\n        # boardcast func to dictionary of tensors as well.\n        elif isinstance(getattr(self, key), dict):\n            for obj_key, obj_item in getattr(self, key).items():\n                if torch.is_tensor(obj_item):\n                    setattr(getattr(self, key), obj_key, func(obj_item))\n    return self\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.clone","title":"<code>clone(self)</code>","text":"<p>Creates a copy of self.</p> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>a copy of self.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def clone(self) -&gt; \"ChoiceDataset\":\n    \"\"\"Creates a copy of self.\n\n    Returns:\n        ChoiceDataset: a copy of self.\n    \"\"\"\n    dictionary = {}\n    for k, v in self.__dict__.items():\n        if torch.is_tensor(v):\n            dictionary[k] = v.clone()\n        else:\n            dictionary[k] = copy.deepcopy(v)\n    new = self.__class__._from_dict(dictionary)\n    new._num_users = self.num_users\n    new._num_items = self.num_items\n    new._num_sessions = self.num_sessions\n    return new\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.summary","title":"<code>summary(self)</code>","text":"<p>A method to summarize the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the dataset.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"A method to summarize the dataset.\n\n    Returns:\n        str: the string representation of the dataset.\n    \"\"\"\n    summary = ['ChoiceDataset with {} sessions, {} items, {} users, {} purchase records (observations) .'.format(\n        self.num_sessions, self.num_items, self.num_users if self.user_index is not None else 'single', len(self))]\n\n    # summarize users.\n    if self.user_index is not None:\n        unique, counts = self.unique(self.user_index)\n        summary.append(f\"The most frequent user is {unique[0]} with {counts[0]} observations; the least frequent user is {unique[-1]} with {counts[-1]} observations; on average, there are {counts.astype(float).mean():.2f} observations per user.\")\n\n        N = len(unique)\n        K = min(5, N)\n        string = f'{K} most frequent users are: ' + ', '.join([f'{unique[i]}({counts[i]} times)' for i in range(K)]) + '.'\n        summary.append(string)\n        string = f'{K} least frequent users are: ' + ', '.join([f'{unique[N-i]}({counts[N-i]} times)' for i in range(1, K+1)]) + '.'\n        summary.append(string)\n\n    # summarize items.\n    unique, counts = self.unique(self.item_index)\n    N = len(unique)\n    K = min(5, N)\n    summary.append(f\"The most frequent item is {unique[0]}, it was chosen {counts[0]} times; the least frequent item is {unique[-1]} it was {counts[-1]} times; on average, each item was purchased {counts.astype(float).mean():.2f} times.\")\n\n    string = f'{K} most frequent items are: ' + ', '.join([f'{unique[i]}({counts[i]} times)' for i in range(K)]) + '.'\n    summary.append(string)\n    string = f'{K} least frequent items are: ' + ', '.join([f'{unique[N-i]}({counts[N-i]} times)' for i in range(1, K+1)]) + '.'\n    summary.append(string)\n\n    summary.append('Attribute Summaries:')\n    for key, item in self.__dict__.items():\n        if self._is_attribute(key) and torch.is_tensor(item):\n            summary.append(\"Observable Tensor '{}' with shape {}\".format(key, item.shape))\n            # price attributes are 3-dimensional tensors, ignore  for cleanness here.\n            if (not self._is_price_attribute(key)) and (not self._is_usersessionitem_attribute(key)) and (not self._is_useritem_attribute(key)) and (not self._is_usersession_attribute(key)):\n                summary.append(str(pd.DataFrame(item.to('cpu').float().numpy()).describe()))\n    print('\\n'.join(summary) + f\"\\ndevice={self.device}\")\n    return None\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.choice_dataset.ChoiceDataset.to","title":"<code>to(self, device)</code>","text":"<p>Moves all tensors in this dataset to the specified PyTorch device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, torch.device]</code> <p>the destination device.</p> required <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>the modified dataset on the new device.</p> Source code in <code>torch_choice/data/choice_dataset.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; \"ChoiceDataset\":\n    \"\"\"Moves all tensors in this dataset to the specified PyTorch device.\n\n    Args:\n        device (Union[str, torch.device]): the destination device.\n\n    Returns:\n        ChoiceDataset: the modified dataset on the new device.\n    \"\"\"\n    return self.apply_tensor(lambda x: x.to(device))\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.device","title":"<code>device: str</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the device of datasets contained in the joint dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>the device of the dataset.</p>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.item_index","title":"<code>item_index: LongTensor</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the current index of each dataset.</p> <p>Returns:</p> Type Description <code>torch.LongTensor</code> <p>the indices of items chosen.</p>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__getitem__","title":"<code>__getitem__(self, indices)</code>  <code>special</code>","text":"<p>Queries samples from the dataset by index.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Union[int, torch.LongTensor]</code> <p>an integer or a 1D tensor of multiple indices.</p> required <p>Returns:</p> Type Description <code>Dict[str, ChoiceDataset]</code> <p>the subset of the dataset. Keys of the dictionary will be names of each dataset     contained (the same as the keys of the <code>datasets</code> argument in the constructor). Values will be subsets     of contained datasets, sliced using the provided indices.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def __getitem__(self, indices: Union[int, torch.LongTensor]) -&gt; Dict[str, ChoiceDataset]:\n    \"\"\"Queries samples from the dataset by index.\n\n    Args:\n        indices (Union[int, torch.LongTensor]): an integer or a 1D tensor of multiple indices.\n\n    Returns:\n        Dict[str, ChoiceDataset]: the subset of the dataset. Keys of the dictionary will be names of each dataset\n            contained (the same as the keys of the ``datasets`` argument in the constructor). Values will be subsets\n            of contained datasets, sliced using the provided indices.\n    \"\"\"\n    return dict((name, d[indices]) for (name, d) in self.datasets.items())\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__init__","title":"<code>__init__(self, **datasets)</code>  <code>special</code>","text":"<p>The initialize methods.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def __init__(self, **datasets) -&gt; None:\n    \"\"\"The initialize methods.\n\n    Args:\n        Arbitrarily many datasets with arbitrary names as keys. In the example above, you can construct\n        ```\n        dataset = JointDataset(food=FoodDataset, drink=DrinkDataset)\n        ```\n        All datasets should have the same length.\n\n    \"\"\"\n    super(JointDataset, self).__init__()\n    self.datasets = datasets\n    # check the length of sub-datasets are the same.\n    assert len(set([len(d) for d in self.datasets.values()])) == 1\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__len__","title":"<code>__len__(self)</code>  <code>special</code>","text":"<p>Get the number of samples in the joint dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of samples in the joint dataset.\n\n    Returns:\n        int: the number of samples in the joint dataset, which is the same as the number of samples in each dataset contained.\n    \"\"\"\n    for d in self.datasets.values():\n        return len(d)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.__repr__","title":"<code>__repr__(self)</code>  <code>special</code>","text":"<p>A method to get a string representation of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the dataset.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"A method to get a string representation of the dataset.\n\n    Returns:\n        str: the string representation of the dataset.\n    \"\"\"\n    out = [f'JointDataset with {len(self.datasets)} sub-datasets: (']\n    for name, dataset in self.datasets.items():\n        out.append(f'\\t{name}: {str(dataset)}')\n    out.append(')')\n    return '\\n'.join(out)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.clone","title":"<code>clone(self)</code>","text":"<p>Returns a copy of the dataset.</p> <p>Returns:</p> Type Description <code>JointDataset</code> <p>a copy of the dataset.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def clone(self) -&gt; \"JointDataset\":\n    \"\"\"Returns a copy of the dataset.\n\n    Returns:\n        JointDataset: a copy of the dataset.\n    \"\"\"\n    return JointDataset(**{name: d.clone() for (name, d) in self.datasets.items()})\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.data.joint_dataset.JointDataset.to","title":"<code>to(self, device)</code>","text":"<p>Moves all datasets in this dataset to the specified PyTorch device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, torch.device]</code> <p>the destination device.</p> required <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>the modified dataset on the new device.</p> Source code in <code>torch_choice/data/joint_dataset.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; \"JointDataset\":\n    \"\"\"Moves all datasets in this dataset to the specified PyTorch device.\n\n    Args:\n        device (Union[str, torch.device]): the destination device.\n\n    Returns:\n        ChoiceDataset: the modified dataset on the new device.\n    \"\"\"\n    for d in self.datasets.values():\n        d = d.to(device)\n    return self\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.device","title":"<code>device: device</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the device of the coefficient.</p> <p>Returns:</p> Type Description <code>torch.device</code> <p>the device of the model.</p>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.num_params","title":"<code>num_params: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved.</p> <p>Returns:</p> Type Description <code>int</code> <p>the total number of learnable parameters.</p>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__init__","title":"<code>__init__(self, formula=None, dataset=None, coef_variation_dict=None, num_param_dict=None, num_items=None, num_users=None, regularization=None, regularization_weight=None, weight_initialization=None, model_outside_option=False)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>a string representing the utility formula. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.</p> <code>None</code> <code>data</code> <code>ChoiceDataset</code> <p>a ChoiceDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients from the ChoiceDataset.</p> required <code>coef_variation_dict</code> <code>Dict[str, str]</code> <p>variable type to variation level dictionary. Keys of this dictionary should be variable names in the dataset (i.e., these starting with <code>itemsession_</code>, <code>price_</code>, <code>user_</code>, etc), or <code>intercept</code> if the researcher requires an intercept term. For each variable name X_var (e.g., <code>user_income</code>) or <code>intercept</code>, the corresponding dictionary key should be one of the following values, this value specifies the \"level of variation\" of the coefficient.</p> <ul> <li> <p><code>constant</code>: the coefficient constant over all users and items: \\(X \beta\\).</p> </li> <li> <p><code>user</code>: user-specific parameters but constant across all items: \\(X \beta_{u}\\).</p> </li> <li> <p><code>item</code>: item-specific parameters but constant across all users, \\(X \beta_{i}\\).     Note that the coefficients for the first item are forced to be zero following the standard practice     in econometrics.</p> </li> <li> <p><code>item-full</code>: the same configuration as <code>item</code>, but does not force the coefficients of the first item to     be zeros.</p> </li> </ul> <p>The following configurations are supported by the package, but we don't recommend using them due to the     large number of parameters. - <code>user-item</code>: parameters that are specific to both user and item, parameter for the first item     for all users are forced to be zero.</p> <ul> <li><code>user-item-full</code>: parameters that are specific to both user and item, explicitly model for all items.</li> </ul> <code>None</code> <code>num_param_dict</code> <code>Optional[Dict[str, int]]</code> <p>variable type to number of parameters dictionary with keys exactly the same as the <code>coef_variation_dict</code>. Values of <code>num_param_dict</code> records numbers of features in each kind of variable. If None is supplied, num_param_dict will be a dictionary with the same keys as the <code>coef_variation_dict</code> dictionary and values of all ones. Default to be None.</p> <code>None</code> <code>num_items</code> <code>int</code> <p>number of items in the dataset.</p> <code>None</code> <code>num_users</code> <code>int</code> <p>number of users in the dataset.</p> <code>None</code> <code>regularization</code> <code>Optional[str]</code> <p>this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None.</p> <code>None</code> <code>regularization_weight</code> <code>Optional[float]</code> <p>the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None.</p> <code>None</code> <code>weight_initialization</code> <code>Optional[Union[str, Dict[str, str]]]</code> <p>controls for how coefficients are initialized; users can pass a string from {'normal', 'uniform', 'zero'} to initialize all coefficients in the same way. Alternatively, users can pass a dictionary with keys exactly the same as the <code>coef_variation_dict</code> dictionary, and values from {'normal', 'uniform', 'zero'} to initialize coefficients of different types of variables differently. By default, all coefficients are initialized following a standard normal distribution.</p> <code>None</code> <code>model_outside_option</code> <code>Optional[bool]</code> <p>whether to explicitly model the outside option (i.e., the consumer did not buy anything). To enable modeling outside option, the outside option is indicated by <code>item_index[n] == -1</code> in the item-index-tensor. In this case, the item-index-tensor can contain values in <code>{-1, 0, 1, ..., num_items-1}</code>. Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in <code>{0, 1, ..., num_items-1}</code>. The utility of the outside option is always set to 0 while computing the probability. By default, model_outside_option is set to False and the model does not model the outside option.</p> <code>False</code> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def __init__(self,\n             formula: Optional[str]=None,\n             dataset: Optional[ChoiceDataset]=None,\n             coef_variation_dict: Optional[Dict[str, str]]=None,\n             num_param_dict: Optional[Dict[str, int]]=None,\n             num_items: Optional[int]=None,\n             num_users: Optional[int]=None,\n             regularization: Optional[str]=None,\n             regularization_weight: Optional[float]=None,\n             weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n             model_outside_option: Optional[bool]=False\n             ) -&gt; None:\n    \"\"\"\n    Args:\n        formula (str): a string representing the utility formula.\n            The formula consists of '(variable_name|variation)'s separated by '+', for example:\n            \"(var1|item) + (var2|user) + (var3|constant)\"\n            where the first part of each term is the name of the variable\n            and the second part is the variation of the coefficient.\n            The variation can be one of the following:\n            'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'.\n            All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.\n        data (ChoiceDataset): a ChoiceDataset object for training the model, the parser will infer dimensions of variables\n            and sizes of coefficients from the ChoiceDataset.\n        coef_variation_dict (Dict[str, str]): variable type to variation level dictionary. Keys of this dictionary\n            should be variable names in the dataset (i.e., these starting with `itemsession_`, `price_`, `user_`, etc), or `intercept`\n            if the researcher requires an intercept term.\n            For each variable name X_var (e.g., `user_income`) or `intercept`, the corresponding dictionary key should\n            be one of the following values, this value specifies the \"level of variation\" of the coefficient.\n\n            - `constant`: the coefficient constant over all users and items: $X \\beta$.\n\n            - `user`: user-specific parameters but constant across all items: $X \\beta_{u}$.\n\n            - `item`: item-specific parameters but constant across all users, $X \\beta_{i}$.\n                Note that the coefficients for the first item are forced to be zero following the standard practice\n                in econometrics.\n\n            - `item-full`: the same configuration as `item`, but does not force the coefficients of the first item to\n                be zeros.\n\n            The following configurations are supported by the package, but we don't recommend using them due to the\n                large number of parameters.\n            - `user-item`: parameters that are specific to both user and item, parameter for the first item\n                for all users are forced to be zero.\n\n            - `user-item-full`: parameters that are specific to both user and item, explicitly model for all items.\n        num_param_dict (Optional[Dict[str, int]]): variable type to number of parameters dictionary with keys exactly the same\n            as the `coef_variation_dict`. Values of `num_param_dict` records numbers of features in each kind of variable.\n            If None is supplied, num_param_dict will be a dictionary with the same keys as the `coef_variation_dict` dictionary\n            and values of all ones. Default to be None.\n        num_items (int): number of items in the dataset.\n        num_users (int): number of users in the dataset.\n        regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of\n            regularization added to the log-likelihood.\n            - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood.\n            - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood.\n            - None does not modify the log-likelihood.\n            Defaults to None.\n        regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood.\n            This term controls the strength of regularization. This argument is required if and only if regularization\n            is not None.\n            Defaults to None.\n        weight_initialization (Optional[Union[str, Dict[str, str]]]): controls for how coefficients are initialized;\n            users can pass a string from {'normal', 'uniform', 'zero'} to initialize all coefficients in the same way.\n            Alternatively, users can pass a dictionary with keys exactly the same as the `coef_variation_dict` dictionary,\n            and values from {'normal', 'uniform', 'zero'} to initialize coefficients of different types of variables differently.\n            By default, all coefficients are initialized following a standard normal distribution.\n        model_outside_option (Optional[bool]): whether to explicitly model the outside option (i.e., the consumer did not buy anything).\n            To enable modeling outside option, the outside option is indicated by `item_index[n] == -1` in the item-index-tensor.\n            In this case, the item-index-tensor can contain values in `{-1, 0, 1, ..., num_items-1}`.\n            Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in `{0, 1, ..., num_items-1}`.\n            The utility of the outside option is always set to 0 while computing the probability.\n            By default, model_outside_option is set to False and the model does not model the outside option.\n    \"\"\"\n    # ==============================================================================================================\n    # Check that the model received a valid combination of inputs so that it can be initialized.\n    # ==============================================================================================================\n    if coef_variation_dict is None and formula is None:\n        raise ValueError(\"Either coef_variation_dict or formula should be provided to specify the model.\")\n\n    if (coef_variation_dict is not None) and (formula is not None):\n        raise ValueError(\"Only one of coef_variation_dict or formula should be provided to specify the model.\")\n\n    if (formula is not None) and (dataset is None):\n        raise ValueError(\"If formula is provided, data should be provided to specify the model.\")\n\n\n    # ==============================================================================================================\n    # Build necessary dictionaries for model initialization.\n    # ==============================================================================================================\n    if formula is None:\n        # Use dictionaries to initialize the model.\n        if num_param_dict is None:\n            warnings.warn(\"`num_param_dict` is not provided, all variables will be treated as having one parameter.\")\n            num_param_dict = {key:1 for key in coef_variation_dict.keys()}\n\n        assert coef_variation_dict.keys() == num_param_dict.keys()\n\n        # variable `var` with variation `spec` to variable `var[spec]`.\n        rename = dict()  # old variable name --&gt; new variable name.\n        for variable, specificity in coef_variation_dict.items():\n            rename[variable] = f\"{variable}[{specificity}]\"\n\n        for old_name, new_name in rename.items():\n            coef_variation_dict[new_name] = coef_variation_dict.pop(old_name)\n            num_param_dict[new_name] = num_param_dict.pop(old_name)\n    else:\n        # Use the formula to infer model.\n        coef_variation_dict, num_param_dict = parse_formula(formula, dataset)\n\n    # ==============================================================================================================\n    # Model Initialization.\n    # ==============================================================================================================\n    super(ConditionalLogitModel, self).__init__()\n\n    self.coef_variation_dict = deepcopy(coef_variation_dict)\n    self.num_param_dict = deepcopy(num_param_dict)\n\n    self.num_items = num_items\n    self.num_users = num_users\n\n    self.regularization = deepcopy(regularization)\n    assert self.regularization in ['L1', 'L2', None], f\"Provided regularization={self.regularization} is not allowed, allowed values are ['L1', 'L2', None].\"\n    self.regularization_weight = regularization_weight\n    if (self.regularization is not None) and (self.regularization_weight is None):\n        raise ValueError(f'You specified regularization type {self.regularization} without providing regularization_weight.')\n    if (self.regularization is None) and (self.regularization_weight is not None):\n        raise ValueError(f'You specified no regularization but you provide regularization_weight={self.regularization_weight}, you should leave regularization_weight as None if you do not want to regularize the model.')\n\n    # check number of parameters specified are all positive.\n    for var_type, num_params in self.num_param_dict.items():\n        assert num_params &gt; 0, f'num_params needs to be positive, got: {num_params}.'\n\n    # infer the number of parameters for intercept if the researcher forgets.\n    for variable in self.coef_variation_dict.keys():\n        if self.is_intercept_term(variable) and variable not in self.num_param_dict.keys():\n            warnings.warn(f\"`{variable}` key found in coef_variation_dict but not in num_param_dict, num_param_dict['{variable}'] has been set to 1.\")\n            self.num_param_dict[variable] = 1\n\n    # inform coefficients their ways of being initialized.\n    self.weight_initialization = deepcopy(weight_initialization)\n\n    # construct trainable parameters.\n    coef_dict = dict()\n    for var_type, variation in self.coef_variation_dict.items():\n        if isinstance(self.weight_initialization, dict):\n            if var_type.split('[')[0] in self.weight_initialization.keys():\n                # use the variable-specific initialization if provided.\n                init = self.weight_initialization[var_type.split('[')[0]]\n            else:\n                # use default initialization.\n                init = None\n        else:\n            # initialize all coefficients in the same way.\n            init = self.weight_initialization\n\n        coef_dict[var_type] = Coefficient(variation=variation,\n                                          num_items=self.num_items,\n                                          num_users=self.num_users,\n                                          num_params=self.num_param_dict[var_type],\n                                          init=init)\n    # A ModuleDict is required to properly register all trainable parameters.\n    # self.parameter() will fail if a python dictionary is used instead.\n    self.coef_dict = nn.ModuleDict(coef_dict)\n    self.model_outside_option = model_outside_option\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.__repr__","title":"<code>__repr__(self)</code>  <code>special</code>","text":"<p>Return a string representation of the model.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the model.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the model.\n\n    Returns:\n        str: the string representation of the model.\n    \"\"\"\n    out_str_lst = ['Conditional logistic discrete choice model, expects input features:\\n']\n    for var_type, num_params in self.num_param_dict.items():\n        out_str_lst.append(f'X[{var_type}] with {num_params} parameters, with {self.coef_variation_dict[var_type]} level variation.')\n    return super().__repr__() + '\\n' + '\\n'.join(out_str_lst) + '\\n' + f'device={self.device}'\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.forward","title":"<code>forward(self, batch, manual_coef_value_dict=None)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ChoiceDataset</code> <p>a <code>ChoiceDataset</code> object.</p> required <code>manual_coef_value_dict</code> <code>Optional[Dict[str, torch.Tensor]]</code> <p>a dictionary with keys in {'u', 'i'} etc and tensors as values. If provided, the model will force coefficient to be the provided values and compute utility conditioned on the provided coefficient values. This feature is useful when the research wishes to plug in particular values of coefficients and examine the utility values. If not provided, the model will use the learned coefficient values in self.coef_dict. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>a tensor of shape (num_trips, num_items) whose (t, i) entry represents     the utility from item i in trip t for the user involved in that trip.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def forward(self,\n            batch: ChoiceDataset,\n            manual_coef_value_dict: Optional[Dict[str, torch.Tensor]] = None\n            ) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        batch: a `ChoiceDataset` object.\n\n        manual_coef_value_dict (Optional[Dict[str, torch.Tensor]], optional): a dictionary with\n            keys in {'u', 'i'} etc and tensors as values. If provided, the model will force\n            coefficient to be the provided values and compute utility conditioned on the provided\n            coefficient values. This feature is useful when the research wishes to plug in particular\n            values of coefficients and examine the utility values. If not provided, the model will\n            use the learned coefficient values in self.coef_dict.\n            Defaults to None.\n\n    Returns:\n        torch.Tensor: a tensor of shape (num_trips, num_items) whose (t, i) entry represents\n            the utility from item i in trip t for the user involved in that trip.\n    \"\"\"\n    x_dict = batch.x_dict\n\n    for variable in self.coef_variation_dict.keys():\n        if self.is_intercept_term(variable):\n            # intercept term has no input tensor from the ChoiceDataset data structure.\n            # the tensor for intercept has only 1 feature, every entry is 1.\n            x_dict['intercept'] = torch.ones((len(batch), self.num_items, 1), device=batch.device)\n            break\n\n    # compute the utility from each item in each choice session.\n    total_utility = torch.zeros((len(batch), self.num_items), device=batch.device)\n    # for each type of variables, apply the corresponding coefficient to input x.\n\n    for var_type, coef in self.coef_dict.items():\n        # variable type is named as \"observable_name[variation]\", retrieve the corresponding observable name.\n        corresponding_observable = var_type.split(\"[\")[0]\n        total_utility += coef(\n            x_dict[corresponding_observable],\n            batch.user_index,\n            manual_coef_value=None if manual_coef_value_dict is None else manual_coef_value_dict[var_type])\n\n    assert total_utility.shape == (len(batch), self.num_items)\n\n    if batch.item_availability is not None:\n        # mask out unavailable items.\n        total_utility[~batch.item_availability[batch.session_index, :]] = torch.finfo(total_utility.dtype).min / 2\n\n    # accommodate the outside option.\n    if self.model_outside_option:\n        # the outside option has zero utility.\n        util_zero = torch.zeros(total_utility.size(0), 1, device=batch.device)  # (len(batch), 1)  zero tensor.\n        # outside option is indicated by item_index == -1, we put it at the end.\n        total_utility = torch.cat((total_utility, util_zero), dim=1)  # (len(batch), num_items+1)\n    return total_utility\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.get_coefficient","title":"<code>get_coefficient(self, variable)</code>","text":"<p>Retrieve the coefficient tensor for the given variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>the variable name.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>the corresponding coefficient tensor of the requested variable.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def get_coefficient(self, variable: str) -&gt; torch.Tensor:\n    \"\"\"Retrieve the coefficient tensor for the given variable.\n\n    Args:\n        variable (str): the variable name.\n\n    Returns:\n        torch.Tensor: the corresponding coefficient tensor of the requested variable.\n    \"\"\"\n    return self.state_dict()[f\"coef_dict.{variable}.coef\"].detach().clone()\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.loss","title":"<code>loss(self, *args, **kwargs)</code>","text":"<p>The loss function to be optimized. This is a wrapper of <code>negative_log_likelihood</code> + regularization loss if required.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def loss(self, *args, **kwargs):\n    \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\"\n    nll = self.negative_log_likelihood(*args, **kwargs)\n    if self.regularization is not None:\n        L = {'L1': 1, 'L2': 2}[self.regularization]\n        for param in self.parameters():\n            nll += self.regularization_weight * torch.norm(param, p=L)\n    return nll\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.negative_log_likelihood","title":"<code>negative_log_likelihood(self, batch, y, is_train=True)</code>","text":"<p>Computes the log-likelihood for the batch and label. TODO: consider remove y, change to label. TODO: consider move this method outside the model, the role of the model is to compute the utility.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ChoiceDataset</code> <p>a ChoiceDataset object containing the data.</p> required <code>y</code> <code>torch.Tensor</code> <p>the label.</p> required <code>is_train</code> <code>bool</code> <p>whether to trace the gradient. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>the negative log-likelihood.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def negative_log_likelihood(self, batch: ChoiceDataset, y: torch.Tensor, is_train: bool=True) -&gt; torch.Tensor:\n    \"\"\"Computes the log-likelihood for the batch and label.\n    TODO: consider remove y, change to label.\n    TODO: consider move this method outside the model, the role of the model is to compute the utility.\n\n    Args:\n        batch (ChoiceDataset): a ChoiceDataset object containing the data.\n        y (torch.Tensor): the label.\n        is_train (bool, optional): whether to trace the gradient. Defaults to True.\n\n    Returns:\n        torch.Tensor: the negative log-likelihood.\n    \"\"\"\n    if is_train:\n        self.train()\n    else:\n        self.eval()\n    # (num_trips, num_items)\n    total_utility = self.forward(batch)\n    # check shapes.\n    if self.model_outside_option:\n        assert total_utility.shape == (len(batch), self.num_items+1)\n        assert torch.all(total_utility[:, -1] == 0), \"The last column of total_utility should be all zeros, which corresponds to the outside option.\"\n    else:\n        assert total_utility.shape == (len(batch), self.num_items)\n    logP = torch.log_softmax(total_utility, dim=1)\n    # since y == -1 indicates the outside option and the last column of total_utility is the outside option, the following\n    # indexing should correctly retrieve the log-likelihood even for outside options.\n    nll = - logP[torch.arange(len(y)), y].sum()\n    return nll\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.conditional_logit_model.ConditionalLogitModel.summary","title":"<code>summary(self)</code>","text":"<p>Print out the current model parameter.</p> Source code in <code>torch_choice/model/conditional_logit_model.py</code> <pre><code>def summary(self):\n    \"\"\"Print out the current model parameter.\"\"\"\n    for var_type, coefficient in self.coef_dict.items():\n        if coefficient is not None:\n            print('Variable Type: ', var_type)\n            print(coefficient.coef)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.device","title":"<code>device: device</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the device of the coefficient.</p> <p>Returns:</p> Type Description <code>torch.device</code> <p>the device of the model.</p>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.num_params","title":"<code>num_params: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the total number of parameters. For example, if there is only an user-specific coefficient to be multiplied with the K-dimensional observable, then the total number of parameters would be K x number of users, assuming no intercept is involved.</p> <p>Returns:</p> Type Description <code>int</code> <p>the total number of learnable parameters.</p>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.__init__","title":"<code>__init__(self, nest_to_item, nest_coef_variation_dict=None, nest_num_param_dict=None, item_coef_variation_dict=None, item_num_param_dict=None, item_formula=None, nest_formula=None, dataset=None, num_users=None, shared_lambda=False, regularization=None, regularization_weight=None, nest_weight_initialization=None, item_weight_initialization=None, model_outside_option=False)</code>  <code>special</code>","text":"<p>Initialization method of the nested logit model.</p> <p>Parameters:</p> Name Type Description Default <code>nest_to_item</code> <code>Dict[object, List[int]]</code> <p>a dictionary maps a nest ID to a list of items IDs of the queried nest.</p> required <code>nest_coef_variation_dict</code> <code>Dict[str, str]</code> <p>a dictionary maps a variable type (i.e., variable group) to the level of variation for the coefficient of this type of variables.</p> <code>None</code> <code>nest_num_param_dict</code> <code>Dict[str, int]</code> <p>a dictionary maps a variable type name to the number of parameters in this variable group.</p> <code>None</code> <code>item_coef_variation_dict</code> <code>Dict[str, str]</code> <p>the same as nest_coef_variation_dict but for item features.</p> <code>None</code> <code>item_num_param_dict</code> <code>Dict[str, int]</code> <p>the same as nest_num_param_dict but for item features.</p> <code>None</code> <code>{nest,</code> <code>item}_formula (str</code> <p>a string representing the utility formula for the {nest, item} level logit model. The formula consists of '(variable_name|variation)'s separated by '+', for example: \"(var1|item) + (var2|user) + (var3|constant)\" where the first part of each term is the name of the variable and the second part is the variation of the coefficient. The variation can be one of the following: 'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'. All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.</p> required <code>dataset</code> <code>JointDataset</code> <p>a JointDataset object for training the model, the parser will infer dimensions of variables and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item'].</p> <code>None</code> <code>num_users</code> <code>Optional[int]</code> <p>number of users to be modelled, this is only required if any of variable type requires user-specific variations. Defaults to None.</p> <code>None</code> <code>shared_lambda</code> <code>bool</code> <p>a boolean indicating whether to enforce the elasticity lambda, which is the coefficient for inclusive values, to be constant for all nests. The lambda enters the nest-level selection as the following Utility of choosing nest k = lambda * inclusive value of nest k                                + linear combination of some other nest level features If set to True, a single lambda will be learned for all nests, otherwise, the model learns an individual lambda for each nest. Defaults to False.</p> <code>False</code> <code>regularization</code> <code>Optional[str]</code> <p>this argument takes values from {'L1', 'L2', None}, which specifies the type of regularization added to the log-likelihood. - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood. - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood. - None does not modify the log-likelihood. Defaults to None.</p> <code>None</code> <code>regularization_weight</code> <code>Optional[float]</code> <p>the weight of parameter norm subtracted from the log-likelihood. This term controls the strength of regularization. This argument is required if and only if regularization is not None. Defaults to None.</p> <code>None</code> <code>{nest,</code> <code>item}_weight_initialization (Optional[Union[str, Dict[str, str]]]</code> <p>methods to initialize the weights of coefficients for {nest, item} level model. Please refer to the <code>weight_initialization</code> keyword in ConditionalLogitModel's documentation for more details.</p> required <code>model_outside_option</code> <code>Optional[bool]</code> <p>whether to explicitly model the outside option (i.e., the consumer did not buy anything). To enable modeling outside option, the outside option is indicated by <code>item_index[n] == -1</code> in the item-index-tensor. In this case, the item-index-tensor can contain values in <code>{-1, 0, 1, ..., num_items-1}</code>. Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in <code>{0, 1, ..., num_items-1}</code>. The utility of the outside option is always set to 0 while computing the probability. By default, model_outside_option is set to False and the model does not model the outside option.</p> <code>False</code> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def __init__(self,\n             nest_to_item: Dict[object, List[int]],\n             # method 1: specify variation and num param. dictionary.\n             nest_coef_variation_dict: Optional[Dict[str, str]]=None,\n             nest_num_param_dict: Optional[Dict[str, int]]=None,\n             item_coef_variation_dict: Optional[Dict[str, str]]=None,\n             item_num_param_dict: Optional[Dict[str, int]]=None,\n             # method 2: specify formula and dataset.\n             item_formula: Optional[str]=None,\n             nest_formula: Optional[str]=None,\n             dataset: Optional[JointDataset]=None,\n             num_users: Optional[int]=None,\n             shared_lambda: bool=False,\n             regularization: Optional[str]=None,\n             regularization_weight: Optional[float]=None,\n             nest_weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n             item_weight_initialization: Optional[Union[str, Dict[str, str]]]=None,\n             model_outside_option: Optional[bool]=False\n             ) -&gt; None:\n    \"\"\"Initialization method of the nested logit model.\n\n    Args:\n        nest_to_item (Dict[object, List[int]]): a dictionary maps a nest ID to a list\n            of items IDs of the queried nest.\n\n        nest_coef_variation_dict (Dict[str, str]): a dictionary maps a variable type\n            (i.e., variable group) to the level of variation for the coefficient of this type\n            of variables.\n        nest_num_param_dict (Dict[str, int]): a dictionary maps a variable type name to\n            the number of parameters in this variable group.\n\n        item_coef_variation_dict (Dict[str, str]): the same as nest_coef_variation_dict but\n            for item features.\n        item_num_param_dict (Dict[str, int]): the same as nest_num_param_dict but for item\n            features.\n\n        {nest, item}_formula (str): a string representing the utility formula for the {nest, item} level logit model.\n            The formula consists of '(variable_name|variation)'s separated by '+', for example:\n            \"(var1|item) + (var2|user) + (var3|constant)\"\n            where the first part of each term is the name of the variable\n            and the second part is the variation of the coefficient.\n            The variation can be one of the following:\n            'constant', 'item', 'item-full', 'user', 'user-item', 'user-item-full'.\n            All spaces in the formula will be ignored, hence please do not use spaces in variable/observable names.\n        dataset (JointDataset): a JointDataset object for training the model, the parser will infer dimensions of variables\n            and sizes of coefficients for the nest level model from dataset.datasets['nest']. The parser will infer dimensions of variables and sizes of coefficients for the item level model from dataset.datasets['item'].\n\n        num_users (Optional[int], optional): number of users to be modelled, this is only\n            required if any of variable type requires user-specific variations.\n            Defaults to None.\n\n        shared_lambda (bool): a boolean indicating whether to enforce the elasticity lambda, which\n            is the coefficient for inclusive values, to be constant for all nests.\n            The lambda enters the nest-level selection as the following\n            Utility of choosing nest k = lambda * inclusive value of nest k\n                                           + linear combination of some other nest level features\n            If set to True, a single lambda will be learned for all nests, otherwise, the\n            model learns an individual lambda for each nest.\n            Defaults to False.\n\n        regularization (Optional[str]): this argument takes values from {'L1', 'L2', None}, which specifies the type of\n            regularization added to the log-likelihood.\n            - 'L1' will subtract regularization_weight * 1-norm of parameters from the log-likelihood.\n            - 'L2' will subtract regularization_weight * 2-norm of parameters from the log-likelihood.\n            - None does not modify the log-likelihood.\n            Defaults to None.\n\n        regularization_weight (Optional[float]): the weight of parameter norm subtracted from the log-likelihood.\n            This term controls the strength of regularization. This argument is required if and only if regularization\n            is not None.\n            Defaults to None.\n\n        {nest, item}_weight_initialization (Optional[Union[str, Dict[str, str]]]): methods to initialize the weights of\n            coefficients for {nest, item} level model. Please refer to the `weight_initialization` keyword in ConditionalLogitModel's documentation for more details.\n\n        model_outside_option (Optional[bool]): whether to explicitly model the outside option (i.e., the consumer did not buy anything).\n            To enable modeling outside option, the outside option is indicated by `item_index[n] == -1` in the item-index-tensor.\n            In this case, the item-index-tensor can contain values in `{-1, 0, 1, ..., num_items-1}`.\n            Otherwise, if the outside option is not modelled, the item-index-tensor should only contain values in `{0, 1, ..., num_items-1}`.\n            The utility of the outside option is always set to 0 while computing the probability.\n            By default, model_outside_option is set to False and the model does not model the outside option.\n    \"\"\"\n    # handle nest level model.\n    using_formula_to_initiate = (item_formula is not None) and (nest_formula is not None)\n    if using_formula_to_initiate:\n        # make sure that the research does not specify duplicated information, which might cause conflict.\n        if (nest_coef_variation_dict is not None) or (item_coef_variation_dict is not None):\n            raise ValueError('You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_coef_variation_dict at the same time.')\n        if (nest_num_param_dict is not None) or (item_num_param_dict is not None):\n            raise ValueError('You specify the {item, nest}_formula to initiate the model, you should not specify the {item, nest}_num_param_dict at the same time.')\n        if dataset is None:\n            raise ValueError('Dataset is required if {item, nest}_formula is specified to initiate the model.')\n\n        nest_coef_variation_dict, nest_num_param_dict = parse_formula(nest_formula, dataset.datasets['nest'])\n        item_coef_variation_dict, item_num_param_dict = parse_formula(item_formula, dataset.datasets['item'])\n\n    else:\n        # check for conflicting information.\n        if (nest_formula is not None) or (item_formula is not None):\n            raise ValueError('You should not specify {item, nest}_formula and {item, nest}_coef_variation_dict at the same time.')\n        # make sure that the research specifies all the required information.\n        if (nest_coef_variation_dict is None) or (item_coef_variation_dict is None):\n            raise ValueError('You should specify the {item, nest}_coef_variation_dict to initiate the model.')\n        if (nest_num_param_dict is None) or (item_num_param_dict is None):\n            raise ValueError('You should specify the {item, nest}_num_param_dict to initiate the model.')\n\n    super(NestedLogitModel, self).__init__()\n    self.nest_to_item = nest_to_item\n    self.nest_coef_variation_dict = nest_coef_variation_dict\n    self.nest_num_param_dict = nest_num_param_dict\n    self.item_coef_variation_dict = item_coef_variation_dict\n    self.item_num_param_dict = item_num_param_dict\n    self.num_users = num_users\n\n    self.nests = list(nest_to_item.keys())\n    self.num_nests = len(self.nests)\n    self.num_items = sum(len(items) for items in nest_to_item.values())\n\n    # nest coefficients.\n    self.nest_coef_dict = self._build_coef_dict(self.nest_coef_variation_dict,\n                                                self.nest_num_param_dict,\n                                                self.num_nests,\n                                                weight_initialization=deepcopy(nest_weight_initialization))\n\n    # item coefficients.\n    self.item_coef_dict = self._build_coef_dict(self.item_coef_variation_dict,\n                                                self.item_num_param_dict,\n                                                self.num_items,\n                                                weight_initialization=deepcopy(item_weight_initialization))\n\n    self.shared_lambda = shared_lambda\n    if self.shared_lambda:\n        self.lambda_weight = nn.Parameter(torch.ones(1), requires_grad=True)\n    else:\n        self.lambda_weight = nn.Parameter(torch.ones(self.num_nests) / 2, requires_grad=True)\n    # breakpoint()\n    # self.iv_weights = nn.Parameter(torch.ones(1), requires_grad=True)\n    # used to warn users if forgot to call clamp.\n    self._clamp_called_flag = True\n\n    self.regularization = regularization\n    assert self.regularization in ['L1', 'L2', None], f\"Provided regularization={self.regularization} is not allowed, allowed values are ['L1', 'L2', None].\"\n    self.regularization_weight = regularization_weight\n    if (self.regularization is not None) and (self.regularization_weight is None):\n        raise ValueError(f'You specified regularization type {self.regularization} without providing regularization_weight.')\n    if (self.regularization is None) and (self.regularization_weight is not None):\n        raise ValueError(f'You specified no regularization but you provide regularization_weight={self.regularization_weight}, you should leave regularization_weight as None if you do not want to regularize the model.')\n\n    self.model_outside_option = model_outside_option\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward","title":"<code>forward(self, batch)</code>","text":"<p>An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the     predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide     this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument.     For more details about the forward passing, please refer to the _forward() method.</p>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.forward--todo-the-conditionallogitmodel-returns-predicted-utility-the-nestedlogitmodel-behaves-the-same","title":"TODO: the ConditionalLogitModel returns predicted utility, the NestedLogitModel behaves the same?","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ChoiceDataset</code> <p>a ChoiceDataset object containing the data batch.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>a tensor of shape (num_trips, num_items) including the log probability of choosing item i in trip t.</p> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def forward(self, batch: ChoiceDataset) -&gt; torch.Tensor:\n    \"\"\"An standard forward method for the model, the user feeds a ChoiceDataset batch and the model returns the\n        predicted log-likelihood tensor. The main forward passing happens in the _forward() method, but we provide\n        this wrapper forward() method for a cleaner API, as forward() only requires a single batch argument.\n        For more details about the forward passing, please refer to the _forward() method.\n\n    # TODO: the ConditionalLogitModel returns predicted utility, the NestedLogitModel behaves the same?\n\n    Args:\n        batch (ChoiceDataset): a ChoiceDataset object containing the data batch.\n\n    Returns:\n        torch.Tensor: a tensor of shape (num_trips, num_items) including the log probability\n        of choosing item i in trip t.\n    \"\"\"\n    return self._forward(batch['nest'].x_dict,\n                         batch['item'].x_dict,\n                         batch['item'].user_index,\n                         batch['item'].item_availability)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.get_coefficient","title":"<code>get_coefficient(self, variable, level=None)</code>","text":"<p>Retrieve the coefficient tensor for the given variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>the variable name.</p> required <code>level</code> <code>str</code> <p>from which level of model to extract the coefficient, can be 'item' or 'nest'. The <code>level</code> argument will be discarded if <code>variable</code> is <code>lambda</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>the corresponding coefficient tensor of the requested variable.</p> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def get_coefficient(self, variable: str, level: Optional[str] = None) -&gt; torch.Tensor:\n    \"\"\"Retrieve the coefficient tensor for the given variable.\n\n    Args:\n        variable (str): the variable name.\n        level (str): from which level of model to extract the coefficient, can be 'item' or 'nest'. The `level` argument will be discarded if `variable` is `lambda`.\n\n    Returns:\n        torch.Tensor: the corresponding coefficient tensor of the requested variable.\n    \"\"\"\n    if variable == 'lambda':\n        return self.lambda_weight.detach().clone()\n\n    if level not in ['item', 'nest']:\n        raise ValueError(f\"Level should be either 'item' or 'nest', got {level}.\")\n\n    return self.state_dict()[f'{level}_coef_dict.{variable}.coef'].detach().clone()\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.log_likelihood","title":"<code>log_likelihood(self, *args)</code>","text":"<p>Computes the log likelihood of the model, please refer to the negative_log_likelihood() method.</p> <p>Returns:</p> Type Description <code>_type_</code> <p>the log likelihood of the model.</p> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def log_likelihood(self, *args):\n    \"\"\"Computes the log likelihood of the model, please refer to the negative_log_likelihood() method.\n\n    Returns:\n        _type_: the log likelihood of the model.\n    \"\"\"\n    return - self.negative_log_likelihood(*args)\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.loss","title":"<code>loss(self, *args, **kwargs)</code>","text":"<p>The loss function to be optimized. This is a wrapper of <code>negative_log_likelihood</code> + regularization loss if required.</p> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def loss(self, *args, **kwargs):\n    \"\"\"The loss function to be optimized. This is a wrapper of `negative_log_likelihood` + regularization loss if required.\"\"\"\n    nll = self.negative_log_likelihood(*args, **kwargs)\n    if self.regularization is not None:\n        L = {'L1': 1, 'L2': 2}[self.regularization]\n        for name, param in self.named_parameters():\n            if name == 'lambda_weight':\n                # we don't regularize the lambda term, we only regularize coefficients.\n                continue\n            nll += self.regularization_weight * torch.norm(param, p=L)\n    return nll\n</code></pre>"},{"location":"api_torch_choice/#torch_choice.model.nested_logit_model.NestedLogitModel.negative_log_likelihood","title":"<code>negative_log_likelihood(self, batch, y, is_train=True)</code>","text":"<p>Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples     in batch instead of the average.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ChoiceDataset</code> <p>the ChoiceDataset object containing the data.</p> required <code>y</code> <code>torch.LongTensor</code> <p>the label.</p> required <code>is_train</code> <code>bool</code> <p>which mode of the model to be used for the forward passing, if we need Hessian of the NLL through auto-grad, <code>is_train</code> should be set to True. If we merely need a performance metric, then <code>is_train</code> can be set to False for better performance. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>torch.scalar_tensor</code> <p>the negative log likelihood of the model.</p> Source code in <code>torch_choice/model/nested_logit_model.py</code> <pre><code>def negative_log_likelihood(self,\n                            batch: ChoiceDataset,\n                            y: torch.LongTensor,\n                            is_train: bool=True) -&gt; torch.scalar_tensor:\n    \"\"\"Computes the negative log likelihood of the model. Please note the log-likelihood is summed over all samples\n        in batch instead of the average.\n\n    Args:\n        batch (ChoiceDataset): the ChoiceDataset object containing the data.\n        y (torch.LongTensor): the label.\n        is_train (bool, optional): which mode of the model to be used for the forward passing, if we need Hessian\n            of the NLL through auto-grad, `is_train` should be set to True. If we merely need a performance metric,\n            then `is_train` can be set to False for better performance.\n            Defaults to True.\n\n    Returns:\n        torch.scalar_tensor: the negative log likelihood of the model.\n    \"\"\"\n    # compute the negative log-likelihood loss directly.\n    if is_train:\n        self.train()\n    else:\n        self.eval()\n    # (num_trips, num_items)\n    logP = self.forward(batch)\n    # check shapes\n    if self.model_outside_option:\n        assert logP.shape == (len(batch['item']), self.num_items+1)\n    else:\n        assert logP.shape == (len(batch['item']), self.num_items)\n    # since y == -1 indicates the outside option and the last column of total_utility is the outside option, the following\n    # indexing should correctly retrieve the log-likelihood even for outside options.\n    nll = - logP[torch.arange(len(y)), y].sum()\n    return nll\n</code></pre>"},{"location":"coefficient_initialization/","title":"Coefficient Initialization","text":"<p>Tianyu Du</p> <p>Added since version <code>1.0.4</code></p> <p>[From ChatGPT] Coefficient initialization is an essential component of model estimation, especially in the context of machine learning and deep learning. The choice of initial coefficients can dramatically impact the efficiency, speed, and even the ultimate success of model training. Poor initialization can lead to slow convergence during the optimization process or result in the model getting stuck in suboptimal local minima, particularly in models with non-convex loss landscapes such as neural networks. Additionally, it can exacerbate the problem of vanishing or exploding gradients, inhibiting the backpropagation process. Conversely, thoughtful and strategic initialization, like Xavier or He initialization, can lead to faster convergence, better generalization performance, and more robust models. Thus, the way coefficients are initialized can significantly influence the effectiveness and reliability of machine learning models.</p> <pre><code>import torch\nimport torch_choice\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: &lt;B3E58761-2785-34C6-A89B-F37110C88A05&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;AE6DCE26-A528-35ED-BB3D-88890D27E6B9&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n  warn(f\"Failed to load image Python extension: {e}\")\n</code></pre>"},{"location":"coefficient_initialization/#conditional-logit-models","title":"Conditional Logit Models","text":""},{"location":"coefficient_initialization/#by-default-coefficients-are-initialized-following-a-standard-gaussian-distribution","title":"By default, coefficients are initialized following a standard Gaussian distribution.","text":"<p>Here we create a \"big\" model of thousands of parameters to illustrate the distribution of coefficients.</p> <pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10)\n</code></pre> <pre><code>def plot_model_initial_coefficients(model_to_plot: torch.nn.Module) -&gt; None:\n    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 4), dpi=150)\n\n    for i, (coef_name, coef_value) in enumerate(model_to_plot.state_dict().items()):\n        arr = coef_value.view(-1,).to(\"cpu\").numpy()\n        axes[i].hist(arr, bins=40)\n        axes[i].set_title(f\"{coef_name} (K={len(arr)})\")\n</code></pre> <pre><code>plot_model_initial_coefficients(model)\n</code></pre> <p></p>"},{"location":"coefficient_initialization/#alternatively-you-can-initialize-to-uniform-or-zeros-using-the-weight_initialization-argument","title":"Alternatively, you can initialize to uniform or zeros using the <code>weight_initialization</code> argument.","text":"<pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10,\n    weight_initialization=\"uniform\")\n\nplot_model_initial_coefficients(model)\n</code></pre> <pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10,\n    weight_initialization=\"zero\")\n\nplot_model_initial_coefficients(model)\n</code></pre> <pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10,\n    weight_initialization=\"normal\")\n\nplot_model_initial_coefficients(model)\n</code></pre>"},{"location":"coefficient_initialization/#you-can-initialize-different-sets-of-coefficients-differently-by-passing-a-dictionary-to-weight_initialization-for-coefficients-not-in-weight_initialization-they-are-initialized-as-a-standard-normal-distribution-the-default","title":"You can initialize different sets of coefficients differently by passing a dictionary to <code>weight_initialization</code>. For coefficients not in <code>weight_initialization</code>, they are initialized as a standard normal distribution (the default).","text":"<pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10,\n    weight_initialization={'var_1': 'uniform',\n                           'var_2': 'normal',\n                           'var_3': 'zero'})  # &lt;-- 'var_4' is missing, and it's initialized using Gaussian.\n\nplot_model_initial_coefficients(model)\n</code></pre>"},{"location":"coefficient_initialization/#for-sure-the-model-complains-if-you-ask-it-to-initialize-using-something-else","title":"For sure, the model complains if you ask it to initialize using something else...","text":"<pre><code>model = torch_choice.model.ConditionalLogitModel(\n    coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n    num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n    num_items=4,\n    num_users=10,\n    weight_initialization={'var_1': 'a-non-existing-distribution',\n                           'var_2': 'normal',\n                           'var_3': 'zero'})\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n/var/folders/r3/rj0t5xcj557855yt3xr0qwnh0000gn/T/ipykernel_55114/3320609896.py in &lt;module&gt;\n----&gt; 1 model = torch_choice.model.ConditionalLogitModel(\n      2     coef_variation_dict={'var_1': 'constant', 'var_2': 'item', 'var_3': 'item-full', 'var_4': 'user'},\n      3     num_param_dict={'var_1': 300, 'var_2': 500, 'var_3': 700, 'var_4': 900},\n      4     num_items=4,\n      5     num_users=10,\n\n\n~/Development/torch-choice/torch_choice/model/conditional_logit_model.py in __init__(self, formula, dataset, coef_variation_dict, num_param_dict, num_items, num_users, regularization, regularization_weight, weight_initialization)\n    190                 init = self.weight_initialization\n    191 \n--&gt; 192             coef_dict[var_type] = Coefficient(variation=variation,\n    193                                               num_items=self.num_items,\n    194                                               num_users=self.num_users,\n\n\n~/Development/torch-choice/torch_choice/model/coefficient.py in __init__(self, variation, num_params, num_items, num_users, init)\n     57             init_func = str_to_init_func[self.init]  # retrieve the initialization function.\n     58         else:\n---&gt; 59             raise ValueError(f\"Unsupported initialization method: {self.init}, supported methods are {list(str_to_init_func.keys())}\")\n     60 \n     61         # construct the trainable.\n\n\nValueError: Unsupported initialization method: a-non-existing-distribution, supported methods are ['zero', 'uniform', 'normal', None]\n</code></pre>"},{"location":"coefficient_initialization/#you-can-inspect-the-method-of-initialization-in-the-string-representation-of-model-coefficients-eg-initializationnormal","title":"You can inspect the method of initialization in the string representation of model coefficients (e.g., <code>initialization=normal</code>).","text":"<pre><code>model\n</code></pre> <pre><code>ConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (var_1[constant]): Coefficient(variation=constant, num_items=4, num_users=10, num_params=300, 300 trainable parameters in total, initialization=uniform, device=cpu).\n    (var_2[item]): Coefficient(variation=item, num_items=4, num_users=10, num_params=500, 1500 trainable parameters in total, initialization=normal, device=cpu).\n    (var_3[item-full]): Coefficient(variation=item-full, num_items=4, num_users=10, num_params=700, 2800 trainable parameters in total, initialization=zero, device=cpu).\n    (var_4[user]): Coefficient(variation=user, num_items=4, num_users=10, num_params=900, 9000 trainable parameters in total, initialization=normal, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[var_1[constant]] with 300 parameters, with constant level variation.\nX[var_2[item]] with 500 parameters, with item level variation.\nX[var_3[item-full]] with 700 parameters, with item-full level variation.\nX[var_4[user]] with 900 parameters, with user level variation.\ndevice=cpu\n</code></pre>"},{"location":"coefficient_initialization/#nested-logit-model","title":"Nested Logit Model","text":"<p>Initializing nested logit models is very similar to initializing conditional logit models. The only difference is you need to pass-in two arguments: <code>nest_weight_initialization</code> and <code>item_weight_initialization</code>. By default, every coefficient is initialized to a standard Gaussian distribution. The coefficient for inclusive values \\(\\lambda\\) has its own way of initialization and cannot be modified.</p> <pre><code>model = torch_choice.model.NestedLogitModel(\n    nest_to_item={1: [0, 1, 2], 2: [3, 4], 3: [5, 6, 7]},\n    #\n    nest_coef_variation_dict={'var_1': 'constant', 'var_2': 'item'},\n    nest_num_param_dict={'var_1': 300, 'var_2': 500},\n    #\n    item_coef_variation_dict={'var_3': 'item-full', 'var_4': 'user'},\n    item_num_param_dict={'var_3': 700, 'var_4': 900},\n    num_users=100,\n    # \n    nest_weight_initialization={'var_1': 'uniform', 'var_2': 'zero'},\n    item_weight_initialization={'var_4': 'uniform'}   # &lt;-- var_3 is missing, it is initialized to Gaussian by default.\n)\n</code></pre> <pre><code>def plot_model_initial_coefficients(model_to_plot: torch.nn.Module) -&gt; None:\n    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25, 4), dpi=150)\n\n    for i, (coef_name, coef_value) in enumerate(model_to_plot.state_dict().items()):\n        arr = coef_value.view(-1,).to(\"cpu\").numpy()\n        axes[i].hist(arr, bins=40)\n        axes[i].set_title(f\"{coef_name} (K={len(arr)})\")\n</code></pre> <pre><code>plot_model_initial_coefficients(model)\n</code></pre> <p></p>"},{"location":"conditional_logit_model_mode_canada/","title":"Tutorial: Conditional Logit Model on ModeCanada Dataset","text":"<p>Author: Tianyu Du (tianyudu@stanford.edu)</p> <p>Update: May. 3, 2022</p> <p>Reference: This tutorial is modified from the Random utility model and the multinomial logit model in th documentation of <code>mlogit</code> package in R.</p> <p>Please note that the dataset involved in this example is fairly small (2,779 choice records), so we don't expect the performance to be faster than the R implementation.</p> <p>We provide this tutorial mainly to check the correctness of our prediction. The fully potential of PyTorch is better exploited on much larger dataset.</p> <p>The executable Jupyter notebook for this tutorial is located at Random Utility Model (RUM) 1: Conditional Logit Model.</p> <p>Let's first import essential Python packages.</p> <pre><code>from time import time\nimport pandas as pd\nimport torch\n\nfrom torch_choice.data import ChoiceDataset, utils\nfrom torch_choice.model import ConditionalLogitModel\n\nfrom torch_choice import run\n</code></pre> <pre><code>/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: &lt;B3E58761-2785-34C6-A89B-F37110C88A05&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;AE6DCE26-A528-35ED-BB3D-88890D27E6B9&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n  warn(f\"Failed to load image Python extension: {e}\")\n</code></pre> <p>This tutorial will run both with and without graphic processing unit (GPU). However, our package is much faster with GPU.</p> <pre><code>if torch.cuda.is_available():\n    print(f'CUDA device used: {torch.cuda.get_device_name()}')\n    device = 'cuda'\nelse:\n    print('Running tutorial on CPU.')\n    device = 'cpu'\n</code></pre> <pre><code>Running tutorial on CPU.\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#load-dataset","title":"Load Dataset","text":"<p>We have included the <code>ModeCanada</code> dataset in our package, which is located at <code>./public_datasets/</code>.</p> <p>The <code>ModeCanada</code> dataset contains individuals' choice on traveling methods.</p> <p>The raw dataset is in a long-format, in which the <code>case</code> variable identifies each choice. Using the terminology mentioned in the data management tutorial, each choice is called a purchasing record (i.e., consumer bought the ticket of a particular travelling mode), and the total number of choices made is denoted as \\(B\\).</p> <p>For example, the first four row below (with <code>case == 109</code>) corresponds to the first choice, the <code>alt</code> column lists all alternatives/items available.</p> <p>The <code>choice</code> column identifies which alternative/item is chosen. The second row in the data snapshot below, we have <code>choice == 1</code> and <code>alt == 'air'</code> for <code>case == 109</code>. This indicates the travelling mode chosen in <code>case = 109</code> was <code>air</code>.</p> <p>Now we convert the raw dataset into the format compatible with our model, for a detailed tutorial on the compatible formats, please refer to the data management tutorial.</p> <p>We focus on cases when four alternatives were available by filtering <code>noalt == 4</code>.</p> <pre><code>df = pd.read_csv('./public_datasets/ModeCanada.csv')\ndf = df.query('noalt == 4').reset_index(drop=True)\ndf.sort_values(by='case', inplace=True)\ndf.head()\n</code></pre> Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 304 109 train 0 377 58.25 215 74 4 45 0 4 1 305 109 air 1 377 142.80 56 85 9 45 0 4 2 306 109 bus 0 377 27.52 301 63 8 45 0 4 3 307 109 car 0 377 71.63 262 0 0 45 0 4 4 308 110 train 0 377 58.25 215 74 4 70 0 4 <p>Since there are 4 rows corresponding to each purchasing record, the length of the long-format data is \\(4 \\times B\\). Please refer to the data management tutorial for notations.</p> <pre><code>df.shape\n</code></pre> <pre><code>(11116, 12)\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#construct-the-item_index-tensor","title":"Construct the <code>item_index</code> tensor","text":"<p>The first thing is to construct the <code>item_index</code> tensor identifying which item (i.e., travel mode) was chosen in each purchasing record.</p> <p>We can now construct the <code>item_index</code> array containing which item was chosen in each purchasing record.</p> <pre><code>item_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\nprint(item_index)\n</code></pre> <pre><code>0       air\n1       air\n2       air\n3       air\n4       air\n       ... \n2774    car\n2775    car\n2776    car\n2777    car\n2778    car\nName: alt, Length: 2779, dtype: object\n</code></pre> <p>Since we will be training our model using <code>PyTorch</code>, we need to encode <code>{'air', 'bus', 'car', 'train'}</code> into integer values.</p> Travel Mode Name Encoded Integer Values air 0 bus 1 car 2 train 3 <p>The generated <code>item_index</code> would be a tensor of shape 2,778 (i.e., number of purchasing records in this dataset) with values <code>{0, 1, 2, 3}</code>.</p> <pre><code>item_names = ['air', 'bus', 'car', 'train']\nnum_items = 4\nencoder = dict(zip(item_names, range(num_items)))\nprint(f\"{encoder=:}\")\nitem_index = item_index.map(lambda x: encoder[x])\nitem_index = torch.LongTensor(item_index)\nprint(f\"{item_index=:}\")\n</code></pre> <pre><code>encoder={'air': 0, 'bus': 1, 'car': 2, 'train': 3}\nitem_index=tensor([0, 0, 0,  ..., 2, 2, 2])\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#construct-observables","title":"Construct Observables","text":"<p>Then let's constrct tensors for observables. As mentioned in the data management tutorial, the session is capturing the temporal dimension of our data. Since we have different values <code>cost</code>, <code>freq</code> and <code>ovt</code> for each purchasing record and for each item, it's natural to say each purchasing record has its own session.</p> <p>Consequently, these three variables are <code>price</code> observables since they vary by both item and session. The tensor holding these observables has shape \\((\\text{numer of purchasing records}, \\text{number of items}, 3)\\)</p> <p>We do the same for variable <code>ivt</code>, we put <code>ivt</code> into a separate tensor because we want to model its coefficient differently later.</p> <pre><code>price_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt',\n                                    values=['cost', 'freq', 'ovt'])\nprint(f'{price_cost_freq_ovt.shape=:}')\n\nprice_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\nprint(f'{price_ivt.shape=:}')\n</code></pre> <pre><code>price_cost_freq_ovt.shape=torch.Size([2779, 4, 3])\nprice_ivt.shape=torch.Size([2779, 4, 1])\n</code></pre> <p>In contrast, the <code>income</code> variable varies only by session (i.e., purchasing record), but not by item. <code>income</code> is therefore naturally a <code>session</code> variable.</p> <pre><code>session_income = df.groupby('case')['income'].first()\nsession_income = torch.Tensor(session_income.values).view(-1, 1)\nprint(f'{session_income.shape=:}')\n</code></pre> <pre><code>session_income.shape=torch.Size([2779, 1])\n</code></pre> <p>To summarize, the <code>ChoiceDataset</code> constructed contains 2779 choice records. Since the original dataset did not reveal the identity of each decision maker, we consider all 2779 choices were made by a single user but in 2779 different sessions to handle variations.</p> <p>In this case, the <code>cost</code>, <code>freq</code> and <code>ovt</code> are observables depending on both sessions and items, we created a <code>price_cost_freq_ovt</code> tensor with shape <code>(num_sessions, num_items, 3) = (2779, 4, 3)</code> to contain these variables. In contrast, the <code>income</code> information depends only on session but not on items, hence we create the <code>session_income</code> tensor to store it.</p> <p>Because we wish to fit item-specific coefficients for the <code>ivt</code> variable, which varies by both sessions and items as well, we create another <code>price_ivt</code> tensor in addition to the <code>price_cost_freq_ovt</code> tensor.</p> <p>Lastly, we put all tensors we created to a single <code>ChoiceDataset</code> object, and move the dataset to the appropriate device.</p> <pre><code>dataset = ChoiceDataset(item_index=item_index,\n                        price_cost_freq_ovt=price_cost_freq_ovt,\n                        session_income=session_income,\n                        price_ivt=price_ivt\n                        ).to(device)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n</code></pre> <p>You can <code>print(dataset)</code> to check shapes of tensors contained in the <code>ChoiceDataset</code>.</p> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#create-the-model","title":"Create the Model","text":"<p>We now construct the <code>ConditionalLogitModel</code> to fit the dataset we constructed above.</p> <p>To start with, we aim to estimate the following model formulation:</p> \\[ U_{uit} = \\beta^0_i + \\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{price:ivt} + \\epsilon_{uit} \\] <p>We now initialize the <code>ConditionalLogitModel</code> to predict choices from the dataset. Please see the documentation for a complete description of the <code>ConditionalLogitModel</code> class.</p> <p>At it's core, the <code>ConditionalLogitModel</code> constructor requires the following four components.</p>"},{"location":"conditional_logit_model_mode_canada/#define-variation-of-each-beta-using-coef_variation_dict","title":"Define variation of each \\(\\beta\\) using <code>coef_variation_dict</code>","text":"<p>The keyword <code>coef_variation_dict</code> is a dictionary with variable names (defined above while constructing the dataset) as keys and values from <code>{constant, user, item, item-full}</code>.</p> <p>For instance, since we wish to have constant coefficients for <code>cost</code>, <code>freq</code> and <code>ovt</code> observables, and these three observables are stored in the <code>price_cost_freq_ovt</code> tensor of the choice dataset, we set <code>coef_variation_dict['price_cost_freq_ovt'] = 'constant'</code> (corresponding to the \\(\\beta^{1\\top} X^{price: (cost, freq, ovt)}_{it}\\) term above).</p> <p>The models allows for the option of zeroing coefficient for one item. The variation of \\(\\beta^3\\) above is specified as <code>item-full</code> which indicates 4 values of \\(\\beta^3\\) is learned (one for each item). In contrast, \\(\\beta^0, \\beta^2\\) are specified to have variation <code>item</code> instead of <code>item-full</code>. In this case, the \\(\\beta\\) correspond to the first item (i.e., the baseline item, which is encoded as 0 in the label tensor, <code>air</code> in our example) is force to be zero.</p> <p>The researcher needs to declare <code>intercept</code> explicitly for the model to fit an intercept as well, otherwise the model assumes zero intercept term.</p>"},{"location":"conditional_logit_model_mode_canada/#define-the-dimension-of-each-beta-using-num_param_dict","title":"Define the dimension of each \\(\\beta\\) using <code>num_param_dict</code>","text":"<p>The <code>num_param_dict</code> is a dictionary with keys exactly the same as the <code>coef_variation_dict</code>. Each of dictionary values tells the dimension of the corresponding observables, hence the dimension of the coefficient. For example, the <code>price_cost_freq_ovt</code> consists of three observables and we set the corresponding to three.</p> <p>Even the model can infer <code>num_param_dict['intercept'] = 1</code>, but we recommend the research to include it for completeness.</p>"},{"location":"conditional_logit_model_mode_canada/#number-of-items","title":"Number of items","text":"<p>The <code>num_items</code> keyword informs the model how many alternatives users are choosing from.</p>"},{"location":"conditional_logit_model_mode_canada/#number-of-users","title":"Number of users","text":"<p>The <code>num_users</code> keyword is an optional integer informing the model how many users there are in the dataset. However, in this example we implicitly assume there is only one user making all the decisions and we do not have any <code>user_obs</code> involved, hence <code>num_users</code> argument is not supplied.</p> <pre><code>model = ConditionalLogitModel(coef_variation_dict={'price_cost_freq_ovt': 'constant',\n                                                   'session_income': 'item',\n                                                   'price_ivt': 'item-full',\n                                                   'intercept': 'item'},\n                              num_param_dict={'price_cost_freq_ovt': 3,\n                                              'session_income': 1,\n                                              'price_ivt': 1,\n                                              'intercept': 1},\n                              num_items=4)\n</code></pre> <p>Then we move the model to the appropriate device.</p> <pre><code>model = model.to(device)\n</code></pre> <p>One can print the <code>ConditionalLogitModel</code> object to obtain a summary of the model.</p> <pre><code>print(model)\n</code></pre> <pre><code>ConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[price_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#creating-model-using-formula","title":"Creating Model using Formula","text":"<p>Alternatively, researchers can create the model using a <code>formula</code> like in R.</p> <p>The formula consists of a list of additive terms separated by <code>+</code> sign, and each term looks like <code>(variable_name|variation)</code>. Where <code>variable_name</code> is the name of the variable in the dataset, and <code>variation</code> is one of <code>constant</code>, <code>user</code>, <code>item</code>, <code>item-full</code>. Initializing the model using <code>formula</code> requires you to pass in the <code>ChoiceDataset</code> object as well so that the model can infer the dimension of each variable.</p> <p>These two ways of creating models lead to equivalent models.</p> <pre><code>model = model = ConditionalLogitModel(\n    formula='(price_cost_freq_ovt|constant) + (session_income|item) + (price_ivt|item-full) + (intercept|item)',\n    dataset=dataset,\n    num_items=4)\n</code></pre> <pre><code>print(model)\n</code></pre> <pre><code>ConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[price_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#train-the-model","title":"Train the Model","text":"<p>We provide an easy-to-use helper function <code>run()</code> imported from <code>torch_choice.utils.run_helper</code> to fit the model with a particular dataset.</p> <p>We provide an easy-to-use model runner for both <code>ConditionalLogitModel</code> and <code>NestedLogitModel</code> (see later) instances.</p> <p>The <code>run()</code> mehtod supports mini-batch updating as well, for small datasets like the one we are dealing right now, we can use <code>batch_size = -1</code> to conduct full-batch gradient update.</p> <p>Here we use the LBFGS optimizer since we are working on a small dataset with only 2,779 choice records and 13 coefficients to be estimated. For larger datasets and larger models, we recommend using the Adam optimizer instead.</p> <pre><code>start_time = time()\nrun(model, dataset, num_epochs=500, learning_rate=0.01, model_optimizer=\"LBFGS\", batch_size=-1)\nprint('Time taken:', time() - start_time)\n</code></pre> <pre><code>==================== model received ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[price_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n==================== data set received ====================\n[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)\n[Validation dataset] None\n[Test dataset] None\n\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n  rank_zero_warn(\n\n  | Name  | Type                  | Params\n------------------------------------------------\n0 | model | ConditionalLogitModel | 13    \n------------------------------------------------\n13        Trainable params\n0         Non-trainable params\n13        Total params\n0.000     Total estimated model params size (MB)\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n\n\nEpoch 499: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 76.72it/s, loss=1.87e+03, v_num=32]\n\n`Trainer.fit` stopped: `max_epochs=500` reached.\n\n\nEpoch 499: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 69.67it/s, loss=1.87e+03, v_num=32]\nTime taken for training: 11.521849155426025\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -1874.3427734375, [Validation] N/A, [Test] N/A\n\n| Coefficient                     |   Estimation |   Std. Err. |    z-value |    Pr(&gt;|z|) | Significance   |\n|:--------------------------------|-------------:|------------:|-----------:|------------:|:---------------|\n| price_cost_freq_ovt[constant]_0 |  -0.0333421  |  0.00709556 |  -4.69901  | 2.61425e-06 | ***            |\n| price_cost_freq_ovt[constant]_1 |   0.0925304  |  0.00509757 |  18.1518   | 0           | ***            |\n| price_cost_freq_ovt[constant]_2 |  -0.0430032  |  0.00322472 | -13.3355   | 0           | ***            |\n| session_income[item]_0          |  -0.0890796  |  0.0183469  |  -4.8553   | 1.20205e-06 | ***            |\n| session_income[item]_1          |  -0.0279925  |  0.00387254 |  -7.22846  | 4.88498e-13 | ***            |\n| session_income[item]_2          |  -0.038146   |  0.00408307 |  -9.34248  | 0           | ***            |\n| price_ivt[item-full]_0          |   0.0595089  |  0.0100727  |   5.90794  | 3.46418e-09 | ***            |\n| price_ivt[item-full]_1          |  -0.00678188 |  0.00443289 |  -1.5299   | 0.126042    |                |\n| price_ivt[item-full]_2          |  -0.00645982 |  0.00189848 |  -3.40262  | 0.000667424 | ***            |\n| price_ivt[item-full]_3          |  -0.00145029 |  0.00118748 |  -1.22132  | 0.221965    |                |\n| intercept[item]_0               |   0.697311   |  1.28022    |   0.544681 | 0.585973    |                |\n| intercept[item]_1               |   1.8437     |  0.708514   |   2.6022   | 0.0092627   | **             |\n| intercept[item]_2               |   3.27381    |  0.624416   |   5.24299  | 1.57999e-07 | ***            |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTime taken: 11.617464065551758\n</code></pre>"},{"location":"conditional_logit_model_mode_canada/#parameter-estimation-from-r","title":"Parameter Estimation from <code>R</code>","text":"<p>The following is the R-output from the <code>mlogit</code> implementation, the estimation, standard error, and log-likelihood from our <code>torch_choice</code> implementation is the same as the result from <code>mlogit</code> implementation.</p> <p>We see that the final log-likelihood of models estimated using two packages are all around <code>-1874</code>.</p> <p>The <code>run()</code> method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\), where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters.</p> <p>Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages.</p>"},{"location":"conditional_logit_model_mode_canada/#r-output","title":"R Output","text":"<p><pre><code>install.packages(\"mlogit\")\nlibrary(\"mlogit\")\ndata(\"ModeCanada\", package = \"mlogit\")\nMC &lt;- dfidx(ModeCanada, subset = noalt == 4)\nml.MC1 &lt;- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air')\n\nsummary(ml.MC1)\n</code></pre> <pre><code>Call:\nmlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, \n    reflevel = \"air\", method = \"nr\")\n\nFrequencies of alternatives:choice\n      air     train       bus       car \n0.3738755 0.1666067 0.0035984 0.4559194 \n\nnr method\n9 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00014 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                    Estimate Std. Error  z-value  Pr(&gt;|z|)    \n(Intercept):train  3.2741952  0.6244152   5.2436 1.575e-07 ***\n(Intercept):bus    0.6983381  1.2802466   0.5455 0.5854292    \n(Intercept):car    1.8441129  0.7085089   2.6028 0.0092464 ** \ncost              -0.0333389  0.0070955  -4.6986 2.620e-06 ***\nfreq               0.0925297  0.0050976  18.1517 &lt; 2.2e-16 ***\novt               -0.0430036  0.0032247 -13.3356 &lt; 2.2e-16 ***\nincome:train      -0.0381466  0.0040831  -9.3426 &lt; 2.2e-16 ***\nincome:bus        -0.0890867  0.0183471  -4.8556 1.200e-06 ***\nincome:car        -0.0279930  0.0038726  -7.2286 4.881e-13 ***\nivt:air            0.0595097  0.0100727   5.9080 3.463e-09 ***\nivt:train         -0.0014504  0.0011875  -1.2214 0.2219430    \nivt:bus           -0.0067835  0.0044334  -1.5301 0.1259938    \nivt:car           -0.0064603  0.0018985  -3.4029 0.0006668 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nLog-Likelihood: -1874.3\nMcFadden R^2:  0.35443 \nLikelihood ratio test : chisq = 2058.1 (p.value = &lt; 2.22e-16)\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"data_management/","title":"Tutorial: Data Management","text":"<p>Author: Tianyu Du (tianyudu@stanford.edu)</p> <p>Note: please go through the introduction tutorial here before proceeding.</p> <p>This notebook aims to help users understand the functionality of <code>ChoiceDataset</code> object. The <code>ChoiceDataset</code> is an instance of the more general PyTorch dataset object holding information of consumer choices. The <code>ChoiceDataset</code> offers easy, clean and efficient data management. The Jupyter-notebook version of this tutorial can be found here.</p> <p>This tutorial provides in-depth explanations on how the <code>torch-choice</code> library manages data. We are also providing an easy-to-use data wrapper converting long-format dataset to <code>ChoiceDataset</code> here, you can harness the <code>torch-choice</code> library without going through this tutorial. </p> <p>Note: since this package was initially proposed for modelling consumer choices, attribute names of <code>ChoiceDataset</code> are borrowed from the consumer choice literature.</p> <p>Note: PyTorch uses the term tensor to denote high dimensional matrices, we will be using tensor and matrix interchangeably.</p> <p>After walking through this tutorial, you should be abel to initiate a <code>ChoiceDataset</code> object as the following and use it to manage data. <pre><code>dataset = ChoiceDataset(\n    # pre-specified keywords of __init__\n    item_index=item_index,  # required.\n    # optional:\n    user_index=user_index,\n    session_index=session_index,\n    item_availability=item_availability,\n    # additional keywords of __init__\n    user_obs=user_obs,\n    item_obs=item_obs,\n    session_obs=session_obs,\n    price_obs=price_obs)\n</code></pre></p>"},{"location":"data_management/#observables","title":"Observables","text":"<p>Observables are tensors with specific shapes, we classify observables into four categories based on their variations.</p>"},{"location":"data_management/#basic-usage","title":"Basic Usage","text":"<p>Optionally, the researcher can incorporate observables of, for example, users and items. Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables.</p> <ol> <li><code>user_obs</code> \\(\\in \\mathbb{R}^{U\\times K_{user}}\\): user observables such as user age.</li> <li><code>item_obs</code> \\(\\in \\mathbb{R}^{I\\times K_{item}}\\): item observables such as item quality.</li> <li><code>session_obs</code> \\(\\in \\mathbb{R}^{S \\times K_{session}}\\): session observable such as whether the purchase was made on weekdays.</li> <li><code>price_obs</code> \\(\\in \\mathbb{R}^{S \\times I \\times K_{price}}\\), price observables are values depending on both session and item such as the price of item.</li> </ol> <p>The researcher should supply them with as appropriate keyword arguments while constructing the <code>ChoiceDataset</code> object.</p>"},{"location":"data_management/#optional-advanced-usage-additional-observables","title":"(Optional) Advanced Usage: Additional Observables","text":"<p>In some cases, the researcher have multiple sets of user (or item, or session, or price) observables, say user income (a scalar variable) and user market membership. The user income a matrix in \\(\\mathbb{R}^{U\\times 1}\\). Further, suppose there are four types of market membership: no-membership, silver-membership, gold-membership, and diamond-membership. The user market membership is a binary matrix in \\(\\{0, 1\\}^{U\\times 4}\\) if we one-hot encode users' membership status.</p> <p>In this case, the researcher can either 1. concatenate <code>user_income</code> and <code>user_market_membership</code> to a \\(\\mathbb{R}^{U\\times (1+4)}\\) matrix and supply it as a single <code>user_obs</code> as the following: <pre><code>dataset = ChoiceDataset(..., user_obs=torch.cat([user_income, user_market_membership], dim=1), ...)\n</code></pre> 2. Or, supply these two sets of observables separately, namely a <code>user_income</code> \\(\\in \\mathbb{R}^{U \\times 1}\\) matrix and a <code>user_market_membership</code> \\(\\in \\mathbb{R}^{U \\times 4}\\) matrix as the following: <pre><code>dataset = ChoiceDataset(..., user_income=user_income, user_market_membership=user_market_membership, ...)\n</code></pre></p> <p>Supplying two separate sets of observables is particularly useful when the researcher wants different kinds of coefficients for different kinds of observables.</p> <p>For example, the researcher wishes to model the utility for user \\(u\\) to purchase item \\(i\\) in session \\(s\\) as the following:</p> \\[ U_{usi} = \\beta_{i} X^{(u)}_{user\\ income} + \\gamma X^{(u)}_{user\\ market\\ membership} + \\varepsilon \\] <p>Please note that the \\(\\beta_i\\) coefficient has an \\(i\\) subscript, which means it's item specific. The \\(\\gamma\\) coefficient has no subscript, which means it's the same for all items.</p> <p>The coefficient for user income is item-specific so that it captures the nature of the product (i.e., a luxury or an essential good). Additionally, the utility representation admits an user market membership becomes shoppers with active memberships tend to purchase more, and the coefficient of this term is constant across all items.</p> <p>As we will cover later in the modelling section, we need to supply two user observable tensors in this case for the model to build coefficient with different levels of variations (i.e., item-specific coefficients versus constant coefficients). In this case, the researcher needs to supply two tensors <code>user_income</code> and <code>user_market_membership</code> as keyword arguments to the <code>ChoiceDataset</code> constructor.</p> <p>Generally, the <code>ChoiceDataset</code> handles multiple user/item/session/price observables internally, the <code>ChoiceDataset</code> class identifies the variation of observables by their prefixes. For example, every keyword arguments passed into <code>ChoiceDataset</code> with name starting with <code>item_</code> (except for the reserved <code>item_availability</code>) will be treated as item observable tensors. Similarly, all keywords with names starting <code>user_</code>, <code>session_</code> and <code>price_</code> (except for reserved names like <code>user_index</code> and <code>session_index</code> mentioned above) will be interpreted as user/session/price observable tensors.</p> <pre><code># import required dependencies.\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch_choice.data import ChoiceDataset, JointDataset\n</code></pre> <pre><code># let's get a helper\ndef print_dict_shape(d):\n    for key, val in d.items():\n        if torch.is_tensor(val):\n            print(f'dict.{key}.shape={val.shape}')\n</code></pre>"},{"location":"data_management/#creating-choicedataset-object","title":"Creating  <code>ChoiceDataset</code> Object","text":"<pre><code># Feel free to modify it as you want.\nnum_users = 10\nnum_items = 4\nnum_sessions = 500\n\nlength_of_dataset = 10000\n</code></pre>"},{"location":"data_management/#step-1-generate-some-random-purchase-records-and-observables","title":"Step 1: Generate some random purchase records and observables","text":"<p>We will be creating a randomly generated dataset with 10000 purchase records from 10 users, 4 items and 500 sessions.</p> <p>We use the term purchase record to denote the observation in the dataset due to the convention in Stata documentation (because observation meant something else in the Stata documentation and we don't want to confuse existing Stata users).</p> <p>As mentioned in the introduction tutorial, one purchase record consists of who (i.e., user) bought what (i.e., item) when and where (i.e., session). </p> <p>The length of the dataset equals the number of purchase records in it.</p> <p>The first step is to randomly generate the purchase records using the following code. For simplicity, we assume all items are available in all sessions.</p> <pre><code># create observables/features, the number of parameters are arbitrarily chosen.\n# generate 128 features for each user, e.g., race, gender.\nuser_obs = torch.randn(num_users, 128)\n# generate 64 features for each user, e.g., quality.\nitem_obs = torch.randn(num_items, 64)\n# generate 10 features for each session, e.g., weekday indicator. \nsession_obs = torch.randn(num_sessions, 10)\n# generate 12 features for each session user pair, e.g., the budget of that user at the shopping day.\nprice_obs = torch.randn(num_sessions, num_items, 12)\n</code></pre> <p>We then generate random observable tensors for users, items, sessions and price observables, the size of observables of each type (i.e., the last dimension in the shape) is arbitrarily chosen.</p> <p>Notes on Encodings Since we will be using PyTorch to train our model, we represent their identities with consecutive integer values instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor). Similarly, you would need to encode user indices and session indices as well. Raw item names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well).</p> <pre><code>item_index = torch.LongTensor(np.random.choice(num_items, size=length_of_dataset))\nuser_index = torch.LongTensor(np.random.choice(num_users, size=length_of_dataset))\nsession_index = torch.LongTensor(np.random.choice(num_sessions, size=length_of_dataset))\n\n# assume all items are available in all sessions.\nitem_availability = torch.ones(num_sessions, num_items).bool()\n</code></pre>"},{"location":"data_management/#step-2-initialize-the-choicedataset","title":"Step 2: Initialize the <code>ChoiceDataset</code>.","text":"<p>You can construct a choice set using the following code, which manage all information for you.</p> <pre><code>dataset = ChoiceDataset(\n    # pre-specified keywords of __init__\n    item_index=item_index,  # required.\n    # optional:\n    user_index=user_index,\n    session_index=session_index,\n    item_availability=item_availability,\n    # additional keywords of __init__\n    user_obs=user_obs,\n    item_obs=item_obs,\n    session_obs=session_obs,\n    price_obs=price_obs)\n</code></pre>"},{"location":"data_management/#what-you-can-do-with-the-choicedataset","title":"What you can do with the <code>ChoiceDataset</code>?","text":""},{"location":"data_management/#printdataset-and-dataset__str__","title":"<code>print(dataset)</code> and <code>dataset.__str__</code>","text":"<p>The command <code>print(dataset)</code> will provide a quick overview of shapes of tensors included in the object as well as where the dataset is located (i.e., host memory or GPU memory).</p> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\n</code></pre>"},{"location":"data_management/#datasetsummary","title":"<code>dataset.summary()</code>","text":"<p>The <code>summary</code> method provides preliminary summarization of the dataset.</p> <pre><code>print(pd.DataFrame(dataset.user_index).value_counts())\n</code></pre> <pre><code>4    1038\n8    1035\n5    1024\n1    1010\n2     997\n0     990\n6     981\n9     980\n3     974\n7     971\ndtype: int64\n</code></pre> <pre><code>print(pd.DataFrame(dataset.item_index).value_counts())\n</code></pre> <pre><code>0    2575\n1    2539\n2    2467\n3    2419\ndtype: int64\n</code></pre> <pre><code>dataset.summary()\n</code></pre> <pre><code>ChoiceDataset with 500 sessions, 4 items, 10 users, 10000 purchase records (observations) .\nThe most frequent user is 4 with 1038 observations; the least frequent user is 7 with 971 observations; on average, there are 1000.00 observations per user.\n5 most frequent users are: 4(1038 times), 8(1035 times), 5(1024 times), 1(1010 times), 2(997 times).\n5 least frequent users are: 7(971 times), 3(974 times), 9(980 times), 6(981 times), 0(990 times).\nThe most frequent item is 0, it was chosen 2575 times; the least frequent item is 3 it was 2419 times; on average, each item was purchased 2500.00 times.\n4 most frequent items are: 0(2575 times), 1(2539 times), 2(2467 times), 3(2419 times).\n4 least frequent items are: 3(2419 times), 2(2467 times), 1(2539 times), 0(2575 times).\nAttribute Summaries:\nObservable Tensor 'user_obs' with shape torch.Size([10, 128])\n             0          1          2          3          4          5    \\\ncount  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \nmean    0.687878  -0.339077  -0.375829   0.086242   0.250604  -0.344643   \nstd     0.738520   1.259936   0.844018   0.766233   0.802785   0.645239   \nmin    -0.578577  -2.135251  -1.335928  -0.911508  -1.396776  -1.519729   \n25%     0.264708  -0.889820  -0.845100  -0.414891  -0.132619  -0.699887   \n50%     0.902505  -0.603065  -0.638757  -0.289223   0.297693  -0.405371   \n75%     1.155211   0.021188  -0.190907   0.712183   0.768554   0.117107   \nmax     1.623162   2.217712   1.624211   1.252059   1.273116   0.571998\n\n             6          7          8          9    ...        118        119  \\\ncount  10.000000  10.000000  10.000000  10.000000  ...  10.000000  10.000000   \nmean    0.423672   0.325855   0.258114  -0.199072  ...  -0.165618  -0.378175   \nstd     1.304160   0.815934   0.938925   1.344848  ...   1.135625   0.940863   \nmin    -1.440672  -1.068176  -1.280547  -2.819688  ...  -1.567793  -1.604171   \n25%    -0.535055   0.051598  -0.178302  -0.801871  ...  -1.114392  -1.066492   \n50%     0.502826   0.369002   0.230939  -0.576039  ...  -0.114789  -0.587483   \n75%     1.227700   0.899518   0.740881   0.820789  ...   0.602045   0.160254   \nmax     2.462891   1.440098   1.828760   1.866570  ...   1.854828   1.386001\n\n             120        121        122        123        124        125  \\\ncount  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \nmean   -0.557321   0.402392  -0.070746  -0.770201   0.594842   0.572671   \nstd     1.128886   0.899030   0.757537   1.044478   0.956856   0.883374   \nmin    -3.131332  -0.907885  -1.296398  -2.159384  -1.244177  -0.462607   \n25%    -0.834223  -0.059528  -0.222124  -1.332558   0.234198  -0.008799   \n50%    -0.613761   0.117478  -0.109676  -0.984450   0.656855   0.466357   \n75%     0.040239   1.136383   0.416972  -0.285216   1.246513   0.772441   \nmax     1.087999   1.757588   1.022053   1.486507   2.010775   2.162550\n\n             126        127  \ncount  10.000000  10.000000  \nmean    0.226993  -0.064205  \nstd     1.463179   0.602277  \nmin    -1.731004  -0.865115  \n25%    -0.951169  -0.418553  \n50%     0.174763  -0.112277  \n75%     0.773072   0.353951  \nmax     2.991696   0.804881\n\n[8 rows x 128 columns]\nObservable Tensor 'item_obs' with shape torch.Size([4, 64])\n             0         1         2         3         4         5         6   \\\ncount  4.000000  4.000000  4.000000  4.000000  4.000000  4.000000  4.000000   \nmean   0.287015 -0.180256 -0.239000  0.169168  0.159036  0.385342 -1.142672   \nstd    1.339318  1.603530  0.722772  0.473407  0.392562  1.327739  0.566069   \nmin   -1.138152 -2.212473 -1.051363 -0.538771 -0.330795 -0.517352 -1.770297   \n25%   -0.558802 -0.990083 -0.745828  0.132031 -0.006671 -0.485835 -1.397787   \n50%    0.170810 -0.012201 -0.154058  0.385432  0.174086 -0.125969 -1.199654   \n75%    1.016628  0.797626  0.352770  0.422569  0.339793  0.745208 -0.944538   \nmax    1.944591  1.515852  0.403479  0.444577  0.618768  2.310656 -0.401083\n\n             7         8         9   ...        54        55        56  \\\ncount  4.000000  4.000000  4.000000  ...  4.000000  4.000000  4.000000   \nmean   0.581071 -0.169341  0.076562  ...  0.055457 -0.002887 -0.160406   \nstd    0.972295  0.978922  1.116274  ...  0.777132  0.903879  1.140101   \nmin   -0.596834 -1.309131 -1.563906  ... -0.481757 -0.997574 -1.721709   \n25%   -0.025344 -0.718815 -0.153971  ... -0.442894 -0.340660 -0.631280   \n50%    0.745386 -0.177989  0.514336  ... -0.240767 -0.105541  0.117918   \n75%    1.351801  0.371485  0.744870  ...  0.257583  0.232232  0.588793   \nmax    1.430348  0.987744  0.841483  ...  1.185118  1.197110  0.844249\n\n             57        58        59        60        61        62        63  \ncount  4.000000  4.000000  4.000000  4.000000  4.000000  4.000000  4.000000  \nmean   0.149579  0.199678  0.088542 -0.356379  1.004674  0.095064 -0.548665  \nstd    0.963564  0.744614  1.170228  0.833992  0.559029  0.912057  0.730697  \nmin   -0.760765 -0.419252 -1.038935 -0.989042  0.442226 -0.989018 -1.445138  \n25%   -0.268040 -0.383280 -0.604213 -0.970008  0.592259 -0.492793 -0.790356  \n50%   -0.075941  0.036190 -0.142981 -0.611959  0.966522  0.230826 -0.546745  \n75%    0.341678  0.619148  0.549774  0.001670  1.378937  0.818683 -0.305054  \nmax    1.510964  1.145585  1.679067  0.787444  1.643426  0.907622  0.343970\n\n[8 rows x 64 columns]\nObservable Tensor 'session_obs' with shape torch.Size([500, 10])\n                0           1           2           3           4           5  \\\ncount  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \nmean    -0.025211   -0.018355   -0.002907    0.091295   -0.061911   -0.046364   \nstd      0.976283    1.029875    0.959884    0.968500    1.020114    1.010222   \nmin     -2.642895   -3.091050   -3.572037   -2.406249   -3.147900   -3.357277   \n25%     -0.745162   -0.685578   -0.636044   -0.629955   -0.754234   -0.732924   \n50%     -0.018775    0.017807   -0.018642    0.112322   -0.090321   -0.070502   \n75%      0.652438    0.646001    0.601829    0.722870    0.640275    0.652521   \nmax      3.044069    3.191774    2.521059    2.695970    3.166039    2.714594\n\n                6           7           8           9  \ncount  500.000000  500.000000  500.000000  500.000000  \nmean     0.000907    0.001370    0.070499   -0.007936  \nstd      1.015561    1.032878    1.036212    0.936091  \nmin     -2.677915   -3.489751   -2.953354   -2.424499  \n25%     -0.679291   -0.671086   -0.582997   -0.681405  \n50%      0.002569   -0.009368    0.087901    0.010856  \n75%      0.703671    0.732814    0.737692    0.618773  \nmax      2.528283    3.259835    2.827300    2.492085  \nObservable Tensor 'price_obs' with shape torch.Size([500, 4, 12])\ndevice=cpu\n</code></pre>"},{"location":"data_management/#datasetnum_users-items-sessions","title":"<code>dataset.num_{users, items, sessions}</code>","text":"<p>You can use the <code>num_{users, items, sessions}</code> attribute to obtain the number of users, items, and sessions, they are determined automatically from the <code>{user, item, session}_obs</code> tensors provided while initializing the dataset object.</p> <p>Note: the print <code>=:</code> operator requires Python3.8 or higher, you can remove <code>=:</code> if you are using an earlier copy of Python.</p> <pre><code>print(f'{dataset.num_users=:}')\nprint(f'{dataset.num_items=:}')\nprint(f'{dataset.num_sessions=:}')\nprint(f'{len(dataset)=:}')\n</code></pre> <pre><code>dataset.num_users=10\ndataset.num_items=4\ndataset.num_sessions=500\nlen(dataset)=10000\n</code></pre>"},{"location":"data_management/#datasetclone","title":"<code>dataset.clone()</code>","text":"<p>The <code>ChoiceDataset</code> offers a <code>clone</code> method allow you to make copy of the dataset, you can modify the cloned dataset arbitrarily without changing the original dataset.</p> <pre><code># clone\nprint(dataset.item_index[:10])\ndataset_cloned = dataset.clone()\ndataset_cloned.item_index = 99 * torch.ones(num_sessions)\nprint(dataset_cloned.item_index[:10])\nprint(dataset.item_index[:10])  # does not change the original dataset.\n</code></pre> <pre><code>tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])\ntensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.])\ntensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])\n</code></pre>"},{"location":"data_management/#datasettocuda-and-dataset_check_device_consistency","title":"<code>dataset.to('cuda')</code> and <code>dataset._check_device_consistency()</code>.","text":"<p>One key advantage of the <code>torch_choice</code> and <code>bemb</code> is their compatibility with GPUs, you can easily move tensors in a <code>ChoiceDataset</code> object between host memory (i.e., cpu memory) and device memory (i.e., GPU memory) using <code>dataset.to()</code> method. Please note that the following code runs only if your machine has a compatible GPU and GPU-compatible version of PyTorch installed.</p> <p>Similarly, one can move data to host-memory using <code>dataset.to('cpu')</code>. The dataset also provides a <code>dataset._check_device_consistency()</code> method to check if all tensors are on the same device. If we only move the <code>label</code> to cpu without moving other tensors, this will result in an error message.</p> <pre><code># move to device\nprint(f'{dataset.device=:}')\nprint(f'{dataset.device=:}')\nprint(f'{dataset.user_index.device=:}')\nprint(f'{dataset.session_index.device=:}')\n\ndataset = dataset.to('cuda')\n\nprint(f'{dataset.device=:}')\nprint(f'{dataset.item_index.device=:}')\nprint(f'{dataset.user_index.device=:}')\nprint(f'{dataset.session_index.device=:}')\n</code></pre> <pre><code>dataset.device=cpu\ndataset.device=cpu\ndataset.user_index.device=cpu\ndataset.session_index.device=cpu\ndataset.device=cuda:0\ndataset.item_index.device=cuda:0\ndataset.user_index.device=cuda:0\ndataset.session_index.device=cuda:0\n</code></pre> <pre><code>dataset._check_device_consistency()\n</code></pre> <pre><code># # NOTE: this cell will result errors, this is intentional.\ndataset.item_index = dataset.item_index.to('cpu')\ndataset._check_device_consistency()\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nException                                 Traceback (most recent call last)\n\n&lt;ipython-input-56-40d626c6d436&gt; in &lt;module&gt;\n      1 # # NOTE: this cell will result errors, this is intentional.\n      2 dataset.item_index = dataset.item_index.to('cpu')\n----&gt; 3 dataset._check_device_consistency()\n\n\n~/Development/torch-choice/torch_choice/data/choice_dataset.py in _check_device_consistency(self)\n    180                 devices.append(val.device)\n    181         if len(set(devices)) &gt; 1:\n--&gt; 182             raise Exception(f'Found tensors on different devices: {set(devices)}.',\n    183                             'Use dataset.to() method to align devices.')\n    184\n\n\nException: (\"Found tensors on different devices: {device(type='cuda', index=0), device(type='cpu')}.\", 'Use dataset.to() method to align devices.')\n</code></pre> <pre><code># create dictionary inputs for model.forward()\n# collapse to a dictionary object.\nprint_dict_shape(dataset.x_dict)\n</code></pre> <pre><code>dict.user_obs.shape=torch.Size([10000, 4, 128])\ndict.item_obs.shape=torch.Size([10000, 4, 64])\ndict.session_obs.shape=torch.Size([10000, 4, 10])\ndict.price_obs.shape=torch.Size([10000, 4, 12])\n</code></pre>"},{"location":"data_management/#subset-method","title":"Subset method","text":"<p>One can use <code>dataset[indices]</code> with <code>indices</code> as an integer-valued tensor or array to get the corresponding rows of the dataset. The example code block below queries the 6256-th, 4119-th, 453-th, 5520-th, and 1877-th row of the dataset object. The <code>item_index</code>, <code>user_index</code>, <code>session_index</code> of the resulted subset will be different from the original dataset, but other tensors will be the same.</p> <pre><code># __getitem__ to get batch.\n# pick 5 random sessions as the mini-batch.\ndataset = dataset.to('cpu')\nindices = torch.Tensor(np.random.choice(len(dataset), size=5, replace=False)).long()\nprint(indices)\nsubset = dataset[indices]\nprint(dataset)\nprint(subset)\n# print_dict_shape(subset.x_dict)\n\n# assert torch.all(dataset.x_dict['price_obs'][indices, :, :] == subset.x_dict['price_obs'])\n# assert torch.all(dataset.item_index[indices] == subset.item_index)\n</code></pre> <pre><code>tensor([1118,  976, 1956,  290, 8283])\nChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\nChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\n</code></pre> <p>The subset method internally creates a copy of the datasets so that any modification applied on the subset will not be reflected on the original dataset. The researcher can feel free to do in-place modification to the subset.</p> <pre><code>print(subset.item_index)\nprint(dataset.item_index[indices])\n\nsubset.item_index += 1  # modifying the batch does not change the original dataset.\n\nprint(subset.item_index)\nprint(dataset.item_index[indices])\n</code></pre> <pre><code>tensor([0, 1, 0, 0, 0])\ntensor([0, 1, 0, 0, 0])\ntensor([1, 2, 1, 1, 1])\ntensor([0, 1, 0, 0, 0])\n</code></pre> <pre><code>print(subset.item_obs[0, 0])\nprint(dataset.item_obs[0, 0])\nsubset.item_obs += 1\nprint(subset.item_obs[0, 0])\nprint(dataset.item_obs[0, 0])\n</code></pre> <pre><code>tensor(-1.5811)\ntensor(-1.5811)\ntensor(-0.5811)\ntensor(-1.5811)\n</code></pre> <pre><code>print(id(subset.item_index))\nprint(id(dataset.item_index[indices]))\n</code></pre> <pre><code>140339656298640\n140339656150528\n</code></pre>"},{"location":"data_management/#using-pytorch-dataloader-for-the-training-loop","title":"Using Pytorch dataloader for the training loop.","text":"<p>The <code>ChoiceDataset</code> object natively support batch samplers from PyTorch. For demonstration purpose, we turned off the shuffling option.</p> <pre><code>from torch.utils.data.sampler import BatchSampler, SequentialSampler, RandomSampler\nshuffle = False  # for demonstration purpose.\nbatch_size = 32\n\n# Create sampler.\nsampler = BatchSampler(\n    RandomSampler(dataset) if shuffle else SequentialSampler(dataset),\n    batch_size=batch_size,\n    drop_last=False)\n\ndataloader = torch.utils.data.DataLoader(dataset,\n                                         sampler=sampler,\n                                         num_workers=1,\n                                         collate_fn=lambda x: x[0],\n                                         pin_memory=(dataset.device == 'cpu'))\n</code></pre> <pre><code>print(f'{item_obs.shape=:}')\nitem_obs_all = item_obs.view(1, num_items, -1).expand(len(dataset), -1, -1)\nitem_obs_all = item_obs_all.to(dataset.device)\nitem_index_all = item_index.to(dataset.device)\nprint(f'{item_obs_all.shape=:}')\n</code></pre> <pre><code>item_obs.shape=torch.Size([4, 64])\nitem_obs_all.shape=torch.Size([10000, 4, 64])\n</code></pre> <pre><code>for i, batch in enumerate(dataloader):\n    first, last = i * batch_size, min(len(dataset), (i + 1) * batch_size)\n    idx = torch.arange(first, last)\n    assert torch.all(item_obs_all[idx, :, :] == batch.x_dict['item_obs'])\n    assert torch.all(item_index_all[idx] == batch.item_index)\n</code></pre> <pre><code>batch.x_dict['item_obs'].shape\n</code></pre> <pre><code>torch.Size([16, 4, 64])\n</code></pre> <pre><code>print_dict_shape(dataset.x_dict)\n</code></pre> <pre><code>dict.user_obs.shape=torch.Size([10000, 4, 128])\ndict.item_obs.shape=torch.Size([10000, 4, 64])\ndict.session_obs.shape=torch.Size([10000, 4, 10])\ndict.price_obs.shape=torch.Size([10000, 4, 12])\n</code></pre> <pre><code>dataset.__len__()\n</code></pre> <pre><code>10000\n</code></pre>"},{"location":"data_management/#chaining-multiple-datasets-jointdataset-examples","title":"Chaining Multiple Datasets: <code>JointDataset</code> Examples","text":"<pre><code>dataset1 = dataset.clone()\ndataset2 = dataset.clone()\njoint_dataset = JointDataset(the_dataset=dataset1, another_dataset=dataset2)\n</code></pre> <pre><code>joint_dataset\n</code></pre> <pre><code>JointDataset with 2 sub-datasets: (\n    the_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\n    another_dataset: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\n)\n</code></pre>"},{"location":"easy_data_management/","title":"Easy Data Wrapper Tutorial","text":"<p>The data construction covered in the Data Management tutorial might be too complicated for users without prior experience in PyTorch. This tutorial offers a helper class to wrap the dataset, all the user needs to know is</p> <p>(1) loading data-frames to Python, Pandas provides one-line solution to loading various types of data files including CSV, TSV, Stata, and Excel.</p> <p>(2) basic usage of pandas. </p> <p>We aim to make this tutorial as self-contained as possible, so you don't need to be worried if you haven't went through the Data Management tutorial. But we invite you to go through that tutorial to obtain a more in-depth understanding of data management in this project.</p> <p>Author: Tianyu Du</p> <p>Date: May. 20, 2022</p> <p>Update: Jul. 9, 2022</p> <pre><code>__author__ = 'Tianyu Du'\n</code></pre> <p>Let's import a few necessary packages.</p> <pre><code>import pandas as pd\nimport torch\nfrom torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper\n</code></pre>"},{"location":"easy_data_management/#references-and-background-for-stata-users","title":"References and Background for Stata Users","text":"<p>This tutorial aim to show how to manage choice datasets using the <code>torch-choice</code> package, we will follow the Stata documentation here to offer a seamless experience for the user to transfer prior knowledge in other packages to our package.</p> <p>From Stata Documentation: Choice models (CM) are models for data with outcomes that are choices. The choices are selected by a decision maker, such as a person or a business (i.e., the user), from a set of possible alternatives (i.e., the items). For instance, we could model choices made by consumers who select a breakfast cereal from several different brands. Or we could model choices made by businesses who chose whether to buy TV, radio, Internet, or newspaper advertising.</p> <p>Models for choice data come in two varieties\u2014models for discrete choices and models for rank-ordered alternatives. When each individual selects a single alternative, say, he or she purchases one box of cereal, the data are discrete choice data. When each individual ranks the choices, say, he or she orders cereals from most favorite to least favorite, the data are rank-ordered data. Stata has commands for fitting both discrete choice models and rank-ordered models.</p> <p>Our <code>torch-choice</code> package handles the discrete choice models in the Stata document above.</p>"},{"location":"easy_data_management/#motivations","title":"Motivations","text":"<p>In the following parts, we demonstrate how to convert a long-format data (e.g., the one used in Stata) to the <code>ChoiceDataset</code> data format expected by our package.</p> <p>But first, Why do we want another <code>ChoiceDataset</code> object instead of just one long-format data-frame? In earlier versions of Stata, we can only have one single data-frame loaded in memory, this would introduce memory error especially when teh dataset is large. For example, you have a dataset of a million decisions recorded, each consists of four items, and each item has a persistent built quality that stay the same in all observations. The Stata format would make a million copy of these variables, which is very inefficient.</p> <p>We would need to collect a couple of data-frames as the essential pieces to build our <code>ChoiceDataset</code>. Don't worry, as soon as you have the data-frames ready, the <code>EasyDataWrapper</code> helper class would take care of the rest.</p> <p>We call a single statistical observation a \"purchase record\" and use this terminology throughout the tutorial. </p> <pre><code>df = pd.read_stata('https://www.stata-press.com/data/r17/carchoice.dta')\n</code></pre> <p>We load the artificial dataset from the Stata website. Here we borrow the description of dataset reported from the <code>describe</code> command in Stata. </p> <pre><code>Contains data from https://www.stata-press.com/data/r17/carchoice.dta\n Observations:         3,160                  Car choice data\n    Variables:             6                  30 Jul 2020 14:58\n---------------------------------------------------------------------------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n---------------------------------------------------------------------------------------------------------------------------------------------------\nconsumerid      int     %8.0g                 ID of individual consumer\ncar             byte    %9.0g      nation     Nationality of car\npurchase        byte    %10.0g                Indicator of car purchased\ngender          byte    %9.0g      gender     Gender: 0 = Female, 1 = Male\nincome          float   %9.0g                 Income (in $1,000)\ndealers         byte    %9.0g                 No. of dealerships in community\n---------------------------------------------------------------------------------------------------------------------------------------------------\nSorted by: consumerid  car\n</code></pre> <p>In this dataset, the first four rows with <code>consumerid == 1</code> corresponds to the first purchasing record, it means the consumer with ID 1 was making the decision among four types of cars (i.e., items) and chose <code>American</code> car (since the <code>purchase == 1</code> in that row of <code>American</code> car).</p> <p>Even though there were four types of cars, not all of them were available all the time. For example, for the purchase record by consumer with ID 4, only American, Japanese, and European cars were available (note that there is no row in the dataset with <code>consumerid == 4</code> and <code>car == 'Korean'</code>, this indicates unavailability of a certain item.)</p> <pre><code>df.head(30)\n</code></pre> consumerid car purchase gender income dealers 0 1 American 1 Male 46.699997 9 1 1 Japanese 0 Male 46.699997 11 2 1 European 0 Male 46.699997 5 3 1 Korean 0 Male 46.699997 1 4 2 American 1 Male 26.100000 10 5 2 Japanese 0 Male 26.100000 7 6 2 European 0 Male 26.100000 2 7 2 Korean 0 Male 26.100000 1 8 3 American 0 Male 32.700001 8 9 3 Japanese 1 Male 32.700001 6 10 3 European 0 Male 32.700001 2 11 4 American 1 Female 49.199997 5 12 4 Japanese 0 Female 49.199997 4 13 4 European 0 Female 49.199997 3 14 5 American 0 Male 24.299999 8 15 5 Japanese 0 Male 24.299999 3 16 5 European 1 Male 24.299999 3 17 6 American 1 Female 39.000000 10 18 6 Japanese 0 Female 39.000000 6 19 6 European 0 Female 39.000000 1 20 7 American 0 Male 33.000000 10 21 7 Japanese 0 Male 33.000000 6 22 7 European 1 Male 33.000000 4 23 7 Korean 0 Male 33.000000 1 24 8 American 1 Male 20.299999 6 25 8 Japanese 0 Male 20.299999 5 26 8 European 0 Male 20.299999 3 27 9 American 0 Male 38.000000 9 28 9 Japanese 1 Male 38.000000 9 29 9 European 0 Male 38.000000 2"},{"location":"easy_data_management/#components-of-the-consumer-choice-modelling-problem","title":"Components of the Consumer Choice Modelling Problem","text":"<p>We begin with essential component of the consumer choice modelling problem. Walking through these components should help you understand what kind of data our models are working on.</p>"},{"location":"easy_data_management/#purchasing-record","title":"Purchasing Record","text":"<p>Each row (record) of the dataset is called a purchasing record, which includes who bought what at when and where. Let \\(B\\) denote the number of purchasing records in the dataset (i.e., number of rows of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record (i.e., who bought what at where and when).</p>"},{"location":"easy_data_management/#items-and-categories","title":"Items and Categories","text":"<p>To begin with, there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\) under our consideration.</p> <p>Further, the researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\). Let \\(I_c\\) denote the collection of items in category \\(c\\), it is easy to verify that</p> \\[ \\bigcup_{c \\in \\{1, 2, \\dots, C\\}} I_c = \\{1, 2, \\dots I\\} \\] <p>If the researcher does not wish to model different categories differently, the researcher can simply put all items in one single category: \\(I_1 = \\{1, 2, \\dots I\\}\\), so that all items belong to the same category.</p> <p>Note: since we will be using PyTorch to train our model, we represent their identities with integer values instead of the raw human-readable names of items (e.g., Dell 24 inch LCD monitor). Raw item names can be encoded easily with sklearn.preprocessing.OrdinalEncoder.</p>"},{"location":"easy_data_management/#users","title":"Users","text":"<p>Each purchaing reocrd is naturally associated with an user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) (who) as well.</p>"},{"location":"easy_data_management/#sessions","title":"Sessions","text":"<p>Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\). For example, when the data came from a single store over the period of a year. In this case, the notion of where does not matter that much, and session \\(s\\) is simply the date of purchase.</p> <p>Another example is that we have the purchase record from different stores, the session \\(s\\) can be defined as a pair of (date, store) instead.</p> <p>If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all rows of the dataset.</p> <p>To summarize, each purchasing record \\(b\\) in the dataset is characterized by a user-session-item tuple \\((u, s, i)\\).</p> <p>When there are multiple items bought by the same user in the same session, there will be multiple rows in the dataset with the same \\((u, s)\\) corresponding to the same receipt.</p>"},{"location":"easy_data_management/#format-the-dataset-a-little-bit","title":"Format the Dataset a Little Bit","text":"<p>The wrapper we built requires several data frames, providing the correct information is all we need to do in this tutorial, the data wrapper will handle the construction of <code>ChoiceDataset</code> for you.</p> <p>Note: The dataset in this tutorial is a bit over-simplified, we only have one purchase record for each user in each session, so the <code>consumerid</code> column identifies all of the user, the session, and the purchase record (because we have different dealers for the same type of car, we define each purchase record of it's session instead of assigning all purchase records to the same session). That is, we have a single user makes a single choice in each single session.</p> <p>The main dataset should contain the following columns:</p> <ol> <li><code>purchase_record_column</code>: a column identifies purchase record (also called case in Stata syntax). this tutorial, the <code>consumerid</code> column is the identifier. For example, the first 4 rows of the dataset (see above) has <code>consumerid == 1</code>, this means we should look at the first 4 rows together and they constitute the first purchase record.</li> <li><code>item_name_column</code>: a column identifies names of items, which is <code>car</code> in the dataset above. This column provides information above the availability as well. As mentioned above, there is no column with <code>car == Korean</code> in the fourth purchasing record (<code>consumerid == 4</code>), so we know that Korean car was not available that time.</li> <li><code>choice_column</code>: a column identifies the choice made by the consumer in each purchase record, which is the <code>purchase</code> column in our example. Exactly one row per purchase record (i.e., rows with the same values in <code>purchase_record_column</code>) should have 1, while the values are zeros for all other rows.</li> <li><code>user_index_column</code>: a optional column identifies the user making the choice, which is also <code>consumerid</code> in our case.</li> <li><code>session_index_column</code>: a optional column identifies the session of the choice, which is also <code>consumerid</code> in our case.</li> </ol> <p>As you might have noticed, the <code>consumerid</code> column in the data-frame identifies multiple pieces of information: <code>purchase_record</code>, <code>user_index</code>, and <code>session_index</code>. This is not a mistake, you can use the same column in <code>df</code> to supply multiple pieces of information. </p> <pre><code>df.gender.value_counts(dropna=False)\n</code></pre> <pre><code>Male      2283\nFemale     854\nNaN         23\nName: gender, dtype: int64\n</code></pre> <p>The only modification required is to convert <code>gender</code> (with values of <code>Male</code>, <code>Female</code> or <code>NaN</code>) to integers because PyTorch does not handle strings. For simplicity, we will assume all <code>NaN</code> gender to be <code>Female</code> (you should not do this in a real application!) and re-define the gender variable as \\(\\mathbb{I}\\{\\texttt{gender} == \\texttt{Male}\\}\\).</p> <pre><code># we change gender to binary 0/1 because pytorch doesn't handle strings.\ndf['gender'] = (df['gender'] == 'Male').astype(int)\n</code></pre> <p>Now the <code>gender</code> column contains only binary integers.</p> <pre><code>df.gender.value_counts(dropna=False)\n</code></pre> <pre><code>1    2283\n0     877\nName: gender, dtype: int64\n</code></pre> <p>The data-frame looks like the following right now:</p> <pre><code>df.head()\n</code></pre> consumerid car purchase gender income dealers 0 1 American 1 1 46.699997 9 1 1 Japanese 0 1 46.699997 11 2 1 European 0 1 46.699997 5 3 1 Korean 0 1 46.699997 1 4 2 American 1 1 26.100000 10"},{"location":"easy_data_management/#adding-the-observables","title":"Adding the Observables","text":"<p>The next step is to identify observables going into the model.</p> <p>Specifically, we would want to add: 1. <code>gender</code> and <code>income</code> as user-specific observables 2. and <code>dealers</code> as (session, item)-specific observable. Such observables are called price observables in our setting, why? because price is the most typical (session, item)-specific observable.</p>"},{"location":"easy_data_management/#method-1-adding-observables-by-extracting-columns-of-the-dataset","title":"Method 1: Adding Observables by Extracting Columns of the Dataset","text":"<p>As you can see, <code>gender</code>, <code>income</code> and <code>dealers</code> are already encompassed in <code>df</code>, the first way to add observables is simply mentioning these columns while initializing the <code>EasyDatasetWrapper</code> object.</p> <p>You can supply a list of names of columns to each of <code>{user, item, session, price}_observable_columns</code> keyword argument. For example, we use <code>user_observable_columns=['gender', 'income']</code> to inform the <code>EasyDatasetWrapper</code> that we wish to derive user-specific observables from the <code>gender</code> and <code>income</code> columns of <code>df</code>.</p> <p>Also, we inform the <code>EasyDatasetWrapper</code> that we want to derive (session, item)-specific (i.e., price observable) by specifying <code>price_observable_columns=['dealers']</code>.</p> <p>Since our package leverages GPU-acceleration, it is necessary to supply the device on which the dataset should reside. The <code>EasyDatasetWrapper</code> also takes a <code>device</code> keyword, which can be either <code>'cpu'</code> or an appropriate CUDA device.</p> <pre><code>if torch.cuda.is_available():\n    device = 'cuda'  # use GPU if available\nelse:\n    device = 'cpu'  # use CPU otherwise\n</code></pre> <pre><code>data_1 = EasyDatasetWrapper(main_data=df,\n                            # TODO: better naming convention? Need to discuss.\n                            # after discussion, we add it to the default value\n                            # in the data wrapper class.\n                            # these are just names.\n                            purchase_record_column='consumerid',\n                            choice_column='purchase',\n                            item_name_column='car',\n                            user_index_column='consumerid',\n                            session_index_column='consumerid',\n                            # it can be derived from columns of the dataframe or supplied as\n                            user_observable_columns=['gender', 'income'],\n                            price_observable_columns=['dealers'],\n                            device=device)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre> <p>The dataset has a <code>summary()</code> method, which can be used to print out the summary of the dataset.</p> <pre><code>data_1.summary()\n</code></pre> <pre><code>* purchase record index range: [1 2 3] ... [883 884 885]\n* Space of 4 items:\n                   0         1         2       3\nitem name  American  European  Japanese  Korean\n* Number of purchase records/cases: 885.\n* Preview of main data frame:\n      consumerid       car  purchase  gender     income  dealers\n0              1  American         1       1  46.699997        9\n1              1  Japanese         0       1  46.699997       11\n2              1  European         0       1  46.699997        5\n3              1    Korean         0       1  46.699997        1\n4              2  American         1       1  26.100000       10\n...          ...       ...       ...     ...        ...      ...\n3155         884  Japanese         1       1  20.900000       10\n3156         884  European         0       1  20.900000        4\n3157         885  American         1       1  30.600000       10\n3158         885  Japanese         0       1  30.600000        5\n3159         885  European         0       1  30.600000        4\n\n[3160 rows x 6 columns]\n* Preview of ChoiceDataset:\nChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], itemsession_dealers=[885, 4, 1], device=cpu)\n</code></pre> <p>You can access the <code>ChoiceDataset</code> object constructed by calling the <code>data.choice_dataset</code> object. </p> <pre><code>data_1.choice_dataset\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], itemsession_dealers=[885, 4, 1], device=cpu)\n</code></pre>"},{"location":"easy_data_management/#method-2-adding-observables-as-data-frames","title":"Method 2: Adding Observables as Data Frames","text":"<p>We can also construct data frames and use data frames to supply different observables. This is useful when you have a large dataset, for example, if there are many purchase records for the same user (to be concrete, say \\(U\\) users and \\(N\\) purchase records for each user, resulting \\(U \\times N\\) total purchase records). Using a single data-frame requires a lot of memory: you need to store \\(U \\times N\\) entires of user genders in total. However, user genders should be persistent across all purchasing records, if we use a separate data-frame mapping user index to gender of the user, we only need to store \\(U\\) entries (i.e., one for each user) of gender information.</p> <p>Similarly, the long-format data requires storing each piece of item-specific information for number of purchase records times, which leads to inefficient usage of disk/memory space.</p>"},{"location":"easy_data_management/#how-do-observable-data-frame-look-like","title":"How Do Observable Data-frame Look Like?","text":"<p>Our package natively support the following four types of observables:</p> <ol> <li> <p>User Observables: user-specific observables (e.g., gender and income) should (1) have length equal to the number of unique users in the dataset (885 here); (2) contains a column named as <code>user_index_column</code> (<code>user_index_column</code> is a variable, the actual column name should be the value of variable <code>user_index_column</code>! E.g., here the user observable data-frame should have a column named <code>'consumerid'</code>); (3) the user observable can have any number of other columns beside the <code>user_index_column</code> column, each of them corresponding to a user-specific observable. For example, a data-frame containing \\(X\\) user-specific observables has shape <code>(num_users, X + 1)</code>.</p> </li> <li> <p>Item Observables item-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique items in the dataset (4 here); (2) contain a column named as <code>item_index_column</code> (<code>item_index_column</code> is a variable, the actual column name should be the value of variable <code>item_index_column</code>! E.g., here the item observable data-frame should have a column named <code>'car'</code>); (3) the item observable can have any number of other columns beside the <code>item_index_column</code> column, each of them corresponding to a item-specific observable.</p> </li> <li> <p>Session Observable session-specific observables (not shown in this tutorial) should be (1) have length equal to the number of unique sessions in the dataset; (2) contain a column named as <code>session_index_column</code> (<code>session_index_column</code> is a variable, the actual column name should be the value of variable <code>session_index_column</code>! E.g., here the session observable data-frame should have a column named <code>'consumerid'</code>); (3) the session observable can have any number of other columns beside the <code>session_index_column</code> column, each of them corresponding to a session-specific observable.</p> </li> <li> <p>Price Observables (session, item)-specific observables (e.g., dealers) should be (1) contains a column named as <code>session_index_column</code> (e.g., <code>consumerid</code> in our example) and a column named as <code>item_name_column</code> (e.g., <code>car</code> in our example), (2) the price observable can have any number of other columns beside the <code>session_index_column</code> and <code>item_name_column</code> columns, each of them corresponding to a (session, item)-specific observable. For example, a data-frame containing \\(X\\) (session, item)-specific observables has shape <code>(num_sessions, num_items, X + 2)</code>.</p> </li> </ol> <p>We encourage the reader to review the Data Management Tutorial for more details on types of observables.</p>"},{"location":"easy_data_management/#suggested-procedure-of-storing-and-loading-data","title":"Suggested Procedure of Storing and Loading Data","text":"<ol> <li>Suppose <code>SESSION_INDEX</code> column in <code>df_main</code> is the index of the session, <code>ALTERNATIVES</code> column is the index of the car.</li> <li>For user-specific observables, you should have a CSV on disk with columns {<code>consumerid</code>, <code>var_1</code>, <code>var_2</code>, ...}.</li> <li>You load the user-specific dataset as <code>user_obs = pd.read_csv(..., index='consumerid')</code>.</li> </ol> <p>Let's first construct the data frame for user genders first.</p> <pre><code>gender = df.groupby('consumerid')['gender'].first().reset_index()\n</code></pre> <p>The user-observable data-frame contains a column of user IDs (the <code>consumerid</code> column), this column should have exactly the same name as the column containing user indices. Otherwise, the wrapper won't know which column corresponds to user IDs and which column corresponds to variables.</p> <pre><code>gender.head()\n</code></pre> consumerid gender 0 1 1 1 2 1 2 3 1 3 4 0 4 5 1 <p>Then, let's build the data-frame for user-specific income variables.</p> <pre><code>income = df.groupby('consumerid')['income'].first().reset_index()\n</code></pre> <pre><code>income.head()\n</code></pre> consumerid income 0 1 46.699997 1 2 26.100000 2 3 32.700001 3 4 49.199997 4 5 24.299999 <p>Please note that we can have multiple observables contained in the same data-frame as well.</p> <pre><code>gender_and_income = df.groupby('consumerid')[['gender', 'income']].first().reset_index()\ngender_and_income\n</code></pre> consumerid gender income 0 1 1 46.699997 1 2 1 26.100000 2 3 1 32.700001 3 4 0 49.199997 4 5 1 24.299999 ... ... ... ... 880 881 1 45.700001 881 882 1 69.800003 882 883 0 45.599998 883 884 1 20.900000 884 885 1 30.600000 <p>885 rows \u00d7 3 columns</p> <p>The price observable data-frame contains two columns identifying session (i.e., the <code>consumerid</code> column) and item (i.e., the <code>car</code> column). The session index column should have exactly the same name as the session index column in <code>df</code> and the column indexing columns should have exactly the same name as the item-name-column in <code>df</code>.</p> <pre><code>dealers = df[['consumerid', 'car', 'dealers']]\n</code></pre> <pre><code>dealers.head()\n</code></pre> consumerid car dealers 0 1 American 9 1 1 Japanese 11 2 1 European 5 3 1 Korean 1 4 2 American 10"},{"location":"easy_data_management/#build-datasets-using-easydatasetwrapper-with-observables-as-data-frames","title":"Build Datasets using <code>EasyDatasetWrapper</code> with Observables as Data-Frames","text":"<p>We can observables as data-frames using <code>{user, item, session, price}_observable_data</code> keyword arguments. </p> <pre><code>data_2 = EasyDatasetWrapper(main_data=df,\n                            purchase_record_column='consumerid',\n                            choice_column='purchase',\n                            item_name_column='car',\n                            user_index_column='consumerid',\n                            session_index_column='consumerid',\n                            # above are the same as before, but we update the following.\n                            user_observable_data={'gender': gender, 'income': income},\n                            price_observable_data={'dealers': dealers},\n                            device=device)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre> <pre><code># Use summary to see what's inside the data wrapper.\ndata_2.summary()\n</code></pre> <pre><code>* purchase record index range: [1 2 3] ... [883 884 885]\n* Space of 4 items:\n                   0         1         2       3\nitem name  American  European  Japanese  Korean\n* Number of purchase records/cases: 885.\n* Preview of main data frame:\n      consumerid       car  purchase  gender     income  dealers\n0              1  American         1       1  46.699997        9\n1              1  Japanese         0       1  46.699997       11\n2              1  European         0       1  46.699997        5\n3              1    Korean         0       1  46.699997        1\n4              2  American         1       1  26.100000       10\n...          ...       ...       ...     ...        ...      ...\n3155         884  Japanese         1       1  20.900000       10\n3156         884  European         0       1  20.900000        4\n3157         885  American         1       1  30.600000       10\n3158         885  Japanese         0       1  30.600000        5\n3159         885  European         0       1  30.600000        4\n\n[3160 rows x 6 columns]\n* Preview of ChoiceDataset:\nChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], itemsession_dealers=[885, 4, 1], device=cpu)\n</code></pre> <p>Alternatively, we can supply user income and gender as a single dataframe, instead of <code>user_gender</code> and <code>user_income</code> tensors, now the constructed <code>ChoiceDataset</code> contains a single <code>user_gender_and_income</code> tensor with shape (885, 2) encompassing both income and gender of users.</p> <pre><code>data_3 = EasyDatasetWrapper(main_data=df,\n                            purchase_record_column='consumerid',\n                            choice_column='purchase',\n                            item_name_column='car',\n                            user_index_column='consumerid',\n                            session_index_column='consumerid',\n                            # above are the same as before, but we update the following.\n                            user_observable_data={'gender_and_income': gender_and_income},\n                            price_observable_data={'dealers': dealers},\n                            device=device)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre> <pre><code>data_3.summary()\n</code></pre> <pre><code>* purchase record index range: [1 2 3] ... [883 884 885]\n* Space of 4 items:\n                   0         1         2       3\nitem name  American  European  Japanese  Korean\n* Number of purchase records/cases: 885.\n* Preview of main data frame:\n      consumerid       car  purchase  gender     income  dealers\n0              1  American         1       1  46.699997        9\n1              1  Japanese         0       1  46.699997       11\n2              1  European         0       1  46.699997        5\n3              1    Korean         0       1  46.699997        1\n4              2  American         1       1  26.100000       10\n...          ...       ...       ...     ...        ...      ...\n3155         884  Japanese         1       1  20.900000       10\n3156         884  European         0       1  20.900000        4\n3157         885  American         1       1  30.600000       10\n3158         885  Japanese         0       1  30.600000        5\n3159         885  European         0       1  30.600000        4\n\n[3160 rows x 6 columns]\n* Preview of ChoiceDataset:\nChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender_and_income=[885, 2], itemsession_dealers=[885, 4, 1], device=cpu)\n</code></pre>"},{"location":"easy_data_management/#method-3-mixing-method-1-and-method-2","title":"Method 3: Mixing Method 1 and Method 2","text":"<p>The <code>EasyDataWrapper</code> also support supplying observables as a mixture of above methods. The following example supplies <code>gender</code> user observable as a data-frame but <code>income</code> and <code>dealers</code> as column names. </p> <pre><code>data_4 = EasyDatasetWrapper(main_data=df,\n                            purchase_record_column='consumerid',\n                            choice_column='purchase',\n                            item_name_column='car',\n                            user_index_column='consumerid',\n                            session_index_column='consumerid',\n                            # above are the same as before, but we update the following.\n                            user_observable_data={'gender': gender},\n                            user_observable_columns=['income'],\n                            price_observable_columns=['dealers'],\n                            device=device)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre> <pre><code>data_4.summary()\n</code></pre> <pre><code>* purchase record index range: [1 2 3] ... [883 884 885]\n* Space of 4 items:\n                   0         1         2       3\nitem name  American  European  Japanese  Korean\n* Number of purchase records/cases: 885.\n* Preview of main data frame:\n      consumerid       car  purchase  gender     income  dealers\n0              1  American         1       1  46.699997        9\n1              1  Japanese         0       1  46.699997       11\n2              1  European         0       1  46.699997        5\n3              1    Korean         0       1  46.699997        1\n4              2  American         1       1  26.100000       10\n...          ...       ...       ...     ...        ...      ...\n3155         884  Japanese         1       1  20.900000       10\n3156         884  European         0       1  20.900000        4\n3157         885  American         1       1  30.600000       10\n3158         885  Japanese         0       1  30.600000        5\n3159         885  European         0       1  30.600000        4\n\n[3160 rows x 6 columns]\n* Preview of ChoiceDataset:\nChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], user_gender=[885, 1], user_income=[885, 1], itemsession_dealers=[885, 4, 1], device=cpu)\n</code></pre>"},{"location":"easy_data_management/#sanity-checks","title":"Sanity Checks","text":"<p>Lastly, let's check choice datasets constructed via different methods are actually the same.</p> <p>The <code>==</code> method of choice datasets will compare the non-NAN entries of all tensors in datasets. </p> <pre><code>print(data_1.choice_dataset == data_2.choice_dataset)\nprint(data_1.choice_dataset == data_4.choice_dataset)\n</code></pre> <pre><code>True\nTrue\n</code></pre> <p>For <code>data_3</code>, we have <code>income</code> and <code>gender</code> combined:</p> <pre><code>data_3.choice_dataset.user_gender_and_income == torch.cat([data_1.choice_dataset.user_gender, data_1.choice_dataset.user_income], dim=1)\n</code></pre> <pre><code>tensor([[True, True],\n        [True, True],\n        [True, True],\n        ...,\n        [True, True],\n        [True, True],\n        [True, True]])\n</code></pre> <p>Now let's compare what's inside the data structure and our raw data.</p> <pre><code>bought_raw = df[df['purchase'] == 1]['car'].values\nbought_data = list()\nencoder = {0: 'American', 1: 'European', 2: 'Japanese', 3: 'Korean'}\nfor b in data_1.choice_dataset.item_index:\n    bought_data.append(encoder[float(b)])\n</code></pre> <pre><code>all(bought_raw == bought_data)\n</code></pre> <pre><code>True\n</code></pre> <p>Then, let's compare the income and gender variable contained in the dataset. </p> <pre><code>X = df.groupby('consumerid')['income'].first().values\nY = data_1.choice_dataset.user_income.cpu().numpy().squeeze()\nall(X == Y)\n</code></pre> <pre><code>True\n</code></pre> <pre><code>X = df.groupby('consumerid')['gender'].first().values\n</code></pre> <pre><code>Y = data_1.choice_dataset.user_gender.cpu().numpy().squeeze()\nall(X == Y)\n</code></pre> <pre><code>True\n</code></pre> <p>Lastly, let's compare the <code>price_dealer</code> variable. Since there are NAN-values in it for unavailable cars, we can't not use <code>all(X == Y)</code> to compare them. We will first fill NANs values with <code>-1</code> and then compare resulted data-frames.</p> <pre><code># rearrange columns to align it with the internal encoding scheme of the data wrapper.\nX = df.pivot('consumerid', 'car', 'dealers')[['American', 'European', 'Japanese', 'Korean']]\n</code></pre> <pre><code>X\n</code></pre> car American European Japanese Korean consumerid 1 9.0 5.0 11.0 1.0 2 10.0 2.0 7.0 1.0 3 8.0 2.0 6.0 NaN 4 5.0 3.0 4.0 NaN 5 8.0 3.0 3.0 NaN ... ... ... ... ... 881 8.0 2.0 10.0 NaN 882 8.0 6.0 8.0 1.0 883 9.0 5.0 8.0 1.0 884 12.0 4.0 10.0 NaN 885 10.0 4.0 5.0 NaN <p>885 rows \u00d7 4 columns</p> <pre><code>Y = data_1.choice_dataset.itemsession_dealers.squeeze(dim=-1)\n</code></pre> <pre><code>Y\n</code></pre> <pre><code>tensor([[ 9.,  5., 11.,  1.],\n        [10.,  2.,  7.,  1.],\n        [ 8.,  2.,  6., nan],\n        ...,\n        [ 9.,  5.,  8.,  1.],\n        [12.,  4., 10., nan],\n        [10.,  4.,  5., nan]])\n</code></pre> <pre><code>print(X.fillna(-1).values == torch.nan_to_num(Y, -1).cpu().numpy())\n</code></pre> <pre><code>[[ True  True  True  True]\n [ True  True  True  True]\n [ True  True  True  True]\n ...\n [ True  True  True  True]\n [ True  True  True  True]\n [ True  True  True  True]]\n</code></pre> <p>This concludes our tutorial on building the dataset, if you wish more in-depth understanding of the data structure, please refer to the Data Management Tutorial.</p>"},{"location":"install/","title":"Installation","text":"<p>This page will guide you through the installation procedure of <code>torch-choice</code> and <code>bemb</code>.</p> <p>There are two parts of this project: the <code>torch_choice</code> library consisting of data management modules, logit and nested-logit models for consumer choice modelling.</p> <p>For researchers wish to use the Bayesian Embedding (BEMB) model, they need to install an additional <code>bemb</code> package, which was built on the top of <code>torch_choice</code>.</p> <p>Note Since this project is still on its pre-release stage and subject to changes, we have not uploaded our packages to PIP or CONDA. Researchers need to install these packages from Github source code.</p>"},{"location":"install/#option-1-install-using-source-code-from-github-repository","title":"Option 1: Install using Source Code from Github Repository","text":"<p>To install <code>torch_choice</code> and <code>bemb</code> from source, 1. Clone the repositories of both <code>torch_choice</code> and <code>bemb</code> to your local machine. 2. Install required dependencies (e.g., PyTorch and PyTorch-Lightning). 3. For each of repositories, run <code>python3 ./setup.py develop</code> to add the package to your Python environment. 4. Check installation by running <code>python3 -c \"import torch_choice; print(torch_choice.__version__)\"</code>. 5. Check installation by running <code>python3 -c \"import bemb; print(bemb.__version__)\"</code>.</p>"},{"location":"install/#option-2-install-using-pip","title":"Option 2: Install using Pip","text":"<p>The <code>torch-choice</code> is available on PIP now here! You can use <code>pip install torch-choice</code> to install it. Note: We are working on publishing BEMB to PIP.</p>"},{"location":"intro/","title":"Introduction","text":"<p>Logistic Regression models the probability that user \\(u\\) chooses item \\(i\\) in session \\(s\\) by the logistic function</p> \\[ P_{uis} = \\frac{e^{\\mu_{uis}}}{\\Sigma_{j \\in A_{us}}e^{\\mu_{ujs}}} \\] <p>where, </p> \\[\\mu_{uis} = \\alpha + \\beta X + \\gamma W + \\dots\\] <p>here \\(X\\), \\(W\\) are predictors (independent variables) for users and items respectively (these can be constant or can vary across session), and greek letters \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are learned parameters. \\(A_{us}\\) is the set of items available for user \\(u\\) in session \\(s\\).</p> <p>When users are choosing over items, we can write utility \\(U_{uis}\\) that user \\(u\\) derives from item \\(i\\) in session \\(s\\), as</p> \\[ U_{uis} = \\mu_{uis} + \\epsilon_{uis} \\] <p>where \\(\\epsilon\\) is an unobserved random error term.</p> <p>If we assume iid extreme value type 1 errors for \\(\\epsilon_{uis}\\), this leads to the above logistic probabilities of user \\(u\\) choosing item \\(i\\) in session \\(s\\), as shown by McFadden, and as often studied in Econometrics.</p>"},{"location":"intro/#package","title":"Package","text":"<p>We implement a fully flexible setup, where we allow  1. coefficients (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\dots\\)) to be constant, user-specific (i.e., \\(\\alpha=\\alpha_u\\)), item-specific (i.e., \\(\\alpha=\\alpha_i\\)), session-specific (i.e., \\(\\alpha=\\alpha_t\\)), or (session, item)-specific (i.e., \\(\\alpha=\\alpha_{ti}\\)). For example, specifying \\(\\alpha\\) to be item-specific is equivalent to adding an item-level fixed effect. 2. Observables (\\(X\\), \\(Y\\), \\(\\dots\\)) to be constant, user-specific, item-specific, session-specific, or (session, item) (such as price) and (session, user) (such as income) specific as well. 3. Specifying availability sets \\(A_{us}\\)</p> <p>This flexibility in coefficients and features allows for more than 20 types of additive terms to \\(U_{uis}\\), which enables modelling rich structures.</p> <p>As such, this package can be used to learn such models for 1. Parameter Estimation, as in the Transportation Choice example below 2. Prediction, as in the MNIST handwritten digits classification example below</p> <p>Examples with Utility Form: 1. Transportation Choice (from the Mode Canada dataset) (Detailed Tutorial)</p> \\[ U_{uit} = \\beta^0_i + \\beta^{1} X^{itemsession: (cost, freq, ovt)}_{it} + \\beta^2_i X^{session:income}_t + \\beta^3_i X_{it}^{itemsession:ivt} + \\epsilon_{uit} \\] <p>This is also described as a conditional logit model in Econometrics. We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case there is one user per session, so that U = S</p> <p>Then, - \\(X^{itemsession: (cost, freq, ovt)}_{it}\\) is a matrix of size (I x S) x (3); it has three entries for each item-session, and is like a price; its coefficient \\(\\beta^{1}\\) has constant variation and is of size (1) x (3). - \\(X^{session: income}_{it}\\) is a matrix which is of size (S) x (1); it has one entry for each session, and it denotes income of the user making the choice in the session. In this case, it is equivalent to \\(X^{usersession: income}_{it}\\) since we observe a user making a decision only once; its coefficient \\(\\beta^2_i\\) has item level variation and is of size (I) x (1) - \\(X_{it}^{itemsession:ivt}\\) is a matrix of size (I x S) x (1); this has one entry for each item-session; it is the price; its coefficent \\(\\beta^3_i\\) has item level variation and is of size (I) x (3)</p> <ol> <li>MNIST classification (Upcoming Detailed Tutorial)</li> </ol> \\[ U_{it} = \\beta_i X^{session:pixelvalues}_{t} + \\epsilon_{it} \\] <p>We note the shapes/sizes of each of the components in the above model. Suppose there are U users, I items and S sessions; in this case, an item is one of the 10 possible digits, so I = 10; there is one user per session, so that U=S; and each session is an image being classified. Then, - \\(X^{session:pixelvalues}_{t}\\) is a matrix of size (S) x (H x W) where H x W are the dimensions of the image being classified; its coefficient \\(\\beta_i\\) has item level vartiation and is of size (I) x (1)</p> <p>This is a classic problem used for exposition in Computer Science to motivate various Machine Learning models. There is no concept of a user in this setup. Our package allows for models of this nature and is fully usable for Machine Learning problems with added flexibility over scikit-learn logistic regression</p> <p>We highly recommend users to go through tutorials we prepared to get a better understanding of what the package offers. We present multiple examples, and for each case we specify the utility form.</p>"},{"location":"intro/#notes-on-encodings","title":"Notes on Encodings","text":"<p>Since we will be using PyTorch to train our model, we accept user and item identities with integer values from [0, 1, .. num_users - 1] and [0, 1, .. num_items - 1] instead of the raw human-readable names of items (e.g., Dell 24-inch LCD monitor) or any other encoding. The user is responsible to encode user indices, item indices and session indices, wherever appliable (some setups do not require session and/or user identifiers) Raw item/user/session names can be encoded easily with sklearn.preprocessing.LabelEncoder (The sklearn.preprocessing.OrdinalEncoder works as well).</p> <p>Here is an example of encoding generic item names to integers using <code>sklearn.preprocessing.LabelEncoder</code>:</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nraw_items = ['Macbook Laptop', 'Dell 24-inch Monitor', 'Orange', 'Apple (Fruit)']\n\nencoded_items = enc.fit_transform(raw_items)\nprint(encoded_items)\n# output: [2 1 3 0]\n\n# for each 0 &lt;= i &lt;= 3, enc.classes_[i] reveals the raw name of item encoded to i.\nprint(enc.classes_)\n# output: ['Apple (Fruit)' 'Dell 24-inch Monitor' 'Macbook Laptop' 'Orange']\n# For example, the first entry of enc.classes_ is 'Apple (Fruit)', this means 'Apple (Fruit)' was encoded to 0 in this process.\n# The last item in the `raw_item` list was 'Apple (Fruit)', and the last item in the `encoded_item` list was 0 as we expected.\n</code></pre>"},{"location":"intro/#components-of-the-choice-modeling-problem","title":"Components of the Choice Modeling Problem","text":"<p>For the rest of this tutorial, we will consider retail supermarket choice as the concrete setting.</p> <p>We aim to predict users' choices while choosing between multiple available items, e.g., which brand of milk the user will purchase in the supermarket.</p> <p>We begin with essential components of the choice modeling problem. Walking through these components helps understand what kind of data our models are working on.</p>"},{"location":"intro/#purchase-record","title":"Purchase Record","text":"<p>A purchase record is a record describing who bought what at when and where. Let \\(B\\) denote the number of purchase records in the dataset (i.e., number of rows/observation of the dataset). Each row \\(b \\in \\{1,2,\\dots, B\\}\\) corresponds to a purchase record.</p>"},{"location":"intro/#what-items-and-categories","title":"What: Items and Categories","text":"<p>To begin with, suppose there are \\(I\\) items indexed by \\(i \\in \\{1,2,\\dots,I\\}\\).</p> <p>The researcher can optionally partition the set items into \\(C\\) categories indexed by \\(c \\in \\{1,2,\\dots,C\\}\\). Let \\(I_c\\) denote the collection of items in category \\(c\\). It's easy to see that the union of all \\(I_c\\) is the entire set of items \\(\\{1, 2, \\dots I\\}\\). Suppose the researcher does not wish to model different categories differently. In that case, the researcher can put all items in one category: \\(I_1 = \\{1, 2, \\dots I\\}\\), so all items belong to the same category.</p> <p>For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\), there is a corresponding \\(i_b \\in \\{1,2,\\dots,I\\}\\) saying which item was chosen in this record.</p>"},{"location":"intro/#who-users","title":"Who: Users","text":"<p>The agent which makes choices in our setting is a user indexed by \\(u \\in \\{1,2,\\dots,U\\}\\) as well.</p> <p>For each purchase record \\(b \\in \\{1,2,\\dots, B\\}\\), there is a corresponding \\(u_b \\in \\{1,2,\\dots,I\\}\\) describing which user was making the decision.</p>"},{"location":"intro/#when-and-where-sessions","title":"When and Where: Sessions","text":"<p>Our data structure encompasses where and when using a notion called session indexed by \\(s \\in \\{1,2,\\dots, S\\}\\).</p> <p>For example, we had the purchase record from five different stores for every day in 2021, then a session \\(s\\) is defined as a pair of (date, storeID), and there are \\(5 \\times 365\\) sessions in total.</p> <p>In another example, suppose the data came from a single store for over a year. In this case, the notion of where is immaterial, and session \\(s\\) is simply the date of purchase.</p> <p>The notion of sessions can be more flexible than just date and location. For example, if we want to distinguish between online ordering and in-store purchasing, we can define the session as (date, storeID, IsOnlineOrdering). The session variable serves as a tool for the researcher to split the dataset; the usefulness of the session will be more evident after introducing observables (features) later.</p> <p>If the researcher does not wish to handle records from different sessions differently, the researcher can assign the same session ID to all dataset rows.</p>"},{"location":"intro/#putting-everything-together","title":"Putting Everything Together","text":"<p>To summarize, each purchase record \\(b \\in \\{1, 2, \\dots, B\\}\\) in the dataset is characterized by a user-session-item tuple \\((u_b, s_b, i_b)\\). The totality of \\(B\\) purchase records consists of the dataset we are modeling.</p> <p>When the same user buys multiple items in the same session, the dataset will have multiple purchase records with the same \\((u, s)\\) corresponding to the same receipt. In this case, the modeling assumption is that the user buys at most one item from each category available to choose from.</p>"},{"location":"intro/#item-availability","title":"Item Availability","text":"<p>It is not necessarily that all items are available in every session; items can get out of stock in particular sessions.</p> <p>To handle these cases, the researcher can optionally provide a boolean tensor \\(A \\in \\{\\texttt{True}, \\texttt{False}\\}^{S\\times I}\\) to indicate which items are available for purchase in each session. \\(A_{s, i} = \\texttt{True}\\) if and only if item \\(i\\) was available in session \\(s\\).</p> <p>While predicting the purchase probabilities, the model sets the probability for these unavailable items to zero and normalizes probabilities among available items. If the item availability is not provided, the model assumes all items are available in all sessions.</p>"},{"location":"intro/#observables","title":"Observables","text":"<p>Next, let's talk about observables. This is the same as a feature in machine learning literature, commonly denoted using \\(X\\). The researcher can incorporate observables of, for example, users and/or items into the model.</p> <p>Currently, the package support the following types of observables, where \\(K_{...}\\) denote the number of observables.</p> <ol> <li><code>user_obs</code> \\(\\in \\mathbb{R}^{U\\times K_{user}}\\): user observables such as user age.</li> <li><code>item_obs</code> \\(\\in \\mathbb{R}^{I\\times K_{item}}\\): item observables such as item quality.</li> <li><code>session_obs</code> \\(\\in \\mathbb{R}^{S \\times K_{session}}\\): session observable such as whether the purchase was made on weekdays.</li> <li><code>itemsession_obs</code> \\(\\in \\mathbb{R}^{S \\times I \\times K_{itemsession}}\\), item-session observables are values depending on both session and item such as the price of item. These can also be called <code>price_obs</code></li> <li><code>usersession_obs</code> \\(\\in \\mathbb{R}^{S \\times U \\times K_{usersession}}\\), user-session observables are values depending on both session and user such as the income of the user.</li> </ol> <p>Please note that we consider these four types as definitions of observable types. For example, whenever a variable is user-specific, then we call it an <code>user_obs</code>. This package defines observables in the above way so that the package can easily track the variation of variables and handle these observable tensors correctly.</p>"},{"location":"intro/#a-toy-example","title":"A Toy Example","text":"<p>Suppose we have a dataset of purchase history from two stores (Store A and B) on two dates (Sep 16 and 17), both stores sell {apple, banana, orange} (<code>num_items=3</code>) and there are three people came to those stores between Sep 16 and 17.</p> user_index session_index item_index Amy Sep-17-2021-Store-A banana Ben Sep-17-2021-Store-B apple Ben Sep-16-2021-Store-A orange Charlie Sep-16-2021-Store-B apple Charlie Sep-16-2021-Store-B orange <p>NOTE: For demonstration purposes, the example dataset has <code>user_index</code>, <code>session_index</code> and <code>item_index</code> as strings, they should be consecutive integers in actual production. One can easily convert them to integers using <code>sklearn.preprocessing.LabelEncoder</code>.</p> <p>In the example above, - <code>user_index=[0,1,1,2,2]</code> (with encoding <code>0=Amy, 1=Ben, 2=Charlie</code>), - <code>session_index=[0,1,2,3,3]</code> (with encoding <code>0=Sep-17-2021-Store-A, 1=Sep-17-2021-Store-B, 2=Sep-16-2021-Store-A, 3=Sep-16-2021-Store-B</code>), - <code>item_index=[0,1,2,1,2]</code> (with encoding <code>0=banana, 1=apple, 2=orange</code>).</p> <p>Suppose we believe people's purchase decision depends on the nutrition levels of these fruits; suppose apple has the highest nutrition level and banana has the lowest one, we can add</p> <p><code>item_obs=[[1.5], [12.0], [3.3]]</code> \\(\\in \\mathbb{R}^{3\\times 1}\\). The shape of this tensor is number-of-items by number-of-observable.</p> <p>NOTE: If someone went to one store and bought multiple items (e.g., Charlie bought both apple and orange at Store B on Sep-16), we include them as separate rows in the dataset and model them independently.</p>"},{"location":"intro/#models","title":"Models","text":"<p>The <code>torch-choice</code> library provides two models, the conditional logit model and the nested logit model, for modeling the dataset. Each model takes in \\((u_b, s_b)\\) altogether with observables and outputs a probability of purchasing each \\(\\tilde{i} \\in \\{1, 2, \\dots, I\\}\\), denoted as \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\). In cases when not all items are available, the model sets the probability of unavailable items to zero and normalizes probabilities among available items. \\(\\hat{p}_{u_b, s_b, \\tilde{i}}\\) is the predicted probability of purchasing item \\(\\tilde{i}\\) in session \\(s_b\\) by user \\(u_b\\) given all information we know. Model parameters are trained using gradient descent algorithm and the loss function is the negative log-likelihood of the model \\(-\\sum_{b=1}^B \\log(\\hat{p}_{u_b, s_b, i_b})\\).</p> <p>The major difference among models lies in the way they compute predicted probabilities.</p>"},{"location":"landing_page_short_tutorial/","title":"Introduction","text":"<p>Welcome to the deep choice documentation site, we will guide you through basics of our package and how to use it.</p> <p>Author: Tianyu Du</p> <p>Date: Jun. 22, 2022</p> <p>Update: Jul. 10, 2022</p> <pre><code>__author__ = 'Tianyu Du'\n</code></pre> <p>In this demonstration, we will guide you through a minimal example of fitting a conditional logit model using our package. We will be referencing to R code and Stata code as well to deliver a smooth knowledge transfer.</p> <p>First thing first, let's import a couple of modules from our package.</p>"},{"location":"landing_page_short_tutorial/#step-0-import-modules","title":"Step 0: Import Modules","text":""},{"location":"landing_page_short_tutorial/#python","title":"Python","text":"<pre><code>import pandas as pd\nfrom torch_choice.utils import EasyDatasetWrapper, run_helper\nfrom torch_choice.model import ConditionalLogitModel\n</code></pre>"},{"location":"landing_page_short_tutorial/#r","title":"R","text":"<pre><code>library(\"mlogit\")\n</code></pre>"},{"location":"landing_page_short_tutorial/#step-1-load-data","title":"Step 1: Load Data","text":"<p>We have include a copy of the <code>ModeCanada</code> dataset in our package: <code>./public_datasets/ModeCanada.csv</code>, it's a very small dataset and please feel free to investigate it using softwares like Microsoft Excel.</p> <p>Let's load the mode canada dataset (TODO: add reference to it).</p>"},{"location":"landing_page_short_tutorial/#python_1","title":"Python","text":"<pre><code>df = pd.read_csv('./public_datasets/ModeCanada.csv').query('noalt == 4').reset_index(drop=True)\n</code></pre>"},{"location":"landing_page_short_tutorial/#r_1","title":"R","text":"<pre><code>ModeCanada &lt;- read.csv('./public_datasets/ModeCanada.csv')\nModeCanada &lt;- select(ModeCanada, -X)\nModeCanada$alt &lt;- as.factor(ModeCanada$alt)\n</code></pre>"},{"location":"landing_page_short_tutorial/#step-2-format-data-frame","title":"Step 2: Format Data-Frame","text":"<p>TODO: add why we need to do it (every package is doing it).</p>"},{"location":"landing_page_short_tutorial/#python_2","title":"Python","text":"<p>Tell the <code>EasyDatasetWrapper</code> about observables</p> <ol> <li>price observable: cost, freq, ovt, ivt</li> <li>session observables: income.</li> </ol> <pre><code>data = EasyDatasetWrapper(\n    main_data=df,\n    purchase_record_column='case',\n    choice_column='choice',\n    item_name_column='alt',\n    user_index_column='case',\n    session_index_column='case',\n    session_observable_columns=['income'],\n    price_observable_columns=['cost', 'freq', 'ovt', 'ivt']\n)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nFinished Creating Choice Dataset.\n</code></pre>"},{"location":"landing_page_short_tutorial/#r_2","title":"R","text":"<pre><code>MC &lt;- dfidx(ModeCanada, subset = noalt == 4)\n</code></pre>"},{"location":"landing_page_short_tutorial/#step-3-define-and-fit-the-conditional-logit-model","title":"Step 3: Define and Fit the Conditional Logit Model","text":""},{"location":"landing_page_short_tutorial/#python_3","title":"Python","text":"<pre><code>model = ConditionalLogitModel(\n    coef_variation_dict={\n        'itemsession_cost': 'constant',\n        'itemsession_freq': 'constant',\n        'itemsession_ovt': 'constant',\n        'session_income': 'item',\n        'itemsession_ivt': 'item-full',\n        'intercept': 'item'\n    },\n    num_items=4\n)\n</code></pre> <pre><code>run_helper.run(model, data.choice_dataset, num_epochs=5000, learning_rate=0.01, batch_size=-1)\n</code></pre> <pre><code>==================== received model ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (itemsession_cost): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (itemsession_freq): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (itemsession_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (itemsession_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[itemsession_cost] with 1 parameters, with constant level variation.\nX[itemsession_freq] with 1 parameters, with constant level variation.\nX[itemsession_ovt] with 1 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[itemsession_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cpu\n==================== received dataset ====================\nChoiceDataset(label=[], item_index=[2779], provided_num_items=[], user_index=[2779], session_index=[2779], item_availability=[], session_income=[2779, 1], itemsession_cost=[2779, 4, 1], itemsession_freq=[2779, 4, 1], itemsession_ovt=[2779, 4, 1], itemsession_ivt=[2779, 4, 1], device=cpu)\n==================== training the model ====================\nEpoch 500: Log-likelihood=-1980.04736328125\nEpoch 1000: Log-likelihood=-1883.31298828125\nEpoch 1500: Log-likelihood=-1878.42333984375\nEpoch 2000: Log-likelihood=-1878.1141357421875\nEpoch 2500: Log-likelihood=-1879.6005859375\nEpoch 3000: Log-likelihood=-1881.0731201171875\nEpoch 3500: Log-likelihood=-1876.06494140625\nEpoch 4000: Log-likelihood=-1877.595703125\nEpoch 4500: Log-likelihood=-1875.7891845703125\nEpoch 5000: Log-likelihood=-1880.450439453125\n==================== model results ====================\nTraining Epochs: 5000\n\nLearning Rate: 0.01\n\nBatch Size: 2779 out of 2779 observations in total\n\nFinal Log-likelihood: -1880.450439453125\n\nCoefficients:\n\n| Coefficient        |   Estimation |   Std. Err. |\n|:-------------------|-------------:|------------:|\n| itemsession_cost_0 | -0.0395517   |  0.00698674 |\n| itemsession_freq_0 |  0.094687    |  0.00504918 |\n| itemsession_ovt_0  | -0.0427526   |  0.00314028 |\n| session_income_0   | -0.0867186   |  0.0174223  |\n| session_income_1   | -0.0268471   |  0.00385441 |\n| session_income_2   | -0.0359928   |  0.00396057 |\n| itemsession_ivt_0  |  0.0597122   |  0.0100132  |\n| itemsession_ivt_1  | -0.00648056  |  0.00417645 |\n| itemsession_ivt_2  | -0.00567451  |  0.00187769 |\n| itemsession_ivt_3  | -0.000954159 |  0.00116984 |\n| intercept_0        | -0.202089    |  1.22288    |\n| intercept_1        |  0.95435     |  0.691519   |\n| intercept_2        |  2.51871     |  0.60307    |\n\n\n\n\n\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (itemsession_cost): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (itemsession_freq): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (itemsession_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=1, 1 trainable parameters in total, device=cpu).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (itemsession_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[itemsession_cost] with 1 parameters, with constant level variation.\nX[itemsession_freq] with 1 parameters, with constant level variation.\nX[itemsession_ovt] with 1 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[itemsession_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre>"},{"location":"landing_page_short_tutorial/#r_3","title":"R","text":"<pre><code>ml.MC1 &lt;- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air')\nsummary(ml.MC1)\n</code></pre> <p>R output: <pre><code>Call:\nmlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, \n    reflevel = \"air\", method = \"nr\")\n\nFrequencies of alternatives:choice\n      air       bus       car     train \n0.3738755 0.0035984 0.4559194 0.1666067 \n\nnr method\n9 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00014 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                    Estimate Std. Error  z-value  Pr(&gt;|z|)    \n(Intercept):bus    0.6983381  1.2802466   0.5455 0.5854292    \n(Intercept):car    1.8441129  0.7085089   2.6028 0.0092464 ** \n(Intercept):train  3.2741952  0.6244152   5.2436 1.575e-07 ***\ncost              -0.0333389  0.0070955  -4.6986 2.620e-06 ***\nfreq               0.0925297  0.0050976  18.1517 &lt; 2.2e-16 ***\novt               -0.0430036  0.0032247 -13.3356 &lt; 2.2e-16 ***\nincome:bus        -0.0890867  0.0183471  -4.8556 1.200e-06 ***\nincome:car        -0.0279930  0.0038726  -7.2286 4.881e-13 ***\nincome:train      -0.0381466  0.0040831  -9.3426 &lt; 2.2e-16 ***\nivt:air            0.0595097  0.0100727   5.9080 3.463e-09 ***\nivt:bus           -0.0067835  0.0044334  -1.5301 0.1259938    \nivt:car           -0.0064603  0.0018985  -3.4029 0.0006668 ***\nivt:train         -0.0014504  0.0011875  -1.2214 0.2219430    \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nLog-Likelihood: -1874.3\nMcFadden R^2:  0.35443 \nLikelihood ratio test : chisq = 2058.1 (p.value = &lt; 2.22e-16)\n</code></pre></p>"},{"location":"nested_logit_model_house_cooling/","title":"Random Utility Model (RUM) Part II: Nested Logit Model","text":"<p>Author: Tianyu Du</p> <p>The package implements the nested logit model as well, which allows researchers to model choices as a two-stage process: the user first picks a nest of purchase and then picks the item from the chosen nest that generates the most utility.</p> <p>Examples here are modified from Exercise 2: Nested logit model by Kenneth Train and Yves Croissant.</p> <p>The House Cooling (HC) dataset from <code>mlogit</code> contains data in R format on the choice of heating and central cooling system for 250 single-family, newly built houses in California.</p> <p>The dataset is small and serve as a demonstration of the nested logit model.</p> <p>The alternatives are:</p> <ul> <li>Gas central heat with cooling <code>gcc</code>,</li> <li>Electric central resistence heat with cooling <code>ecc</code>,</li> <li>Electric room resistence heat with cooling <code>erc</code>,</li> <li>Electric heat pump, which provides cooling also <code>hpc</code>,</li> <li>Gas central heat without cooling <code>gc</code>,</li> <li>Electric central resistence heat without cooling <code>ec</code>,</li> <li>Electric room resistence heat without cooling <code>er</code>.</li> <li>Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is not an alternative.</li> </ul> <p>The variables are:</p> <ul> <li><code>depvar</code> gives the name of the chosen alternative,</li> <li><code>ich.alt</code> are the installation cost for the heating portion of the system,</li> <li><code>icca</code> is the installation cost for cooling</li> <li><code>och.alt</code> are the operating cost for the heating portion of the system</li> <li><code>occa</code> is the operating cost for cooling</li> <li><code>income</code> is the annual income of the household</li> </ul> <p>Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the operating cost and for the other alternatives with cooling.</p>"},{"location":"nested_logit_model_house_cooling/#nested-logit-model-background","title":"Nested Logit Model: Background","text":"<p>The following code block provides an example initialization of the <code>NestedLogitModel</code> (please refer to examples below for details). <pre><code>model = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_coef_variation_dict={},\n                         nest_num_param_dict={},\n                         item_coef_variation_dict={'price_obs': 'constant'},\n                         item_num_param_dict={'price_obs': 7},\n                         shared_lambda=True)\n</code></pre></p> <p>The nested logit model decompose the utility of choosing item \\(i\\) into the (1) item-specific values and (2) nest specify values.  For simplicity, suppose item \\(i\\)  belongs to nest \\(k \\in \\{1, \\dots, K\\}\\): \\(i \\in B_k\\).</p> \\[ U_{uit} = W_{ukt} + Y_{uit} \\] <p>Where both \\(W\\) and \\(Y\\) are estimated using linear models from as in the conditional logit model.</p> <p>The log-likelihood for user \\(u\\) to choose item \\(i\\) at time/session \\(t\\) decomposes into the item-level likelihood and nest-level likelihood.</p> \\[ \\log P(i \\mid u, t) = \\log P(i \\mid u, t, B_k) + \\log P(k \\mid u, t) \\\\ = \\log \\left(\\frac{\\exp(Y_{uit}/\\lambda_k)}{\\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)}\\right) + \\log \\left( \\frac{\\exp(W_{ukt} + \\lambda_k I_{ukt})}{\\sum_{\\ell=1}^K \\exp(W_{u\\ell t} + \\lambda_\\ell I_{u\\ell t})}\\right) \\] <p>The inclusive value of nest \\(k\\), \\(I_{ukt}\\) is defined as \\(\\log \\sum_{j \\in B_k} \\exp(Y_{ujt}/\\lambda_k)\\), which is the expected utility from choosing the best alternative from nest \\(k\\).</p> <p>The <code>nest_to_item</code> keyword defines a dictionary of the mapping \\(k \\mapsto B_k\\), where keys of <code>nest_to_item</code>  are integer \\(k\\)'s and  <code>nest_to_item[k]</code>  is a list consisting of IDs of items in \\(B_k\\).</p> <p>The <code>{nest, item}_coef_variation_dict</code> provides specification to \\(W_{ukt}\\) and \\(Y_{uit}\\) respectively, <code>torch_choice</code> allows for empty nest level models by providing an empty dictionary (in this case, \\(W_{ukt} = \\epsilon_{ukt}\\)) since the inclusive value term \\(\\lambda_k I_{ukt}\\) will be used to model the choice over nests. However, by specifying an empty second stage model (\\(Y_{uit} = \\epsilon_{uit}\\)), the nested logit model reduces to a conditional logit model of choices over nests. Hence, one should never use the <code>NestedLogitModel</code> class with an empty item-level model.</p> <p>Similar to the conditional logit model, <code>{nest, item}_num_param_dict</code> specify the dimension (number of observables to be multiplied with the coefficient) of coefficients. The above code initializes a simple model built upon item-time-specific observables \\(X_{it} \\in \\mathbb{R}^7\\),</p> \\[ Y_{uit} = \\beta^\\top X_{it} + \\epsilon_{uit} \\\\ W_{ukt} = \\epsilon_{ukt} \\] <p>The research may wish to enfoce the elasiticity \\(\\lambda_k\\) to be constant across nests, setting <code>shared_lambda=True</code> enforces \\(\\lambda_k = \\lambda\\ \\forall k \\in [K]\\).</p>"},{"location":"nested_logit_model_house_cooling/#load-essential-packages","title":"Load Essential Packages","text":"<p>We firstly read essential packages for this tutorial.</p> <pre><code># ignore warnings for nicer outputs.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport torch\n\nfrom torch_choice.data import ChoiceDataset, JointDataset, utils\nfrom torch_choice.model.nested_logit_model import NestedLogitModel\nfrom torch_choice import run\nprint(torch.__version__)\n</code></pre> <pre><code>2.0.0\n</code></pre> <p>We then select the appropriate device to run the model on, our package supports both CPU and GPU.</p> <pre><code>if torch.cuda.is_available():\n    print(f'CUDA device used: {torch.cuda.get_device_name()}')\n    DEVICE = 'cuda'\nelse:\n    print('Running tutorial on CPU')\n    DEVICE = 'cpu'\n</code></pre> <pre><code>Running tutorial on CPU\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#load-datasets","title":"Load Datasets","text":"<p>We firstly read the dataset for this tutorial, the <code>csv</code> file can be found at <code>./public_datasets/HC.csv</code>. Alternatively, we load the dataset directly from the Github website.</p> <pre><code>df = pd.read_csv('https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/HC.csv', index_col=0)\ndf = df.reset_index(drop=True)\ndf.head()\n</code></pre> depvar icca occa income ich och idx.id1 idx.id2 inc.room inc.cooling int.cooling cooling.modes room.modes 0 False 0.00 0.00 20 24.50 4.09 1 ec 0 0 0 False False 1 False 27.28 2.95 20 7.86 4.09 1 ecc 0 20 1 True False 2 False 0.00 0.00 20 7.37 3.85 1 er 20 0 0 False True 3 True 27.28 2.95 20 8.79 3.85 1 erc 20 20 1 True True 4 False 0.00 0.00 20 24.08 2.26 1 gc 0 0 0 False False <p>The raw dataset is in a long-format (i.e., each row contains information of one item).</p> <pre><code>df['idx.id2'].value_counts()\n</code></pre> <pre><code>ec     250\necc    250\ner     250\nerc    250\ngc     250\ngcc    250\nhpc    250\nName: idx.id2, dtype: int64\n</code></pre> <pre><code># what was actually chosen.\nitem_index = df[df['depvar'] == True].sort_values(by='idx.id1')['idx.id2'].reset_index(drop=True)\nitem_names = ['ec', 'ecc', 'er', 'erc', 'gc', 'gcc', 'hpc']\nnum_items = df['idx.id2'].nunique()\n# cardinal encoder.\nencoder = dict(zip(item_names, range(num_items)))\nitem_index = item_index.map(lambda x: encoder[x])\nitem_index = torch.LongTensor(item_index)\n</code></pre> <p>Because we will be training our model with <code>PyTorch</code>, we need to encode item names to integers (from 0 to 6). We do this manually in this exercise given the small amount of items, for more items, one can use sklearn.preprocessing.OrdinalEncoder to encode.</p> <p>Raw item names will be encoded as the following.</p> <pre><code>encoder\n</code></pre> <pre><code>{'ec': 0, 'ecc': 1, 'er': 2, 'erc': 3, 'gc': 4, 'gcc': 5, 'hpc': 6}\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#nest-level-dataset","title":"Nest Level Dataset","text":"<p>We firstly construct the nest-level dataset, however, there is no observable that is constant within the same nest, so we don't need to include any observable tensor to the <code>nest_dataset</code>.</p> <p>All we need to do is adding the <code>item_index</code> (i.e., which item is chosen) to the dataset, so that <code>nest_dataset</code> knows the total number of choices made.</p> <pre><code># nest feature: no nest feature, all features are item-level.\nnest_dataset = ChoiceDataset(item_index=item_index.clone()).to(DEVICE)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#item-level-dataset","title":"Item Level Dataset","text":"<p>For simplicity, we treat each purchasing record as its own session. Moreover, we treat all observables as price observables (i.e., varying by both session and item).</p> <p>Since there are 7 observables in total, the resulted <code>price_obs</code> has shape (250, 7, 7) corresponding to <code>number_of_sessions</code> by <code>number_of_items</code> by <code>number_of_observables</code>. </p> <pre><code># item feature.\nitem_feat_cols = ['ich', 'och', 'icca', 'occa', 'inc.room', 'inc.cooling', 'int.cooling']\nprice_obs = utils.pivot3d(df, dim0='idx.id1', dim1='idx.id2', values=item_feat_cols)\nprice_obs.shape\n</code></pre> <pre><code>torch.Size([250, 7, 7])\n</code></pre> <p>Then, we construct the item level dataset by providing both <code>item_index</code> and <code>price_obs</code>.</p> <p>We move <code>item_dataset</code> to the appropriate device as well. This is only necessary if we are using GPU to accelerate the model.</p> <pre><code>item_dataset = ChoiceDataset(item_index=item_index, price_obs=price_obs).to(DEVICE)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n</code></pre> <p>Finally, we chain the nest-level and item-level dataset into a single <code>JointDataset</code>.</p> <pre><code>dataset = JointDataset(nest=nest_dataset, item=item_dataset)\n</code></pre> <p>One can print the joint dataset to see its contents, and tensors contained in each of these sub-datasets.</p> <pre><code>print(dataset)\n</code></pre> <pre><code>JointDataset with 2 sub-datasets: (\n    nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu)\n)\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#examples","title":"Examples","text":"<p>There are multiple ways to group 7 items into nests, different classification will result in different utility functions and estimations (see the background of nested logit models).</p> <p>We will demonstrate the usage of our package by presenting three different categorization schemes and corresponding model estimations.</p>"},{"location":"nested_logit_model_house_cooling/#example-1","title":"Example 1","text":"<p>In the first example, the model is specified to have the cooling alternatives <code>{gcc, ecc, erc, hpc}</code> in one nest and the non-cooling alternatives <code>{gc, ec, er}</code> in another nest.</p> <p>We create a <code>nest_to_item</code> dictionary to inform the model our categorization scheme. The dictionary should have keys ranging from <code>0</code> to <code>number_of_nests - 1</code>, each integer corresponds to a nest. The value of each key is a list of item IDs in the nest, the encoding of item names should be exactly the same as in the construction of <code>item_index</code>.</p> <pre><code>nest_to_item = {0: ['gcc', 'ecc', 'erc', 'hpc'],\n                1: ['gc', 'ec', 'er']}\n\n# encode items to integers.\nfor k, v in nest_to_item.items():\n    v = [encoder[item] for item in v]\n    nest_to_item[k] = sorted(v)\n</code></pre> <p>In this example, we have item <code>[1, 3, 5, 6]</code> in the first nest (i.e., the nest with ID <code>0</code>) and the rest of items in the second nest (i.e., the nest with ID <code>1</code>).</p> <pre><code>print(nest_to_item)\n</code></pre> <pre><code>{0: [1, 3, 5, 6], 1: [0, 2, 4]}\n</code></pre> <p>Next, let's create the <code>NestedLogitModel</code> class!</p> <p>The first thing to put in is the <code>nest_to_item</code> dictionary we just built.</p> <p>For <code>nest_coef_variation_dict</code>, <code>nest_num_param_dict</code>, since we don't have any nest-specific observables, we can simply put an empty dictionary there.</p> <p>Coefficients for all observables are constant across items, and there are 7 observables in total.</p> <p>As for <code>shared_lambda=True</code>, please refer to the background recap for nested logit model.</p> <pre><code>model = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_coef_variation_dict={},\n                         nest_num_param_dict={},\n                         item_coef_variation_dict={'price_obs': 'constant'},\n                         item_num_param_dict={'price_obs': 7},\n                         shared_lambda=True)\n\nmodel = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_formula='',\n                         item_formula='(price_obs|constant)',\n                         dataset=dataset,\n                         shared_lambda=True)\n\nmodel = model.to(DEVICE)\n</code></pre> <p>You can print the model to get summary information of the <code>NestedLogitModel</code> class.</p> <pre><code>print(model)\n</code></pre> <pre><code>NestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre> <p>NOTE: We are computing the standard errors using \\(\\sqrt{\\text{diag}(H^{-1})}\\), where \\(H\\) is the hessian of negative log-likelihood with respect to model parameters. This leads to slight different results compared with R implementation.</p> <p>Here we use the LBFGS optimizer since we are working on a small dataset and 8 coefficients to be estimated. For larger datasets and larger models, we recommend using the Adam optimizer instead.</p> <pre><code>run(model, dataset, num_epochs=1000, model_optimizer=\"LBFGS\")\n</code></pre> <pre><code>==================== model received ====================\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n==================== data set received ====================\n[Train dataset] JointDataset with 2 sub-datasets: (\n    nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu)\n)\n[Validation dataset] None\n[Test dataset] None\n\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type             | Params\n-------------------------------------------\n0 | model | NestedLogitModel | 8     \n-------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 136.14it/s, loss=178, v_num=29]\n\n`Trainer.fit` stopped: `max_epochs=1000` reached.\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 121.79it/s, loss=178, v_num=29]\nTime taken for training: 13.282686233520508\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -178.124755859375, [Validation] N/A, [Test] N/A\n\n| Coefficient                |   Estimation |   Std. Err. |   z-value |    Pr(&gt;|z|) | Significance   |\n|:---------------------------|-------------:|------------:|----------:|------------:|:---------------|\n| lambda_weight_0            |     0.585898 |   0.166624  |   3.51628 | 0.000437634 | ***            |\n| item_price_obs[constant]_0 |    -0.554846 |   0.144515  |  -3.83936 | 0.000123357 | ***            |\n| item_price_obs[constant]_1 |    -0.857842 |   0.237496  |  -3.61203 | 0.000303804 | ***            |\n| item_price_obs[constant]_2 |    -0.225084 |   0.110576  |  -2.03556 | 0.0417943   | *              |\n| item_price_obs[constant]_3 |    -1.08945  |   1.03675   |  -1.05084 | 0.293332    |                |\n| item_price_obs[constant]_4 |    -0.37895  |   0.100705  |  -3.76299 | 0.000167895 | ***            |\n| item_price_obs[constant]_5 |     0.249572 |   0.0518543 |   4.81295 | 1.4872e-06  | ***            |\n| item_price_obs[constant]_6 |    -5.99973  |   4.82952   |  -1.2423  | 0.214124    |                |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#r-output","title":"R Output","text":"<p>Here we provide the output from <code>mlogit</code> model in <code>R</code> for estimation reference.</p> <p>Coefficient names reported are slightly different in <code>Python</code> and <code>R</code>, please use the following table for comparison. Please note that the <code>lambda_weight_0</code> in <code>Python</code> (at the top) corresponds to the <code>iv</code> (inclusive value) in <code>R</code> (at the bottom). Orderings of coefficients for observables should be the same in both languages.</p> Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling <pre><code>## \n## Call:\n## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n##     inc.cooling + int.cooling | 0, data = HC, nests = list(cooling = c(\"gcc\", \n##     \"ecc\", \"erc\", \"hpc\"), other = c(\"gc\", \"ec\", \"er\")), un.nest.el = TRUE)\n## \n## Frequencies of alternatives:choice\n##    ec   ecc    er   erc    gc   gcc   hpc \n## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n## \n## bfgs method\n## 11 iterations, 0h:0m:0s \n## g'(-H)^-1g = 7.26E-06 \n## successive function values within tolerance limits \n## \n## Coefficients :\n##              Estimate Std. Error z-value  Pr(&gt;|z|)    \n## ich         -0.554878   0.144205 -3.8478 0.0001192 ***\n## och         -0.857886   0.255313 -3.3601 0.0007791 ***\n## icca        -0.225079   0.144423 -1.5585 0.1191212    \n## occa        -1.089458   1.219821 -0.8931 0.3717882    \n## inc.room    -0.378971   0.099631 -3.8038 0.0001425 ***\n## inc.cooling  0.249575   0.059213  4.2149 2.499e-05 ***\n## int.cooling -6.000415   5.562423 -1.0787 0.2807030    \n## iv           0.585922   0.179708  3.2604 0.0011125 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Log-Likelihood: -178.12\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#example-2","title":"Example 2","text":"<p>The second example is similar to the first one, but we change the way we group items into different nests. Re-estimate the model with the room alternatives in one nest and the central alternatives in another nest. (Note that a heat pump is a central system.)</p> <pre><code>nest_to_item = {0: ['ec', 'ecc', 'gc', 'gcc', 'hpc'],\n                    1: ['er', 'erc']}\nfor k, v in nest_to_item.items():\n    v = [encoder[item] for item in v]\n    nest_to_item[k] = sorted(v)\n\n# these two initializations are equivalent.\nmodel = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_coef_variation_dict={},\n                         nest_num_param_dict={},\n                         item_coef_variation_dict={'price_obs': 'constant'},\n                         item_num_param_dict={'price_obs': 7},\n                         shared_lambda=True)\nprint(model)\n\nmodel = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_formula='',\n                         item_formula='(price_obs|constant)',\n                         dataset=dataset,\n                         shared_lambda=True)\nprint(model)\nmodel = model.to(DEVICE)\n</code></pre> <pre><code>NestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre> <pre><code>run(model, dataset, num_epochs=1000, model_optimizer=\"LBFGS\", learning_rate=0.3)\n</code></pre> <pre><code>==================== model received ====================\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n==================== data set received ====================\n[Train dataset] JointDataset with 2 sub-datasets: (\n    nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu)\n)\n[Validation dataset] None\n[Test dataset] None\n\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type             | Params\n-------------------------------------------\n0 | model | NestedLogitModel | 8     \n-------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 116.29it/s, loss=180, v_num=30]\n\n`Trainer.fit` stopped: `max_epochs=1000` reached.\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 107.92it/s, loss=180, v_num=30]\nTime taken for training: 7.460475206375122\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -180.02308654785156, [Validation] N/A, [Test] N/A\n\n| Coefficient                |   Estimation |   Std. Err. |   z-value |   Pr(&gt;|z|) | Significance   |\n|:---------------------------|-------------:|------------:|----------:|-----------:|:---------------|\n| lambda_weight_0            |     1.3621   |    0.55502  |   2.45415 | 0.0141217  | *              |\n| item_price_obs[constant]_0 |    -1.13826  |    0.444239 |  -2.56226 | 0.0103993  | *              |\n| item_price_obs[constant]_1 |    -1.82546  |    0.738092 |  -2.47321 | 0.0133906  | *              |\n| item_price_obs[constant]_2 |    -0.337469 |    0.20258  |  -1.66585 | 0.0957429  |                |\n| item_price_obs[constant]_3 |    -2.06347  |    1.76159  |  -1.17136 | 0.241453   |                |\n| item_price_obs[constant]_4 |    -0.757264 |    0.278476 |  -2.71931 | 0.00654181 | **             |\n| item_price_obs[constant]_5 |     0.416903 |    0.170012 |   2.4522  | 0.0141987  | *              |\n| item_price_obs[constant]_6 |   -13.8256   |    8.09395  |  -1.70814 | 0.0876098  |                |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#r-output_1","title":"R Output","text":"<p>You can use the table for converting coefficient names reported by <code>Python</code> and <code>R</code>:</p> Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling <pre><code>## \n## Call:\n## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n##     inc.cooling + int.cooling | 0, data = HC, nests = list(central = c(\"ec\", \n##     \"ecc\", \"gc\", \"gcc\", \"hpc\"), room = c(\"er\", \"erc\")), un.nest.el = TRUE)\n## \n## Frequencies of alternatives:choice\n##    ec   ecc    er   erc    gc   gcc   hpc \n## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n## \n## bfgs method\n## 10 iterations, 0h:0m:0s \n## g'(-H)^-1g = 5.87E-07 \n## gradient close to zero \n## \n## Coefficients :\n##              Estimate Std. Error z-value Pr(&gt;|z|)  \n## ich          -1.13818    0.54216 -2.0993  0.03579 *\n## och          -1.82532    0.93228 -1.9579  0.05024 .\n## icca         -0.33746    0.26934 -1.2529  0.21024  \n## occa         -2.06328    1.89726 -1.0875  0.27681  \n## inc.room     -0.75722    0.34292 -2.2081  0.02723 *\n## inc.cooling   0.41689    0.20742  2.0099  0.04444 *\n## int.cooling -13.82487    7.94031 -1.7411  0.08167 .\n## iv            1.36201    0.65393  2.0828  0.03727 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Log-Likelihood: -180.02\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#example-3","title":"Example 3","text":"<p>For the third example, we now group items into three nests. Specifically, we have items <code>gcc</code>, <code>ecc</code> and <code>erc</code> in the first nest (nest <code>0</code> in the <code>nest_to_item</code> dictionary), <code>hpc</code> in a nest (nest <code>1</code>) alone, and items <code>gc</code>, <code>ec</code> and <code>er</code> in the last nest (nest <code>2</code>).</p> <pre><code>nest_to_item = {0: ['gcc', 'ecc', 'erc'],\n                1: ['hpc'],\n                2: ['gc', 'ec', 'er']}\nfor k, v in nest_to_item.items():\n    v = [encoder[item] for item in v]\n    nest_to_item[k] = sorted(v)\n\nmodel = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_coef_variation_dict={},\n                         nest_num_param_dict={},\n                         item_coef_variation_dict={'price_obs': 'constant'},\n                         item_num_param_dict={'price_obs': 7},\n                         shared_lambda=True)\n\nmodel = NestedLogitModel(nest_to_item=nest_to_item,\n                         nest_formula='',\n                         item_formula='(price_obs|constant)',\n                         dataset=dataset,\n                         shared_lambda=True)\n\nmodel = model.to(DEVICE)\n</code></pre> <pre><code>run(model, dataset, num_epochs=1000, model_optimizer=\"LBFGS\", learning_rate=0.3)\n</code></pre> <pre><code>==================== model received ====================\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n==================== data set received ====================\n[Train dataset] JointDataset with 2 sub-datasets: (\n    nest: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cpu)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cpu)\n)\n[Validation dataset] None\n[Test dataset] None\n\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type             | Params\n-------------------------------------------\n0 | model | NestedLogitModel | 8     \n-------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 142.92it/s, loss=180, v_num=31]\n\n`Trainer.fit` stopped: `max_epochs=1000` reached.\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 126.62it/s, loss=180, v_num=31]\nTime taken for training: 7.50447416305542\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -180.26324462890625, [Validation] N/A, [Test] N/A\n\n| Coefficient                |   Estimation |   Std. Err. |   z-value |    Pr(&gt;|z|) | Significance   |\n|:---------------------------|-------------:|------------:|----------:|------------:|:---------------|\n| lambda_weight_0            |     0.956541 |   0.197062  |   4.854   | 1.20994e-06 | ***            |\n| item_price_obs[constant]_0 |    -0.838394 |   0.099096  |  -8.46042 | 0           | ***            |\n| item_price_obs[constant]_1 |    -1.3316   |   0.184895  |  -7.2019  | 5.93747e-13 | ***            |\n| item_price_obs[constant]_2 |    -0.25613  |   0.1263    |  -2.02795 | 0.0425655   | *              |\n| item_price_obs[constant]_3 |    -1.40566  |   1.14467   |  -1.22801 | 0.219443    |                |\n| item_price_obs[constant]_4 |    -0.571352 |   0.0750316 |  -7.61482 | 2.64233e-14 | ***            |\n| item_price_obs[constant]_5 |     0.311357 |   0.0550892 |   5.65187 | 1.58715e-08 | ***            |\n| item_price_obs[constant]_6 |   -10.4134   |   5.19301   |  -2.00528 | 0.0449335   | *              |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nNestedLogitModel(\n  (nest_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs[constant]): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre>"},{"location":"nested_logit_model_house_cooling/#r-output_2","title":"R Output","text":"<p>You can use the table for converting coefficient names reported by <code>Python</code> and <code>R</code>:</p> Coefficient (Python) Coefficient (R) lambda_weight_0 iv item_price_obs_0 ich item_price_obs_1 och item_price_obs_2 icca item_price_obs_3 occa item_price_obs_4 inc.room item_price_obs_5 inc.cooling item_price_obs_6 int.cooling <pre><code>## \n## Call:\n## mlogit(formula = depvar ~ ich + och + icca + occa + inc.room + \n##     inc.cooling + int.cooling | 0, data = HC, nests = list(n1 = c(\"gcc\", \n##     \"ecc\", \"erc\"), n2 = c(\"hpc\"), n3 = c(\"gc\", \"ec\", \"er\")), \n##     un.nest.el = TRUE)\n## \n## Frequencies of alternatives:choice\n##    ec   ecc    er   erc    gc   gcc   hpc \n## 0.004 0.016 0.032 0.004 0.096 0.744 0.104 \n## \n## bfgs method\n## 8 iterations, 0h:0m:0s \n## g'(-H)^-1g = 3.71E-08 \n## gradient close to zero \n## \n## Coefficients :\n##               Estimate Std. Error z-value  Pr(&gt;|z|)    \n## ich          -0.838394   0.100546 -8.3384 &lt; 2.2e-16 ***\n## och          -1.331598   0.252069 -5.2827 1.273e-07 ***\n## icca         -0.256131   0.145564 -1.7596   0.07848 .  \n## occa         -1.405656   1.207281 -1.1643   0.24430    \n## inc.room     -0.571352   0.077950 -7.3297 2.307e-13 ***\n## inc.cooling   0.311355   0.056357  5.5247 3.301e-08 ***\n## int.cooling -10.413384   5.612445 -1.8554   0.06354 .  \n## iv            0.956544   0.180722  5.2929 1.204e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Log-Likelihood: -180.26\n</code></pre>"},{"location":"optimizer/","title":"Tutorial: Optimization Algorithms","text":"<p>Author: Tianyu Du (tianyudu@stanford.edu)</p> <p>Update: May. 14, 2023</p> <p>Let's first import essential Python packages.</p> <pre><code>import pandas as pd\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_choice.data import ChoiceDataset, utils\nfrom torch_choice.model import ConditionalLogitModel\n\nfrom torch_choice import run\n</code></pre> <pre><code>/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: &lt;B3E58761-2785-34C6-A89B-F37110C88A05&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;AE6DCE26-A528-35ED-BB3D-88890D27E6B9&gt; /Users/tianyudu/miniforge3/envs/dev/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n  warn(f\"Failed to load image Python extension: {e}\")\n</code></pre> <pre><code>print(torch.__version__)\nprint(f\"{torch.cuda.is_available()=:}\")\n</code></pre> <pre><code>2.0.0\ntorch.cuda.is_available()=False\n</code></pre> <pre><code>if torch.cuda.is_available():\n    print(f'CUDA device used: {torch.cuda.get_device_name()}')\n    device = 'cuda'\nelse:\n    print('Running tutorial on CPU.')\n    device = 'cpu'\n</code></pre> <pre><code>Running tutorial on CPU.\n</code></pre> <pre><code>df = pd.read_csv('./public_datasets/ModeCanada.csv')\ndf = df.query('noalt == 4').reset_index(drop=True)\ndf.sort_values(by='case', inplace=True)\ndf.head()\nitem_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\nitem_names = ['air', 'bus', 'car', 'train']\nnum_items = 4\nencoder = dict(zip(item_names, range(num_items)))\nitem_index = item_index.map(lambda x: encoder[x])\nitem_index = torch.LongTensor(item_index)\nprice_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt',\n                                    values=['cost', 'freq', 'ovt'])\n\nprice_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\nsession_income = df.groupby('case')['income'].first()\nsession_income = torch.Tensor(session_income.values).view(-1, 1)\n\ndataset = ChoiceDataset(item_index=item_index,\n                        price_cost_freq_ovt=price_cost_freq_ovt,\n                        session_income=session_income,\n                        price_ivt=price_ivt\n                        ).to(device)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n</code></pre> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)\n</code></pre> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = \"cpu\"\nmodel = ConditionalLogitModel(\n    formula='(price_cost_freq_ovt|constant) + (session_income|item) + (price_ivt|item-full) + (intercept|item)',\n    dataset=dataset,\n    num_items=4).to(device)\nrun(model, dataset, num_epochs=500, learning_rate=0.01, batch_size=-1, model_optimizer=\"LBFGS\", device=device)\n</code></pre> <pre><code>GPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type                  | Params\n------------------------------------------------\n0 | model | ConditionalLogitModel | 13    \n------------------------------------------------\n13        Trainable params\n0         Non-trainable params\n13        Total params\n0.000     Total estimated model params size (MB)\n\n\n==================== model received ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[price_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n==================== data set received ====================\n[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cpu)\n[Validation dataset] None\n[Test dataset] None\nEpoch 499: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 40.10it/s, loss=1.87e+03, v_num=15]\n\n`Trainer.fit` stopped: `max_epochs=500` reached.\n\n\nEpoch 499: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 38.63it/s, loss=1.87e+03, v_num=15]\nTime taken for training: 12.536703109741211\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -1874.3427734375, [Validation] N/A, [Test] N/A\n\n| Coefficient                     |   Estimation |   Std. Err. |    z-value |    Pr(&gt;|z|) | Significance   |\n|:--------------------------------|-------------:|------------:|-----------:|------------:|:---------------|\n| price_cost_freq_ovt[constant]_0 |  -0.0333376  |  0.00709551 |  -4.69841  | 2.62196e-06 | ***            |\n| price_cost_freq_ovt[constant]_1 |   0.0925288  |  0.00509756 |  18.1516   | 0           | ***            |\n| price_cost_freq_ovt[constant]_2 |  -0.0430023  |  0.0032247  | -13.3353   | 0           | ***            |\n| session_income[item]_0          |  -0.0891035  |  0.018348   |  -4.85631  | 1.19595e-06 | ***            |\n| session_income[item]_1          |  -0.0279937  |  0.00387255 |  -7.22876  | 4.87388e-13 | ***            |\n| session_income[item]_2          |  -0.038145   |  0.00408308 |  -9.34222  | 0           | ***            |\n| price_ivt[item-full]_0          |   0.059507   |  0.0100727  |   5.90777  | 3.46776e-09 | ***            |\n| price_ivt[item-full]_1          |  -0.00678584 |  0.00443389 |  -1.53045  | 0.125905    |                |\n| price_ivt[item-full]_2          |  -0.00646072 |  0.00189849 |  -3.40309  | 0.000666291 | ***            |\n| price_ivt[item-full]_3          |  -0.00145041 |  0.00118748 |  -1.22142  | 0.221927    |                |\n| intercept[item]_0               |   0.699403   |  1.28026    |   0.546298 | 0.584861    |                |\n| intercept[item]_1               |   1.84431    |  0.708509   |   2.60309  | 0.00923886  | **             |\n| intercept[item]_2               |   3.2741     |  0.624415   |   5.24347  | 1.57586e-07 | ***            |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (price_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[price_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre>"},{"location":"optimizer/#parameter-estimation-from-r","title":"Parameter Estimation from <code>R</code>","text":"<p>The following is the R-output from the <code>mlogit</code> implementation, the estimation, standard error, and log-likelihood from our <code>torch_choice</code> implementation is the same as the result from <code>mlogit</code> implementation.</p> <p>We see that the final log-likelihood of models estimated using two packages are all around <code>-1874</code>.</p> <p>The <code>run()</code> method calculates the standard deviation using \\(\\sqrt{\\text{diag}(H^{-1})}\\), where \\(H\\) is the hessian of negative log-likelihood with repsect to model parameters.</p> <p>Names of coefficients are slightly different, one can use the following conversion table to compare estimations and standard deviations reported by both packages.</p>"},{"location":"optimizer/#r-output","title":"R Output","text":"<p><pre><code>install.packages(\"mlogit\")\nlibrary(\"mlogit\")\ndata(\"ModeCanada\", package = \"mlogit\")\nMC &lt;- dfidx(ModeCanada, subset = noalt == 4)\nml.MC1 &lt;- mlogit(choice ~ cost + freq + ovt | income | ivt, MC, reflevel='air')\n\nsummary(ml.MC1)\n</code></pre> <pre><code>Call:\nmlogit(formula = choice ~ cost + freq + ovt | income | ivt, data = MC, \n    reflevel = \"air\", method = \"nr\")\n\nFrequencies of alternatives:choice\n      air     train       bus       car \n0.3738755 0.1666067 0.0035984 0.4559194 \n\nnr method\n9 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00014 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                    Estimate Std. Error  z-value  Pr(&gt;|z|)    \n(Intercept):train  3.2741952  0.6244152   5.2436 1.575e-07 ***\n(Intercept):bus    0.6983381  1.2802466   0.5455 0.5854292    \n(Intercept):car    1.8441129  0.7085089   2.6028 0.0092464 ** \ncost              -0.0333389  0.0070955  -4.6986 2.620e-06 ***\nfreq               0.0925297  0.0050976  18.1517 &lt; 2.2e-16 ***\novt               -0.0430036  0.0032247 -13.3356 &lt; 2.2e-16 ***\nincome:train      -0.0381466  0.0040831  -9.3426 &lt; 2.2e-16 ***\nincome:bus        -0.0890867  0.0183471  -4.8556 1.200e-06 ***\nincome:car        -0.0279930  0.0038726  -7.2286 4.881e-13 ***\nivt:air            0.0595097  0.0100727   5.9080 3.463e-09 ***\nivt:train         -0.0014504  0.0011875  -1.2214 0.2219430    \nivt:bus           -0.0067835  0.0044334  -1.5301 0.1259938    \nivt:car           -0.0064603  0.0018985  -3.4029 0.0006668 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nLog-Likelihood: -1874.3\nMcFadden R^2:  0.35443 \nLikelihood ratio test : chisq = 2058.1 (p.value = &lt; 2.22e-16)\n</code></pre></p> <pre><code>\n</code></pre>"},{"location":"paper_demo/","title":"Replication Materials for the Torch-Choice Paper","text":"<p>Author: Tianyu Du</p> <p>Email: <code>tianyudu@stanford.edu</code></p> <p>This repository contains the replication materials for the paper \"Torch-Choice: A Library for Choice Models in PyTorch\". Due to the limited space in the main paper, we have omitted some codes and outputs in the paper. This repository contains the full version of codes mentioned in the paper.</p> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch_choice\nfrom torch_choice import run\nfrom tqdm import tqdm\nfrom torch_choice.data import ChoiceDataset, JointDataset, utils, load_mode_canada_dataset, load_house_cooling_dataset_v1\nfrom torch_choice.model import ConditionalLogitModel, NestedLogitModel\n</code></pre> <pre><code>torch_choice.__version__\n</code></pre> <pre><code>'1.0.3'\n</code></pre>"},{"location":"paper_demo/#data-structure","title":"Data Structure","text":"<pre><code>car_choice = pd.read_csv(\"https://raw.githubusercontent.com/gsbDBI/torch-choice/main/tutorials/public_datasets/car_choice.csv\")\ncar_choice.head()\n</code></pre> record_id session_id consumer_id car purchase gender income speed discount price 0 1 1 1 American 1 1 46.699997 10 0.94 90 1 1 1 1 Japanese 0 1 46.699997 8 0.94 110 2 1 1 1 European 0 1 46.699997 7 0.94 50 3 1 1 1 Korean 0 1 46.699997 8 0.94 10 4 2 2 2 American 1 1 26.100000 10 0.95 100"},{"location":"paper_demo/#adding-observables-method-1-observables-derived-from-columns-of-the-main-dataset","title":"Adding Observables, Method 1: Observables Derived from Columns of the Main Dataset","text":"<pre><code>user_observable_columns=[\"gender\", \"income\"]\nfrom torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper\ndata_wrapper_from_columns = EasyDatasetWrapper(\n    main_data=car_choice,\n    purchase_record_column='record_id',\n    choice_column='purchase',\n    item_name_column='car',\n    user_index_column='consumer_id',\n    session_index_column='session_id',\n    user_observable_columns=['gender', 'income'],\n    item_observable_columns=['speed'],\n    session_observable_columns=['discount'],\n    itemsession_observable_columns=['price'])\n\ndata_wrapper_from_columns.summary()\ndataset = data_wrapper_from_columns.choice_dataset\n# ChoiceDataset(label=[], item_index=[885], provided_num_items=[], user_index=[885], session_index=[885], item_availability=[885, 4], item_speed=[4, 1], user_gender=[885, 1], user_income=[885, 1], session_discount=[885, 1], itemsession_price=[885, 4, 1], device=cpu)\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n* purchase record index range: [1 2 3] ... [883 884 885]\n* Space of 4 items:\n                   0         1         2       3\nitem name  American  European  Japanese  Korean\n* Number of purchase records/cases: 885.\n* Preview of main data frame:\n      record_id  session_id  consumer_id       car  purchase  gender  \\\n0             1           1            1  American         1       1   \n1             1           1            1  Japanese         0       1   \n2             1           1            1  European         0       1   \n3             1           1            1    Korean         0       1   \n4             2           2            2  American         1       1   \n...         ...         ...          ...       ...       ...     ...   \n3155        884         884          884  Japanese         1       1   \n3156        884         884          884  European         0       1   \n3157        885         885          885  American         1       1   \n3158        885         885          885  Japanese         0       1   \n3159        885         885          885  European         0       1\n\n         income  speed  discount  price  \n0     46.699997     10      0.94     90  \n1     46.699997      8      0.94    110  \n2     46.699997      7      0.94     50  \n3     46.699997      8      0.94     10  \n4     26.100000     10      0.95    100  \n...         ...    ...       ...    ...  \n3155  20.900000      8      0.89    100  \n3156  20.900000      7      0.89     40  \n3157  30.600000     10      0.81    100  \n3158  30.600000      8      0.81     50  \n3159  30.600000      7      0.81     40\n\n[3160 rows x 10 columns]\n* Preview of ChoiceDataset:\nChoiceDataset(label=[], item_index=[885], user_index=[885], session_index=[885], item_availability=[885, 4], item_speed=[4, 1], user_gender=[885, 1], user_income=[885, 1], session_discount=[885, 1], itemsession_price=[885, 4, 1], device=cpu)\n</code></pre>"},{"location":"paper_demo/#adding-observables-method-2-added-as-separated-dataframes","title":"Adding Observables, Method 2: Added as Separated DataFrames","text":"<pre><code># create dataframes for gender and income. The dataframe for user-specific observable needs to have the `consumer_id` column.\ngender = car_choice.groupby('consumer_id')['gender'].first().reset_index()\nincome = car_choice.groupby('consumer_id')['income'].first().reset_index()\n# alternatively, put gender and income in the same dataframe.\ngender_and_income = car_choice.groupby('consumer_id')[['gender', 'income']].first().reset_index()\n# speed as item observable, the dataframe requires a `car` column.\nspeed = car_choice.groupby('car')['speed'].first().reset_index()\n# discount as session observable. the dataframe requires a `session_id` column.\ndiscount = car_choice.groupby('session_id')['discount'].first().reset_index()\n# create the price as itemsession observable, the dataframe requires both `car` and `session_id` columns.\nprice = car_choice[['car', 'session_id', 'price']]\n# fill in NANs for (session, item) pairs that the item was not available in that session.\nprice = price.pivot('car', 'session_id', 'price').melt(ignore_index=False).reset_index()\n</code></pre> <pre><code>data_wrapper_from_dataframes = EasyDatasetWrapper(\n    main_data=car_choice,\n    purchase_record_column='record_id',\n    choice_column='purchase',\n    item_name_column='car',\n    user_index_column='consumer_id',\n    session_index_column='session_id',\n    user_observable_data={'gender': gender, 'income': income},\n    # alternatively, supply gender and income as a single dataframe.\n    # user_observable_data={'gender_and_income': gender_and_income},\n    item_observable_data={'speed': speed},\n    session_observable_data={'discount': discount},\n    itemsession_observable_data={'price': price})\n\n# the second method creates exactly the same ChoiceDataset as the previous method.\nassert data_wrapper_from_dataframes.choice_dataset == data_wrapper_from_columns.choice_dataset\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre> <pre><code>data_wrapper_mixed = EasyDatasetWrapper(\n    main_data=car_choice,\n    purchase_record_column='record_id',\n    choice_column='purchase',\n    item_name_column='car',\n    user_index_column='consumer_id',\n    session_index_column='session_id',\n    user_observable_data={'gender': gender, 'income': income},\n    item_observable_data={'speed': speed},\n    session_observable_data={'discount': discount},\n    itemsession_observable_columns=['price'])\n\n# these methods create exactly the same choice dataset.\nassert data_wrapper_mixed.choice_dataset == data_wrapper_from_columns.choice_dataset == data_wrapper_from_dataframes.choice_dataset\n</code></pre> <pre><code>Creating choice dataset from stata format data-frames...\nNote: choice sets of different sizes found in different purchase records: {'size 4': 'occurrence 505', 'size 3': 'occurrence 380'}\nFinished Creating Choice Dataset.\n</code></pre>"},{"location":"paper_demo/#constructing-a-choice-dataset-method-2-building-from-tensors","title":"Constructing a Choice Dataset, Method 2: Building from Tensors","text":"<pre><code>N = 10_000\nnum_users = 10\nnum_items = 4\nnum_sessions = 500\n\n\nuser_obs = torch.randn(num_users, 128)\nitem_obs = torch.randn(num_items, 64)\nuseritem_obs = torch.randn(num_users, num_items, 32)\nsession_obs = torch.randn(num_sessions, 10)\nitemsession_obs = torch.randn(num_sessions, num_items, 12)\nusersessionitem_obs = torch.randn(num_users, num_sessions, num_items, 8)\n\nitem_index = torch.LongTensor(np.random.choice(num_items, size=N))\nuser_index = torch.LongTensor(np.random.choice(num_users, size=N))\nsession_index = torch.LongTensor(np.random.choice(num_sessions, size=N))\nitem_availability = torch.ones(num_sessions, num_items).bool()\n\ndataset = ChoiceDataset(\n    # required:\n    item_index=item_index,\n    # optional:\n    user_index=user_index, session_index=session_index, item_availability=item_availability,\n    # observable tensors are supplied as keyword arguments with special prefixes.\n    user_obs=user_obs, item_obs=item_obs, useritem_obs=useritem_obs, session_obs=session_obs, itemsession_obs=itemsession_obs, usersessionitem_obs=usersessionitem_obs)\n</code></pre> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], itemsession_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 8], device=cpu)\n</code></pre>"},{"location":"paper_demo/#functionalities-of-the-choice-dataset","title":"Functionalities of the Choice Dataset","text":"<pre><code>print(f'{dataset.num_users=:}')\n# dataset.num_users=10\nprint(f'{dataset.num_items=:}')\n# dataset.num_items=4\nprint(f'{dataset.num_sessions=:}')\n# dataset.num_sessions=500\nprint(f'{len(dataset)=:}')\n# len(dataset)=10000\n</code></pre> <pre><code>dataset.num_users=10\ndataset.num_items=4\ndataset.num_sessions=500\nlen(dataset)=10000\n</code></pre> <pre><code># clone\nprint(dataset.item_index[:10])\n# tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])\ndataset_cloned = dataset.clone()\n# modify the cloned dataset.\ndataset_cloned.item_index = 99 * torch.ones(num_sessions)\nprint(dataset_cloned.item_index[:10])\n# the cloned dataset is changed.\n# tensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.])\nprint(dataset.item_index[:10])\n# the original dataset does not change.\n# tensor([2, 2, 3, 1, 3, 2, 2, 1, 0, 1])\n</code></pre> <pre><code>tensor([0, 1, 3, 1, 2, 0, 3, 2, 3, 1])\ntensor([99., 99., 99., 99., 99., 99., 99., 99., 99., 99.])\ntensor([0, 1, 3, 1, 2, 0, 3, 2, 3, 1])\n</code></pre> <pre><code># move to device\nprint(f'{dataset.device=:}')\n# dataset.device=cpu\nprint(f'{dataset.device=:}')\n# dataset.device=cpu\nprint(f'{dataset.user_index.device=:}')\n# dataset.user_index.device=cpu\nprint(f'{dataset.session_index.device=:}')\n# dataset.session_index.device=cpu\n\n\nif torch.cuda.is_available():\n    # please note that this can only be demonstrated \n    dataset = dataset.to('cuda')\n\n    print(f'{dataset.device=:}')\n    # dataset.device=cuda:0\n    print(f'{dataset.item_index.device=:}')\n    # dataset.item_index.device=cuda:0\n    print(f'{dataset.user_index.device=:}')\n    # dataset.user_index.device=cuda:0\n    print(f'{dataset.session_index.device=:}')\n    # dataset.session_index.device=cuda:0\n\n    dataset._check_device_consistency()\n</code></pre> <pre><code>dataset.device=cpu\ndataset.device=cpu\ndataset.user_index.device=cpu\ndataset.session_index.device=cpu\n</code></pre> <pre><code>def print_dict_shape(d):\n    for key, val in d.items():\n        if torch.is_tensor(val):\n            print(f'dict.{key}.shape={val.shape}')\nprint_dict_shape(dataset.x_dict)\n</code></pre> <pre><code>dict.user_obs.shape=torch.Size([10000, 4, 128])\ndict.item_obs.shape=torch.Size([10000, 4, 64])\ndict.useritem_obs.shape=torch.Size([10000, 4, 32])\ndict.session_obs.shape=torch.Size([10000, 4, 10])\ndict.itemsession_obs.shape=torch.Size([10000, 4, 12])\ndict.usersessionitem_obs.shape=torch.Size([10000, 4, 8])\n</code></pre> <pre><code># __getitem__ to get batch.\n# pick 5 random sessions as the mini-batch.\ndataset = dataset.to('cpu')\nindices = torch.Tensor(np.random.choice(len(dataset), size=5, replace=False)).long()\nprint(indices)\n# tensor([1118,  976, 1956,  290, 8283])\nsubset = dataset[indices]\nprint(dataset)\n# ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\nprint(subset)\n# ChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], session_obs=[500, 10], price_obs=[500, 4, 12], device=cpu)\n</code></pre> <pre><code>tensor([6419, 3349, 6741, 3078, 6424])\nChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], itemsession_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 8], device=cpu)\nChoiceDataset(label=[], item_index=[5], user_index=[5], session_index=[5], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], itemsession_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 8], device=cpu)\n</code></pre> <pre><code>print(subset.item_index)\n# tensor([0, 1, 0, 0, 0])\nprint(dataset.item_index[indices])\n# tensor([0, 1, 0, 0, 0])\n\nsubset.item_index += 1  # modifying the batch does not change the original dataset.\n\nprint(subset.item_index)\n# tensor([1, 2, 1, 1, 1])\nprint(dataset.item_index[indices])\n# tensor([0, 1, 0, 0, 0])\n</code></pre> <pre><code>tensor([2, 1, 1, 0, 0])\ntensor([2, 1, 1, 0, 0])\ntensor([3, 2, 2, 1, 1])\ntensor([2, 1, 1, 0, 0])\n</code></pre> <pre><code>print(subset.item_obs[0, 0])\n# tensor(-1.5811)\nprint(dataset.item_obs[0, 0])\n# tensor(-1.5811)\n\nsubset.item_obs += 1\nprint(subset.item_obs[0, 0])\n# tensor(-0.5811)\nprint(dataset.item_obs[0, 0])\n# tensor(-1.5811)\n</code></pre> <pre><code>tensor(0.1007)\ntensor(0.1007)\ntensor(1.1007)\ntensor(0.1007)\n</code></pre> <pre><code>print(id(subset.item_index))\n# 140339656298640\nprint(id(dataset.item_index[indices]))\n# 140339656150528\n# these two are different objects in memory.\n</code></pre> <pre><code>11458049504\n11458562704\n</code></pre>"},{"location":"paper_demo/#chaining-multiple-datasets-with-jointdataset","title":"Chaining Multiple Datasets with JointDataset","text":"<pre><code>item_level_dataset = dataset.clone()\nnest_level_dataset = dataset.clone()\njoint_dataset = JointDataset(\n    item=item_level_dataset,\n    nest=nest_level_dataset)\n\nprint(joint_dataset)\n</code></pre> <pre><code>JointDataset with 2 sub-datasets: (\n    item: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], itemsession_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 8], device=cpu)\n    nest: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 4], user_obs=[10, 128], item_obs=[4, 64], useritem_obs=[10, 4, 32], session_obs=[500, 10], itemsession_obs=[500, 4, 12], usersessionitem_obs=[10, 500, 4, 8], device=cpu)\n)\n</code></pre> <pre><code>from torch.utils.data.sampler import BatchSampler, SequentialSampler, RandomSampler\nshuffle = False  # for demonstration purpose.\nbatch_size = 32\n\n# Create sampler.\nsampler = BatchSampler(\n    RandomSampler(dataset) if shuffle else SequentialSampler(dataset),\n    batch_size=batch_size,\n    drop_last=False)\n\ndataloader = torch.utils.data.DataLoader(dataset,\n                                         sampler=sampler,\n                                         collate_fn=lambda x: x[0],\n                                         pin_memory=(dataset.device == 'cpu'))\n</code></pre> <pre><code>print(f'{item_obs.shape=:}')\n# item_obs.shape=torch.Size([4, 64])\nitem_obs_all = item_obs.view(1, num_items, -1).expand(len(dataset), -1, -1)\nitem_obs_all = item_obs_all.to(dataset.device)\nitem_index_all = item_index.to(dataset.device)\nprint(f'{item_obs_all.shape=:}')\n# item_obs_all.shape=torch.Size([10000, 4, 64])\n</code></pre> <pre><code>item_obs.shape=torch.Size([4, 64])\nitem_obs_all.shape=torch.Size([10000, 4, 64])\n</code></pre> <pre><code>for i, batch in enumerate(dataloader):\n    first, last = i * batch_size, min(len(dataset), (i + 1) * batch_size)\n    idx = torch.arange(first, last)\n    assert torch.all(item_obs_all[idx, :, :] == batch.x_dict['item_obs'])\n    assert torch.all(item_index_all[idx] == batch.item_index)\n</code></pre> <pre><code>batch.x_dict['item_obs'].shape\n# torch.Size([16, 4, 64])\n</code></pre> <pre><code>torch.Size([16, 4, 64])\n</code></pre> <pre><code>print_dict_shape(dataset.x_dict)\n# dict.user_obs.shape=torch.Size([10000, 4, 128])\n# dict.item_obs.shape=torch.Size([10000, 4, 64])\n# dict.session_obs.shape=torch.Size([10000, 4, 10])\n# dict.price_obs.shape=torch.Size([10000, 4, 12])\n</code></pre> <pre><code>dict.user_obs.shape=torch.Size([10000, 4, 128])\ndict.item_obs.shape=torch.Size([10000, 4, 64])\ndict.useritem_obs.shape=torch.Size([10000, 4, 32])\ndict.session_obs.shape=torch.Size([10000, 4, 10])\ndict.itemsession_obs.shape=torch.Size([10000, 4, 12])\ndict.usersessionitem_obs.shape=torch.Size([10000, 4, 8])\n</code></pre> <pre><code>dataset.__len__()\n# 10000\n</code></pre> <pre><code>10000\n</code></pre>"},{"location":"paper_demo/#conditional-logit-model","title":"Conditional Logit Model","text":"<pre><code>dataset = load_mode_canada_dataset() \n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n</code></pre> <pre><code>dataset\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], itemsession_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], itemsession_ivt=[2779, 4, 1], device=cpu)\n</code></pre> <pre><code>model = ConditionalLogitModel(\n    formula='(itemsession_cost_freq_ovt|constant) + (session_income|item) + (itemsession_ivt|item-full) + (intercept|item)',\n    dataset=dataset,\n    num_items=4)\n</code></pre> <pre><code>model = ConditionalLogitModel(\n    coef_variation_dict={'itemsession_cost_freq_ovt': 'constant',\n                         'session_income': 'item',\n                         'itemsession_ivt': 'item-full',\n                         'intercept': 'item'},\n    num_param_dict={'itemsession_cost_freq_ovt': 3,\n                    'session_income': 1,\n                    'itemsession_ivt': 1,\n                    'intercept': 1},\n    num_items=4)\n</code></pre> <pre><code>model = ConditionalLogitModel(\n    coef_variation_dict={'itemsession_cost_freq_ovt': 'constant',\n                         'session_income': 'item',\n                         'itemsession_ivt': 'item-full',\n                         'intercept': 'item'},\n    num_param_dict={'itemsession_cost_freq_ovt': 3,\n                    'session_income': 1,\n                    'itemsession_ivt': 1,\n                    'intercept': 1},\n    num_items=4,\n    regularization=\"L1\", regularization_weight=0.5)\n</code></pre> <pre><code>from torch_choice import run\nrun(model, dataset, batch_size=-1, learning_rate=0.01, num_epochs=1000, model_optimizer=\"LBFGS\")\n</code></pre> <pre><code>GPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n==================== model received ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (itemsession_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (itemsession_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[itemsession_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[itemsession_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n==================== data set received ====================\n[Train dataset] ChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], itemsession_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], itemsession_ivt=[2779, 4, 1], device=cpu)\n[Validation dataset] None\n[Test dataset] None\n\n\n\n  | Name  | Type                  | Params\n------------------------------------------------\n0 | model | ConditionalLogitModel | 13    \n------------------------------------------------\n13        Trainable params\n0         Non-trainable params\n13        Total params\n0.000     Total estimated model params size (MB)\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 107.14it/s, loss=1.88e+03, v_num=45]\n\n`Trainer.fit` stopped: `max_epochs=1000` reached.\n\n\nEpoch 999: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 98.73it/s, loss=1.88e+03, v_num=45] \nTime taken for training: 18.987757921218872\nSkip testing, no test dataset is provided.\n==================== model results ====================\nLog-likelihood: [Training] -1874.63818359375, [Validation] N/A, [Test] N/A\n\n| Coefficient                           |   Estimation |   Std. Err. |      z-value |    Pr(&gt;|z|) | Significance   |\n|:--------------------------------------|-------------:|------------:|-------------:|------------:|:---------------|\n| itemsession_cost_freq_ovt[constant]_0 | -0.0372949   |  0.00709483 |  -5.25663    | 1.46723e-07 | ***            |\n| itemsession_cost_freq_ovt[constant]_1 |  0.0934485   |  0.00509605 |  18.3374     | 0           | ***            |\n| itemsession_cost_freq_ovt[constant]_2 | -0.0427757   |  0.00322198 | -13.2762     | 0           | ***            |\n| session_income[item]_0                | -0.0862389   |  0.0183019  |  -4.71202    | 2.4527e-06  | ***            |\n| session_income[item]_1                | -0.0269126   |  0.00384874 |  -6.99258    | 2.69873e-12 | ***            |\n| session_income[item]_2                | -0.0370584   |  0.00406312 |  -9.12069    | 0           | ***            |\n| itemsession_ivt[item-full]_0          |  0.0593796   |  0.0100867  |   5.88689    | 3.93536e-09 | ***            |\n| itemsession_ivt[item-full]_1          | -0.00634707  |  0.0042809  |  -1.48265    | 0.138168    |                |\n| itemsession_ivt[item-full]_2          | -0.00583223  |  0.00189433 |  -3.07879    | 0.00207844  | **             |\n| itemsession_ivt[item-full]_3          | -0.00137813  |  0.00118697 |  -1.16105    | 0.245622    |                |\n| intercept[item]_0                     | -9.98532e-09 |  1.26823    |  -7.8734e-09 | 1           |                |\n| intercept[item]_1                     |  1.32592     |  0.703708   |   1.88419    | 0.0595399   |                |\n| intercept[item]_2                     |  2.8192      |  0.618182   |   4.56047    | 5.10383e-06 | ***            |\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (itemsession_cost_freq_ovt[constant]): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cpu).\n    (session_income[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n    (itemsession_ivt[item-full]): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[itemsession_cost_freq_ovt[constant]] with 3 parameters, with constant level variation.\nX[session_income[item]] with 1 parameters, with item level variation.\nX[itemsession_ivt[item-full]] with 1 parameters, with item-full level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\ndevice=cpu\n</code></pre> <pre><code>! tensorboard --logdir ./lightning_logs --port 6006\n</code></pre> <pre><code>TensorFlow installation not found - running with reduced feature set.\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.12.1 at http://localhost:6006/ (Press CTRL+C to quit)\n^C\n</code></pre>"},{"location":"paper_demo/#nested-logit-model","title":"Nested Logit Model","text":"<p>The code demo for nested logit models in the paper was abstract, please refer to the nested-logit model tutorial for executable code.</p>"},{"location":"post_estimation_demos/","title":"Tutorial: Post-Estimations","text":"<p>Author: Tianyu Du (tianyudu@stanford.edu)</p> <p>This tutorial covers the toolkit in <code>torch-choice</code> for visualizing and analyzing models after model estimation.</p> <p>Note: models demonstrated in this tutorial are for demonstration purpose only, hence we don't estimate them in this tutorial. Instead, this tutorial focuses on APIs to visualize and analyze models.</p> <pre><code># import required dependencies.\nfrom time import time\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_choice.data import ChoiceDataset, JointDataset, utils\nfrom torch_choice.model import ConditionalLogitModel, NestedLogitModel\nfrom torch_choice.utils.run_helper import run\n</code></pre> <pre><code># let's get a helper\ndef print_dict_shape(d):\n    for key, val in d.items():\n        if torch.is_tensor(val):\n            print(f'dict.{key}.shape={val.shape}')\n</code></pre>"},{"location":"post_estimation_demos/#creating-choicedataset-object","title":"Creating  <code>ChoiceDataset</code> Object","text":"<p>We first create a dummy <code>ChoiceDataset</code> object, please refer to the data management tutorial for more details.</p> <pre><code># Feel free to modify it as you want.\nnum_users = 100\nnum_items = 25\nnum_sessions = 500\n\nlength_of_dataset = 10000\n# create observables/features, the number of parameters are arbitrarily chosen.\n# generate 128 features for each user, e.g., race, gender.\nuser_obs = torch.randn(num_users, 128)\n# generate 64 features for each user, e.g., quality.\nitem_obs = torch.randn(num_items, 64)\n# generate 10 features for each session, e.g., weekday indicator. \nsession_obs = torch.randn(num_sessions, 10)\n# generate 12 features for each session user pair, e.g., the budget of that user at the shopping day.\nitemsession_obs = torch.randn(num_sessions, num_items, 12)\nitem_index = torch.LongTensor(np.random.choice(num_items, size=length_of_dataset))\nuser_index = torch.LongTensor(np.random.choice(num_users, size=length_of_dataset))\nsession_index = torch.LongTensor(np.random.choice(num_sessions, size=length_of_dataset))\n# assume all items are available in all sessions.\nitem_availability = torch.ones(num_sessions, num_items).bool()\n\n# initialize a ChoiceDataset object.\ndataset = ChoiceDataset(\n    # pre-specified keywords of __init__\n    item_index=item_index,  # required.\n    # optional:\n    num_users=num_users,\n    num_items=num_items,\n    user_index=user_index,\n    session_index=session_index,\n    item_availability=item_availability,\n    # additional keywords of __init__\n    user_obs=user_obs,\n    item_obs=item_obs,\n    session_obs=session_obs,\n    itemsession_obs=itemsession_obs)\n</code></pre> <pre><code>print(dataset)\n</code></pre> <pre><code>ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu)\n</code></pre>"},{"location":"post_estimation_demos/#conditional-logit-model","title":"Conditional Logit Model","text":"<p>Suppose that we are creating a very complicated dummy model as the following. Please note that model and dataset here are for demonstration purpose only, the model is unlikely to converge if one estimate it on this dataset.</p> \\[ U_{uis} = \\alpha + \\beta_i + \\gamma_u + \\delta_i^\\top \\textbf{x}^{(user)}_u + \\eta^\\top \\textbf{y}^{(item)}_i + \\theta_u^\\top \\textbf{z}^{(session)}_{s} + \\kappa_i^\\top \\textbf{w}^{(itemsession)}_{is} + \\iota_u^\\top \\textbf{w}^{(itemsession)}_{is} + \\epsilon_{uis} \\] <pre><code>model = ConditionalLogitModel(formula='(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)',\n                              dataset=dataset,\n                              num_users=num_users,\n                              num_items=num_items)\n\n# estimate the model... omitted in this tutorial.\n</code></pre> <pre><code>model\n</code></pre> <pre><code>ConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu).\n    (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu).\n    (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu).\n    (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu).\n    (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu).\n    (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu).\n    (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[intercept[constant]] with 1 parameters, with constant level variation.\nX[intercept[item]] with 1 parameters, with item level variation.\nX[intercept[user]] with 1 parameters, with user level variation.\nX[user_obs[item]] with 128 parameters, with item level variation.\nX[item_obs[constant]] with 64 parameters, with constant level variation.\nX[session_obs[user]] with 10 parameters, with user level variation.\nX[itemsession_obs[item]] with 12 parameters, with item level variation.\nX[itemsession_obs[user]] with 12 parameters, with user level variation.\ndevice=cpu\n</code></pre>"},{"location":"post_estimation_demos/#retrieving-model-parameters-with-the-get_coefficient-method","title":"Retrieving Model Parameters with the <code>get_coefficient()</code> method.","text":"<p>In the model representation above, we can see that the model has coefficients from <code>intercept[constant]</code> to <code>itemsession_obs</code>.  The <code>get_coefficient()</code> method allows users to retrieve the coefficient values from the model using the general syntax <code>model.get_coefficient(COEFFICIENT_NAME)</code>.</p> <p>For example, <code>model.get_coefficient('intercept[constant]')</code> will return the value of \\(\\alpha\\), which is a scalar.</p> <pre><code>model.get_coefficient('intercept[constant]')\n</code></pre> <pre><code>tensor([0.3743])\n</code></pre> <p><code>model.get_coefficient('intercept[user]')</code> returns the array of \\(\\gamma_u\\)'s, which is a 1D array of length <code>num_users</code>.</p> <pre><code>model.get_coefficient('intercept[user]').shape\n</code></pre> <pre><code>torch.Size([100, 1])\n</code></pre> <p><code>model.get_coefficient('session_obs[user]')</code> returns the corresponding coefficient \\(\\theta_u\\), which is a 2D array of shape <code>(num_users, num_session_features)</code>. Each row of the returned tensor corresponds to the coefficient vector of a user.</p> <pre><code>model.get_coefficient('session_obs[user]').shape\n</code></pre> <pre><code>torch.Size([100, 10])\n</code></pre> <p>Lastly, the <code>itemsession_obs</code> (a 12-dimensional feature vector for each \\((i, s)\\) pairs) affects the utility through both \\(\\kappa_i\\) and \\(\\iota_u\\). For each item (except for the first item indexed with <code>0</code>, all coefficients of it are <code>0</code>), the <code>get_coefficient()</code> method returns a 2D array of shape <code>(num_items-1, num_itemsession_features)</code>.</p> <p>The first row of the returned tensor corresponds to the coefficient vector of the second item, and so on.</p> <p><code>model.get_coefficient('itemsession_obs[user]')</code> provides the user-specific relationship between utility and item-session observables, \\(\\iota_u\\), which is a 2D array of shape <code>(num_users, num_itemsession_features)</code>. Each row of the returned tensor corresponds to the coefficient vector of a user.</p> <pre><code>model.get_coefficient('itemsession_obs[item]').shape\n</code></pre> <pre><code>torch.Size([24, 12])\n</code></pre> <pre><code>model.get_coefficient('itemsession_obs[user]').shape\n</code></pre> <pre><code>torch.Size([100, 12])\n</code></pre>"},{"location":"post_estimation_demos/#visualizing-model-parameters","title":"Visualizing Model Parameters","text":"<p>Researchers can use any plotting library to visualize the model parameters. Here we use <code>matplotlib</code> to demonstrate how to visualize the model parameters.</p> <p>For example, we can plot the distribution of user fixed effect \\(\\gamma_u\\)'s as the following.</p> <ol> <li>Researcher can use the <code>get_coefficient()</code> method to retrieve the coefficient values. </li> </ol> <pre><code>gamma = model.get_coefficient('intercept[user]')\n</code></pre> <ol> <li>After estimating the model with GPU, the coefficient values are stored in the GPU memory. We need move the coefficient values to CPU memory and convert it to a numpy array before plotting.</li> </ol> <pre><code>gamma = gamma.cpu().numpy()\n</code></pre> <ol> <li>The tensor of individual fixed effects has shape (num_users, 1), you can use <code>squeeze()</code> to remove the dimension of size 1. Since we haven't updated the model in this tutorial, the coefficient values are all zeros.</li> </ol> <pre><code>gamma = gamma.squeeze()\ngamma\n</code></pre> <pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)\n</code></pre> <ol> <li>Researcher can use <code>matplotlib</code> to plot the distribution of the coefficient values. For example, the distribution plot of coefficients is helpful to identify potential groups of users with different preferences.</li> </ol> <pre><code>fig, ax = plt.subplots(figsize=(10, 5))\nax.hist(gamma)\nplt.show()\n</code></pre> <p></p>"},{"location":"post_estimation_demos/#nested-logit-model","title":"Nested Logit Model","text":"<p>The nested logit model has a very similar interface for coefficient extraction to the conditional logit model demonstrated above.</p> <p>Consider a nested logit model with the same item-level model but with nest-level model incorporating user-fixed effect, category-fixed effect (specified by <code>(1|item)</code> in the <code>nest_formula</code>), and user-specific coefficient on a 64-dimensional nest-specific observable (specified by <code>(item_obs|user)</code> in the <code>nest_formula</code>).</p> <p>The only difference is researcher would need to retrieve the coefficients of the nested logit model using the <code>get_coefficient()</code> method with the <code>level</code> argument.</p>"},{"location":"post_estimation_demos/#nestedlogitmodelget_coefficient-method","title":"<code>NestedLogitModel.get_coefficient()</code> Method.","text":"<pre><code>nest_to_item = {\n    0: [0, 1, 2, 3, 4],\n    1: [5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14],\n    3: [15, 16, 17, 18, 19],\n    4: [20, 21, 22, 23, 24]\n}\n\nnest_dataset = ChoiceDataset(item_index=item_index, user_index=user_index, num_items=len(nest_to_item), num_users=num_users, item_obs=torch.randn(len(nest_to_item), 64))\njoint_dataset = JointDataset(nest=nest_dataset, item=dataset)\njoint_dataset\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\n\n\n\n\n\nJointDataset with 2 sub-datasets: (\n    nest: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[], item_obs=[5, 64], device=cpu)\n    item: ChoiceDataset(label=[], item_index=[10000], user_index=[10000], session_index=[10000], item_availability=[500, 25], user_obs=[100, 128], item_obs=[25, 64], session_obs=[500, 10], itemsession_obs=[500, 25, 12], device=cpu)\n)\n</code></pre> <pre><code>nested_model = NestedLogitModel(nest_to_item=nest_to_item,\n                                nest_formula='(1|user) + (1|item) + (item_obs|user)',\n                                item_formula='(1|constant) + (1|item) + (1|user) + (user_obs|item) + (item_obs|constant) + (session_obs|user) + (itemsession_obs|item) + (itemsession_obs|user)',\n                                num_users=num_users,\n                                dataset=joint_dataset,\n                                shared_lambda=False)\nnested_model\n</code></pre> <pre><code>NestedLogitModel(\n  (nest_coef_dict): ModuleDict(\n    (intercept[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=5, num_users=100, num_params=1, 4 trainable parameters in total, device=cpu).\n    (item_obs[user]): Coefficient(variation=user, num_items=5, num_users=100, num_params=64, 6400 trainable parameters in total, device=cpu).\n  )\n  (item_coef_dict): ModuleDict(\n    (intercept[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=1, 1 trainable parameters in total, device=cpu).\n    (intercept[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=1, 24 trainable parameters in total, device=cpu).\n    (intercept[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=1, 100 trainable parameters in total, device=cpu).\n    (user_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=128, 3072 trainable parameters in total, device=cpu).\n    (item_obs[constant]): Coefficient(variation=constant, num_items=25, num_users=100, num_params=64, 64 trainable parameters in total, device=cpu).\n    (session_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=10, 1000 trainable parameters in total, device=cpu).\n    (itemsession_obs[item]): Coefficient(variation=item, num_items=25, num_users=100, num_params=12, 288 trainable parameters in total, device=cpu).\n    (itemsession_obs[user]): Coefficient(variation=user, num_items=25, num_users=100, num_params=12, 1200 trainable parameters in total, device=cpu).\n  )\n)\n</code></pre> <pre><code># estimate the model... omitted in this tutorial.\n</code></pre> <p>For example, you can use the following code snippet to retrieve the coefficient of the user-fixed effect in the nest level model, which is a vector with <code>num_users</code> elements.</p> <pre><code>nested_model.get_coefficient('intercept[user]', level='nest').shape\n</code></pre> <pre><code>torch.Size([100, 1])\n</code></pre> <p>Similarly, by changing to <code>level='item'</code>, the researcher can obtain the coefficient of user-specific fixed effect in the item level model, which is also a vector with <code>num_users</code> elements.</p> <pre><code>nested_model.get_coefficient('intercept[user]', level='item').shape\n</code></pre> <pre><code>torch.Size([100, 1])\n</code></pre> <p>This API generalizes to all other coefficients listed above such as <code>itemsession_obs[item]</code> and <code>itemsession_obs[user]</code>.</p> <p>One exception is the coefficients for inclusive values, (often denoted as \\(\\lambda\\)). Researchers can retrieve the coefficient of the inclusive value by using <code>get_coefficient('lambda')</code> without specifying the <code>level</code> argument (<code>get_coefficient</code> will disregard any <code>level</code> argument if the coefficient name is <code>lambda</code>). The returned value is a scalar if <code>shared_lambda</code> is <code>True</code>, and a 1D array of length <code>num_nests</code> if <code>shared_lambda</code> is <code>False</code>. In our case, the returned value is an array of length five (we have five nests in this model).</p> <pre><code>nested_model.get_coefficient('lambda')\n</code></pre> <pre><code>tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000])\n</code></pre>"},{"location":"projects/","title":"Research Projects using this Package","text":""},{"location":"projects/#question-answering-data-for-educational-applications","title":"Question-Answering Data for Educational Applications","text":"<p>Tutorial on Educational Question-Answering</p>"},{"location":"regularization/","title":"Regularization: \\(L_1\\) and \\(L_2\\)","text":"<p>Author: Tianyu Du Date: Sept. 28, 2022</p> <p>Also known as weight decay or penalized regression. Adding the regularization loss term would shrink coefficient magnitudes and better prevent over-fitting.</p> <p>Specifically, we add the \\(L_1\\) or \\(L_2\\) norm of coefficients to the loss (negative log-likelihood) function.</p> \\[ \\text{Loss} = \\text{NegativeLogLikelihood} + \\alpha \\sum_{c \\in \\text{model coefficients}} ||c||_p \\quad p \\in \\{1, 2\\} \\] <p>Readers can adjust the \\(\\alpha\\) weight to control the strength of regularization.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch_choice.data import ChoiceDataset, JointDataset, utils\nfrom torch_choice.model.nested_logit_model import NestedLogitModel\nfrom torch_choice.model import ConditionalLogitModel\nfrom torch_choice.utils.run_helper import run\n</code></pre> <pre><code>if torch.cuda.is_available():\n    print(f'CUDA device used: {torch.cuda.get_device_name()}')\n    device = 'cuda'\nelse:\n    print('Running tutorial on CPU.')\n    device = 'cpu'\n</code></pre> <pre><code>CUDA device used: NVIDIA GeForce RTX 3090\n</code></pre>"},{"location":"regularization/#conditional-logit-model","title":"Conditional Logit Model","text":"<pre><code>df = pd.read_csv('./public_datasets/ModeCanada.csv')\ndf = df.query('noalt == 4').reset_index(drop=True)\ndf.sort_values(by='case', inplace=True)\nitem_index = df[df['choice'] == 1].sort_values(by='case')['alt'].reset_index(drop=True)\nitem_names = ['air', 'bus', 'car', 'train']\nnum_items = 4\nencoder = dict(zip(item_names, range(num_items)))\nitem_index = item_index.map(lambda x: encoder[x])\nitem_index = torch.LongTensor(item_index)\nprice_cost_freq_ovt = utils.pivot3d(df, dim0='case', dim1='alt',\n                                    values=['cost', 'freq', 'ovt'])\n\nprice_ivt = utils.pivot3d(df, dim0='case', dim1='alt', values='ivt')\nsession_income = df.groupby('case')['income'].first()\nsession_income = torch.Tensor(session_income.values).view(-1, 1)\ndataset = ChoiceDataset(item_index=item_index,\n                        price_cost_freq_ovt=price_cost_freq_ovt,\n                        session_income=session_income,\n                        price_ivt=price_ivt\n                        ).to(device)\nprint(dataset)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\nChoiceDataset(label=[], item_index=[2779], user_index=[], session_index=[2779], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n</code></pre> <pre><code># shuffle the dataset.\nN = len(dataset)\nshuffle_index = np.random.permutation(N)\n\ntrain_index = shuffle_index[:int(0.7 * N)]\ntest_index = shuffle_index[int(0.7 * N):]\n\n# splits of dataset.\ndataset_train, dataset_test = dataset[train_index], dataset[test_index]\n</code></pre> <pre><code>conditional_logit_common_arguments = {\n    \"coef_variation_dict\": {'price_cost_freq_ovt': 'constant',\n                            'session_income': 'item',\n                            'price_ivt': 'item-full',\n                            'intercept': 'item'},\n    \"num_param_dict\": {'price_cost_freq_ovt': 3,\n                       'session_income': 1,\n                       'price_ivt': 1,\n                       'intercept': 1},\n    \"num_items\": 4,\n}\n</code></pre> <pre><code>def train_conditional_logit_model(regularization, regularization_weight):\n    model = ConditionalLogitModel(**conditional_logit_common_arguments,\n                                regularization=regularization,\n                                regularization_weight=regularization_weight).to(device)\n\n    run(model, dataset_train, dataset_test=dataset_test, num_epochs=50000, learning_rate=0.003, batch_size=-1)\n    # report total model weight\n    print('Total weight L2 norm:', sum([torch.norm(param, p=2) for param in model.parameters()]))\n</code></pre> <pre><code>train_conditional_logit_model(regularization=None, regularization_weight=None)\n</code></pre> <pre><code>==================== received model ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n    (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt] with 3 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[price_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cuda:0\n==================== received dataset ====================\nChoiceDataset(label=[], item_index=[1945], user_index=[], session_index=[1945], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n==================== training the model ====================\nEpoch 5000: Log-likelihood=-1322.9208984375\nEpoch 10000: Log-likelihood=-1322.427490234375\nEpoch 15000: Log-likelihood=-1322.361572265625\nEpoch 20000: Log-likelihood=-1322.354736328125\nEpoch 25000: Log-likelihood=-1322.4718017578125\nEpoch 30000: Log-likelihood=-1331.5247802734375\nEpoch 35000: Log-likelihood=-1322.3544921875\nEpoch 40000: Log-likelihood=-1322.421142578125\nEpoch 45000: Log-likelihood=-1322.3602294921875\nEpoch 50000: Log-likelihood=-1322.495849609375\nTest set log-likelihood:  -554.70849609375\n==================== model results ====================\nTraining Epochs: 50000\n\nLearning Rate: 0.003\n\nBatch Size: 1945 out of 1945 observations in total\n\nFinal Log-likelihood: -1322.495849609375\n\nCoefficients:\n\n| Coefficient           |   Estimation |   Std. Err. |\n|:----------------------|-------------:|------------:|\n| price_cost_freq_ovt_0 |  -0.0308257  |  0.00839731 |\n| price_cost_freq_ovt_1 |   0.0945616  |  0.00598799 |\n| price_cost_freq_ovt_2 |  -0.0397223  |  0.00373588 |\n| session_income_0      |  -0.0716898  |  0.0195864  |\n| session_income_1      |  -0.0273578  |  0.00459898 |\n| session_income_2      |  -0.038647   |  0.00484347 |\n| price_ivt_0           |   0.0564822  |  0.0117201  |\n| price_ivt_1           |  -0.00936753 |  0.00582746 |\n| price_ivt_2           |  -0.00678837 |  0.00222236 |\n| price_ivt_3           |  -0.00175041 |  0.00139018 |\n| intercept_0           |   0.899362   |  1.53674    |\n| intercept_1           |   2.24992    |  0.848803   |\n| intercept_2           |   3.50811    |  0.747974   |\nTotal weight L2 norm: tensor(2.6599, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <pre><code>train_conditional_logit_model(regularization='L1', regularization_weight=5)\n</code></pre> <pre><code>==================== received model ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n    (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt] with 3 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[price_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cuda:0\n==================== received dataset ====================\nChoiceDataset(label=[], item_index=[1945], user_index=[], session_index=[1945], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n==================== training the model ====================\nEpoch 5000: Log-likelihood=-1327.5283203125\nEpoch 10000: Log-likelihood=-1327.5472412109375\nEpoch 15000: Log-likelihood=-1327.5458984375\nEpoch 20000: Log-likelihood=-1327.5452880859375\nEpoch 25000: Log-likelihood=-1327.54931640625\nEpoch 30000: Log-likelihood=-1327.9013671875\nEpoch 35000: Log-likelihood=-1327.5465087890625\nEpoch 40000: Log-likelihood=-1327.6224365234375\nEpoch 45000: Log-likelihood=-1327.5556640625\nEpoch 50000: Log-likelihood=-1333.43359375\nTest set log-likelihood:  -556.6971435546875\n==================== model results ====================\nTraining Epochs: 50000\n\nLearning Rate: 0.003\n\nBatch Size: 1945 out of 1945 observations in total\n\nFinal Log-likelihood: -1333.43359375\n\nCoefficients:\n\n| Coefficient           |   Estimation |   Std. Err. |\n|:----------------------|-------------:|------------:|\n| price_cost_freq_ovt_0 | -0.0485882   |  0.0084985  |\n| price_cost_freq_ovt_1 |  0.0963804   |  0.00600474 |\n| price_cost_freq_ovt_2 | -0.0381796   |  0.00383793 |\n| session_income_0      | -0.0766308   |  0.0208468  |\n| session_income_1      | -0.0225714   |  0.00444105 |\n| session_income_2      | -0.0326763   |  0.00488883 |\n| price_ivt_0           |  0.0531795   |  0.0118078  |\n| price_ivt_1           | -0.0166434   |  0.0080002  |\n| price_ivt_2           | -0.00397061  |  0.00221348 |\n| price_ivt_3           | -0.00189491  |  0.00140921 |\n| intercept_0           |  0.000167495 |  1.69499    |\n| intercept_1           |  0.000309494 |  0.833982   |\n| intercept_2           |  1.2901      |  0.729501   |\nTotal weight L2 norm: tensor(1.3817, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <pre><code>train_conditional_logit_model(regularization='L2', regularization_weight=5)\n</code></pre> <pre><code>==================== received model ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n    (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt] with 3 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[price_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cuda:0\n==================== received dataset ====================\nChoiceDataset(label=[], item_index=[1945], user_index=[], session_index=[1945], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n==================== training the model ====================\nEpoch 5000: Log-likelihood=-1327.98876953125\nEpoch 10000: Log-likelihood=-1327.377197265625\nEpoch 15000: Log-likelihood=-1327.3466796875\nEpoch 20000: Log-likelihood=-1327.345458984375\nEpoch 25000: Log-likelihood=-1327.433349609375\nEpoch 30000: Log-likelihood=-1327.3453369140625\nEpoch 35000: Log-likelihood=-1327.34521484375\nEpoch 40000: Log-likelihood=-1327.3885498046875\nEpoch 45000: Log-likelihood=-1327.3486328125\nEpoch 50000: Log-likelihood=-1327.34765625\nTest set log-likelihood:  -555.1453857421875\n==================== model results ====================\nTraining Epochs: 50000\n\nLearning Rate: 0.003\n\nBatch Size: 1945 out of 1945 observations in total\n\nFinal Log-likelihood: -1327.34765625\n\nCoefficients:\n\n| Coefficient           |   Estimation |   Std. Err. |\n|:----------------------|-------------:|------------:|\n| price_cost_freq_ovt_0 |  -0.0482729  |  0.0083645  |\n| price_cost_freq_ovt_1 |   0.0967298  |  0.00595309 |\n| price_cost_freq_ovt_2 |  -0.0376925  |  0.0037188  |\n| session_income_0      |  -0.0749973  |  0.019634   |\n| session_income_1      |  -0.0231255  |  0.00446823 |\n| session_income_2      |  -0.032398   |  0.00475483 |\n| price_ivt_0           |   0.0534635  |  0.0117147  |\n| price_ivt_1           |  -0.0153539  |  0.00731768 |\n| price_ivt_2           |  -0.00426721 |  0.00219745 |\n| price_ivt_3           |  -0.00154632 |  0.00138443 |\n| intercept_0           |  -0.201299   |  1.60544    |\n| intercept_1           |   0.00875631 |  0.823289   |\n| intercept_2           |   1.29872    |  0.715818   |\nTotal weight L2 norm: tensor(1.5968, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <pre><code>train_conditional_logit_model(regularization='L1', regularization_weight=1E5)\n</code></pre> <pre><code>==================== received model ====================\nConditionalLogitModel(\n  (coef_dict): ModuleDict(\n    (price_cost_freq_ovt): Coefficient(variation=constant, num_items=4, num_users=None, num_params=3, 3 trainable parameters in total, device=cuda:0).\n    (session_income): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n    (price_ivt): Coefficient(variation=item-full, num_items=4, num_users=None, num_params=1, 4 trainable parameters in total, device=cuda:0).\n    (intercept): Coefficient(variation=item, num_items=4, num_users=None, num_params=1, 3 trainable parameters in total, device=cuda:0).\n  )\n)\nConditional logistic discrete choice model, expects input features:\n\nX[price_cost_freq_ovt] with 3 parameters, with constant level variation.\nX[session_income] with 1 parameters, with item level variation.\nX[price_ivt] with 1 parameters, with item-full level variation.\nX[intercept] with 1 parameters, with item level variation.\ndevice=cuda:0\n==================== received dataset ====================\nChoiceDataset(label=[], item_index=[1945], user_index=[], session_index=[1945], item_availability=[], price_cost_freq_ovt=[2779, 4, 3], session_income=[2779, 1], price_ivt=[2779, 4, 1], device=cuda:0)\n==================== training the model ====================\nEpoch 5000: Log-likelihood=-2680.06005859375\nEpoch 10000: Log-likelihood=-2431.19091796875\nEpoch 15000: Log-likelihood=-2651.45849609375\nEpoch 20000: Log-likelihood=-2578.85107421875\nEpoch 25000: Log-likelihood=-2525.41650390625\nEpoch 30000: Log-likelihood=-2554.415283203125\nEpoch 35000: Log-likelihood=-2570.41845703125\nEpoch 40000: Log-likelihood=-2658.0556640625\nEpoch 45000: Log-likelihood=-2560.906005859375\nEpoch 50000: Log-likelihood=-2677.46826171875\nTest set log-likelihood:  -1136.294921875\n==================== model results ====================\nTraining Epochs: 50000\n\nLearning Rate: 0.003\n\nBatch Size: 1945 out of 1945 observations in total\n\nFinal Log-likelihood: -2677.46826171875\n\nCoefficients:\n\n| Coefficient           |   Estimation |   Std. Err. |\n|:----------------------|-------------:|------------:|\n| price_cost_freq_ovt_0 |  0.000446639 | 0.00574829  |\n| price_cost_freq_ovt_1 | -0.000407603 | 0.00415769  |\n| price_cost_freq_ovt_2 |  0.000226522 | 0.0021607   |\n| session_income_0      | -4.7971e-05  | 0.00383794  |\n| session_income_1      |  0.00117954  | 0.00375016  |\n| session_income_2      |  0.00041626  | 0.00359678  |\n| price_ivt_0           | -0.000192594 | 0.00875022  |\n| price_ivt_1           | -0.000618745 | 0.000871537 |\n| price_ivt_2           | -0.000398202 | 0.00165723  |\n| price_ivt_3           |  0.000407054 | 0.00104901  |\n| intercept_0           | -0.000648632 | 0.567814    |\n| intercept_1           | -0.000525868 | 0.580968    |\n| intercept_2           | -0.000405973 | 0.505175    |\nTotal weight L2 norm: tensor(1.3426, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n</code></pre>"},{"location":"regularization/#on-nested-logit-model","title":"On Nested Logit Model","text":"<pre><code>df = pd.read_csv('./public_datasets/HC.csv', index_col=0)\ndf = df.reset_index(drop=True)\ndf.head()\n\n# what was actually chosen.\nitem_index = df[df['depvar'] == True].sort_values(by='idx.id1')['idx.id2'].reset_index(drop=True)\nitem_names = ['ec', 'ecc', 'er', 'erc', 'gc', 'gcc', 'hpc']\nnum_items = df['idx.id2'].nunique()\n# cardinal encoder.\nencoder = dict(zip(item_names, range(num_items)))\nitem_index = item_index.map(lambda x: encoder[x])\nitem_index = torch.LongTensor(item_index)\n\n# category feature: no category feature, all features are item-level.\ncategory_dataset = ChoiceDataset(item_index=item_index.clone()).to(device)\n\n# item feature.\nitem_feat_cols = ['ich', 'och', 'icca', 'occa', 'inc.room', 'inc.cooling', 'int.cooling']\nprice_obs = utils.pivot3d(df, dim0='idx.id1', dim1='idx.id2', values=item_feat_cols)\n\nitem_dataset = ChoiceDataset(item_index=item_index, price_obs=price_obs).to(device)\n\ndataset = JointDataset(category=category_dataset, item=item_dataset)\n\ncategory_to_item = {0: ['gcc', 'ecc', 'erc', 'hpc'],\n                    1: ['gc', 'ec', 'er']}\n\n# encode items to integers.\nfor k, v in category_to_item.items():\n    v = [encoder[item] for item in v]\n    category_to_item[k] = sorted(v)\n</code></pre> <pre><code>No `session_index` is provided, assume each choice instance is in its own session.\nNo `session_index` is provided, assume each choice instance is in its own session.\n</code></pre> <pre><code>def train_nested_logit_model(regularization, regularization_weight):\n    model = NestedLogitModel(category_to_item=category_to_item,\n                         category_coef_variation_dict={},\n                         category_num_param_dict={},\n                         item_coef_variation_dict={'price_obs': 'constant'},\n                         item_num_param_dict={'price_obs': 7},\n                         regularization=regularization,\n                         regularization_weight=regularization_weight,\n                         shared_lambda=True).to(device)\n    run(model, dataset, num_epochs=10000)\n</code></pre> <pre><code>train_nested_logit_model(None, None)\n</code></pre> <pre><code>==================== received model ====================\nNestedLogitModel(\n  (category_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cuda:0).\n  )\n)\n==================== received dataset ====================\nJointDataset with 2 sub-datasets: (\n    category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0)\n)\n==================== training the model ====================\nEpoch 1000: Log-likelihood=-226.63345336914062\nEpoch 2000: Log-likelihood=-189.08030700683594\nEpoch 3000: Log-likelihood=-181.08639526367188\nEpoch 4000: Log-likelihood=-179.11544799804688\nEpoch 5000: Log-likelihood=-178.78994750976562\nEpoch 6000: Log-likelihood=-178.64102172851562\nEpoch 7000: Log-likelihood=-178.50711059570312\nEpoch 8000: Log-likelihood=-178.36279296875\nEpoch 9000: Log-likelihood=-178.23562622070312\nEpoch 10000: Log-likelihood=-178.15724182128906\n==================== model results ====================\nTraining Epochs: 10000\n\nLearning Rate: 0.01\n\nBatch Size: 250 out of 250 observations in total\n\nFinal Log-likelihood: -178.15724182128906\n\nCoefficients:\n\n| Coefficient      |   Estimation |   Std. Err. |\n|:-----------------|-------------:|------------:|\n| lambda_weight_0  |     0.569814 |   0.163447  |\n| item_price_obs_0 |    -0.5397   |   0.141929  |\n| item_price_obs_1 |    -0.834805 |   0.233345  |\n| item_price_obs_2 |    -0.242956 |   0.110592  |\n| item_price_obs_3 |    -1.27541  |   1.03548   |\n| item_price_obs_4 |    -0.368249 |   0.0986935 |\n| item_price_obs_5 |     0.247266 |   0.0513082 |\n| item_price_obs_6 |    -4.78207  |   4.7152    |\n</code></pre> <pre><code>train_nested_logit_model(\"L1\", 10)\n</code></pre> <pre><code>==================== received model ====================\nNestedLogitModel(\n  (category_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cuda:0).\n  )\n)\n==================== received dataset ====================\nJointDataset with 2 sub-datasets: (\n    category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0)\n)\n==================== training the model ====================\nEpoch 1000: Log-likelihood=-186.81593322753906\nEpoch 2000: Log-likelihood=-187.0428924560547\nEpoch 3000: Log-likelihood=-188.46871948242188\nEpoch 4000: Log-likelihood=-187.3245849609375\nEpoch 5000: Log-likelihood=-187.10488891601562\nEpoch 6000: Log-likelihood=-187.18087768554688\nEpoch 7000: Log-likelihood=-187.34005737304688\nEpoch 8000: Log-likelihood=-187.11846923828125\nEpoch 9000: Log-likelihood=-187.3697509765625\nEpoch 10000: Log-likelihood=-187.0865478515625\n==================== model results ====================\nTraining Epochs: 10000\n\nLearning Rate: 0.01\n\nBatch Size: 250 out of 250 observations in total\n\nFinal Log-likelihood: -187.0865478515625\n\nCoefficients:\n\n| Coefficient      |   Estimation |   Std. Err. |\n|:-----------------|-------------:|------------:|\n| lambda_weight_0  |  0.0530321   |   0.0531535 |\n| item_price_obs_0 | -0.0512223   |   0.0514528 |\n| item_price_obs_1 | -0.0779116   |   0.078385  |\n| item_price_obs_2 | -0.187379    |   0.087971  |\n| item_price_obs_3 | -0.00119437  |   0.863954  |\n| item_price_obs_4 | -0.0346545   |   0.0350824 |\n| item_price_obs_5 |  0.183375    |   0.034789  |\n| item_price_obs_6 |  0.000892786 |   3.57438   |\n</code></pre> <pre><code>train_nested_logit_model(\"L2\", 10)\n</code></pre> <pre><code>==================== received model ====================\nNestedLogitModel(\n  (category_coef_dict): ModuleDict()\n  (item_coef_dict): ModuleDict(\n    (price_obs): Coefficient(variation=constant, num_items=7, num_users=None, num_params=7, 7 trainable parameters in total, device=cuda:0).\n  )\n)\n==================== received dataset ====================\nJointDataset with 2 sub-datasets: (\n    category: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], device=cuda:0)\n    item: ChoiceDataset(label=[], item_index=[250], user_index=[], session_index=[250], item_availability=[], price_obs=[250, 7, 7], device=cuda:0)\n)\n==================== training the model ====================\nEpoch 1000: Log-likelihood=-219.621826171875\nEpoch 2000: Log-likelihood=-200.87660217285156\nEpoch 3000: Log-likelihood=-192.0721435546875\nEpoch 4000: Log-likelihood=-183.12820434570312\nEpoch 5000: Log-likelihood=-182.87225341796875\nEpoch 6000: Log-likelihood=-183.52407836914062\nEpoch 7000: Log-likelihood=-183.50723266601562\nEpoch 8000: Log-likelihood=-183.5075225830078\nEpoch 9000: Log-likelihood=-183.50465393066406\nEpoch 10000: Log-likelihood=-183.5073699951172\n==================== model results ====================\nTraining Epochs: 10000\n\nLearning Rate: 0.01\n\nBatch Size: 250 out of 250 observations in total\n\nFinal Log-likelihood: -183.5073699951172\n\nCoefficients:\n\n| Coefficient      |   Estimation |   Std. Err. |\n|:-----------------|-------------:|------------:|\n| lambda_weight_0  |    0.181474  |   0.108225  |\n| item_price_obs_0 |   -0.174871  |   0.102564  |\n| item_price_obs_1 |   -0.265047  |   0.156401  |\n| item_price_obs_2 |   -0.258935  |   0.0949367 |\n| item_price_obs_3 |   -0.151668  |   0.898396  |\n| item_price_obs_4 |   -0.118241  |   0.0697575 |\n| item_price_obs_5 |    0.193267  |   0.0380327 |\n| item_price_obs_6 |   -0.0374295 |   3.90292   |\n</code></pre>"},{"location":"test/","title":"Compatibility Check List","text":"<p>We have tested the tutorials using the following environments, please let us know if there is any issue with our packages on other systems.</p> Tutorial Platform Versions CPU GPU Device Tested Data Management MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Data Management Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Conditional Logit Model MacOS 12.2 Python 3.9 PyTorch 1.10.0 M1 Max N/A cpu Conditional Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda Nested Logit Model MacOS 12.2 Python 3.9.7 PyTorch 1.10.0 M1 Max N/A cpu Nested Logit Model Ubuntu 20.04 Python 3.8.10 PyTorch 1.10.1 CUDA 11.3 11700F RTX3090 cpu and cuda"}]}